{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMKHTsedaum7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import argparse\n",
    "from copy import deepcopy #Add Deepcopy for args\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1gRd7qVcEwI"
   },
   "source": [
    "# 1. Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KShxDU_Jcj3u"
   },
   "source": [
    "### 1.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTiRk82qctLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020_공모전\u001b[m\u001b[m/\r\n",
      "IMG_0221.png\r\n",
      "Input(DG)_data.xlsx\r\n",
      "Input(DG)_log.xlsx\r\n",
      "Output(HA)_data.xlsx\r\n",
      "Output(HA)_log.xlsx\r\n",
      "\u001b[34mPython\u001b[m\u001b[m/\r\n",
      "\u001b[34mTN(DG_HA)_exp01\u001b[m\u001b[m/\r\n",
      "\u001b[34mTN(DG_HA)_exp02\u001b[m\u001b[m/\r\n",
      "\u001b[34mTP(DG_HA)_exp01\u001b[m\u001b[m/\r\n",
      "\u001b[34mTP(DG_HA)_exp02\u001b[m\u001b[m/\r\n",
      "\u001b[34mUnity\u001b[m\u001b[m/\r\n",
      "corr_p(덕곡_함안).xlsx\r\n",
      "corr_s(덕곡_함안).xlsx\r\n",
      "\u001b[34metc\u001b[m\u001b[m/\r\n",
      "\u001b[34mjupyter\u001b[m\u001b[m/\r\n",
      "\u001b[34mmodel_save\u001b[m\u001b[m/\r\n",
      "\u001b[34mresult\u001b[m\u001b[m/\r\n",
      "\u001b[34mresult02\u001b[m\u001b[m/\r\n",
      "\u001b[34m과외\u001b[m\u001b[m/\r\n",
      "시디1_05.pdf\r\n",
      "\u001b[34m임시\u001b[m\u001b[m/\r\n",
      "졸논_서론_피드백.docx\r\n",
      "덕곡_함안.xlsx\r\n",
      "졸논_본문_피드백.pptx\r\n",
      "덕곡_함안_상관계수.xlsx\r\n",
      "\u001b[34m시각디자인\u001b[m\u001b[m/\r\n",
      "\u001b[34m졸업논문\u001b[m\u001b[m/\r\n",
      "\u001b[34m환경공학\u001b[m\u001b[m/\r\n",
      "현풍대암덕곡_데이터.xlsx\r\n",
      "대암덕곡상류유사도.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "ls #현재경로에 있는 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dln4LnKpc1CX"
   },
   "outputs": [],
   "source": [
    "UpStream_data = pd.read_excel('Input(DG)_log.xlsx')\n",
    "DownStream_data = pd.read_excel('Output(HA)_log.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GRAwoM_c068"
   },
   "outputs": [],
   "source": [
    "# print(UpStream_data.columns)\n",
    "# print(DownStream_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [],
   "source": [
    "#날짜 인덱스화\n",
    "UpData = UpStream_data.set_index('Date')\n",
    "DownData = DownStream_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 74 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DukGok_DO          349 non-null    float64\n",
      " 1   DukGok_BOD         349 non-null    float64\n",
      " 2   DukGok_COD         349 non-null    float64\n",
      " 3   DukGok_SS          349 non-null    float64\n",
      " 4   DukGok_TN          349 non-null    float64\n",
      " 5   DukGok_TP          349 non-null    float64\n",
      " 6   DukGok_Chl_a       349 non-null    float64\n",
      " 7   ChilSeo_COD        349 non-null    float64\n",
      " 8   ChilSeo_SS         349 non-null    float64\n",
      " 9   ChilSeo_TN         349 non-null    float64\n",
      " 10  ChilSeo_TP         349 non-null    float64\n",
      " 11  ChilSeo_Flow_mean  349 non-null    float64\n",
      " 12  ChilSeo_Flow_day   349 non-null    float64\n",
      " 13  ChilSeo_COD_load   349 non-null    float64\n",
      " 14  ChilSeo_SS_load    349 non-null    float64\n",
      " 15  ChilSeo_TN_load    349 non-null    float64\n",
      " 16  ChilSeo_TP_load    349 non-null    float64\n",
      " 17  BukChang_Rain      349 non-null    float64\n",
      " 18  BukChang_Solar     349 non-null    float64\n",
      " 19  HwangGang_DO       349 non-null    float64\n",
      " 20  HwangGang_BOD      349 non-null    float64\n",
      " 21  HwangGang_COD      349 non-null    float64\n",
      " 22  HwangGang_SS       349 non-null    float64\n",
      " 23  HwangGang_TN       349 non-null    float64\n",
      " 24  HwangGang_TP       349 non-null    float64\n",
      " 25  HwangGang_Chl_a    349 non-null    float64\n",
      " 26  HwangGang_Flow     349 non-null    float64\n",
      " 27  SanBan_DO          349 non-null    float64\n",
      " 28  SanBan_BOD         349 non-null    float64\n",
      " 29  SanBan_COD         349 non-null    float64\n",
      " 30  SanBan_SS          349 non-null    float64\n",
      " 31  SanBan_TN          349 non-null    float64\n",
      " 32  SanBan_TP          349 non-null    float64\n",
      " 33  SanBan_Chl_a       349 non-null    float64\n",
      " 34  SanBan_Flow        349 non-null    float64\n",
      " 35  NamGang_DO         349 non-null    float64\n",
      " 36  NamGang_BOD        349 non-null    float64\n",
      " 37  NamGang_COD        349 non-null    float64\n",
      " 38  NamGang_SS         349 non-null    float64\n",
      " 39  NamGang_TN         349 non-null    float64\n",
      " 40  NamGang_TP         349 non-null    float64\n",
      " 41  NamGang_Chl_a      349 non-null    float64\n",
      " 42  NamGang_Flow       349 non-null    float64\n",
      " 43  GyeaSung_DO        349 non-null    float64\n",
      " 44  GyeaSung_BOD       349 non-null    float64\n",
      " 45  GyeaSung_COD       349 non-null    float64\n",
      " 46  GyeaSung_SS        349 non-null    float64\n",
      " 47  GyeaSung_TN        349 non-null    float64\n",
      " 48  GyeaSung_TP        349 non-null    float64\n",
      " 49  GyeaSung_Chl_a     349 non-null    float64\n",
      " 50  GyeaSung_Flow      349 non-null    float64\n",
      " 51  GwangRyeo_DO       349 non-null    float64\n",
      " 52  GwangRyeo_BOD      349 non-null    float64\n",
      " 53  GwangRyeo_COD      349 non-null    float64\n",
      " 54  GwangRyeo_SS       349 non-null    float64\n",
      " 55  GwangRyeo_TN       349 non-null    float64\n",
      " 56  GwangRyeo_TP       349 non-null    float64\n",
      " 57  GwangRyeo_Chl_a    349 non-null    float64\n",
      " 58  GwangRyeo_Flow     349 non-null    float64\n",
      " 59  ToPyeong_DO        349 non-null    float64\n",
      " 60  ToPyeong_BOD       349 non-null    float64\n",
      " 61  ToPyeong_COD       349 non-null    float64\n",
      " 62  ToPyeong_SS        349 non-null    float64\n",
      " 63  ToPyeong_TN        349 non-null    float64\n",
      " 64  ToPyeong_TP        349 non-null    float64\n",
      " 65  ToPyeong_Chl_a     349 non-null    float64\n",
      " 66  ToPyeong_Flow      349 non-null    float64\n",
      " 67  DukGok_Temp        349 non-null    float64\n",
      " 68  HwangGang_Temp     349 non-null    float64\n",
      " 69  SanBan_Temp        349 non-null    float64\n",
      " 70  NamGang_Temp       349 non-null    float64\n",
      " 71  GyeaSung_Temp      349 non-null    float64\n",
      " 72  GwangRyeo_Temp     349 non-null    float64\n",
      " 73  ToPyeong_Temp      349 non-null    float64\n",
      "dtypes: float64(74)\n",
      "memory usage: 204.5+ KB\n"
     ]
    }
   ],
   "source": [
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 41 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   DukGok_DO       349 non-null    float64\n",
      " 1   DukGok_BOD      349 non-null    float64\n",
      " 2   DukGok_COD      349 non-null    float64\n",
      " 3   DukGok_SS       349 non-null    float64\n",
      " 4   DukGok_TN       349 non-null    float64\n",
      " 5   DukGok_TP       349 non-null    float64\n",
      " 6   DukGok_Temp     349 non-null    float64\n",
      " 7   BukChang_Rain   349 non-null    float64\n",
      " 8   BukChang_Solar  349 non-null    float64\n",
      " 9   HwangGang_DO    349 non-null    float64\n",
      " 10  HwangGang_SS    349 non-null    float64\n",
      " 11  HwangGang_TP    349 non-null    float64\n",
      " 12  HwangGang_Temp  349 non-null    float64\n",
      " 13  HwangGang_Flow  349 non-null    float64\n",
      " 14  SanBan_DO       349 non-null    float64\n",
      " 15  SanBan_COD      349 non-null    float64\n",
      " 16  SanBan_TP       349 non-null    float64\n",
      " 17  SanBan_Temp     349 non-null    float64\n",
      " 18  SanBan_Flow     349 non-null    float64\n",
      " 19  NamGang_DO      349 non-null    float64\n",
      " 20  NamGang_COD     349 non-null    float64\n",
      " 21  NamGang_SS      349 non-null    float64\n",
      " 22  NamGang_TN      349 non-null    float64\n",
      " 23  NamGang_TP      349 non-null    float64\n",
      " 24  NamGang_Temp    349 non-null    float64\n",
      " 25  NamGang_Chl_a   349 non-null    float64\n",
      " 26  NamGang_Flow    349 non-null    float64\n",
      " 27  GyeaSung_DO     349 non-null    float64\n",
      " 28  GyeaSung_TP     349 non-null    float64\n",
      " 29  GyeaSung_Temp   349 non-null    float64\n",
      " 30  GyeaSung_Flow   349 non-null    float64\n",
      " 31  GwangRyeo_DO    349 non-null    float64\n",
      " 32  GwangRyeo_SS    349 non-null    float64\n",
      " 33  GwangRyeo_TP    349 non-null    float64\n",
      " 34  GwangRyeo_Temp  349 non-null    float64\n",
      " 35  ToPyeong_DO     349 non-null    float64\n",
      " 36  ToPyeong_BOD    349 non-null    float64\n",
      " 37  ToPyeong_TP     349 non-null    float64\n",
      " 38  ToPyeong_Chl_a  349 non-null    float64\n",
      " 39  ToPyeong_Flow   349 non-null    float64\n",
      " 40  ToPyeong_Temp   349 non-null    float64\n",
      "dtypes: float64(41)\n",
      "memory usage: 114.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#넣고싶은 상류 항목 컬럼 선택 - TP setting(0.25기준)\n",
    "UpData = UpData.iloc[:,[0,1,2,3,4,5,67,17,18,19,22,24,68,26,27,29,32,69,34,35,37,38,39,40,70,41,42,43,48,71,50,51,54,56,72,59,60,64,65,66,73]]\n",
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #넣고싶은 상류 항목 컬럼 선택 - TN setting\n",
    "# UpData = UpData.iloc[:,[0,4,5,18,7,8,10,12,13,19,15,16,17]]\n",
    "# UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   HamAn_DO     349 non-null    float64\n",
      " 1   HamAn_BOD    349 non-null    float64\n",
      " 2   HamAn_COD    349 non-null    float64\n",
      " 3   HamAn_SS     349 non-null    float64\n",
      " 4   HamAn_TN     349 non-null    float64\n",
      " 5   HamAn_TP     349 non-null    float64\n",
      " 6   HamAn_Chl_a  349 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "DownData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HamAn_TP\n"
     ]
    }
   ],
   "source": [
    "#알고싶은 하류 항목 컬럼 넘버 넣기('Date'항목이 인덱스화 돼서 컬럼 넘버가 -1씩 됨)\n",
    "Colum = 5\n",
    "print(DownData.columns[Colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Tp4o0Hc0ZL"
   },
   "source": [
    "### 1.2 Data Preprocessing(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChKYCAtTdGpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "UpScaler = MinMaxScaler() #상류데이터용\n",
    "DownScaler = MinMaxScaler() #하류데이터용\n",
    "\n",
    "#나중에 결과를 DeNormalizing 하기 위해 나누어 사용 하였다.\n",
    "\n",
    "def DeNormalize(Y, Data_name, column_num, Scaler_Type):\n",
    "    \n",
    "    data = Data_name\n",
    "    Scaler = Scaler_Type\n",
    "    \n",
    "    _max = Scaler.data_max_[column_num] # 역정규화 하려는 데이터의 컬럼 번호\n",
    "    _min = Scaler.data_min_[column_num] \n",
    "    \n",
    "    X = Y*(_max-_min) + _min\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DukGok_DO         0\n",
      "DukGok_BOD        0\n",
      "DukGok_COD        0\n",
      "DukGok_SS         0\n",
      "DukGok_TN         0\n",
      "DukGok_TP         0\n",
      "DukGok_Temp       0\n",
      "BukChang_Rain     0\n",
      "BukChang_Solar    0\n",
      "HwangGang_DO      0\n",
      "HwangGang_SS      0\n",
      "HwangGang_TP      0\n",
      "HwangGang_Temp    0\n",
      "HwangGang_Flow    0\n",
      "SanBan_DO         0\n",
      "SanBan_COD        0\n",
      "SanBan_TP         0\n",
      "SanBan_Temp       0\n",
      "SanBan_Flow       0\n",
      "NamGang_DO        0\n",
      "NamGang_COD       0\n",
      "NamGang_SS        0\n",
      "NamGang_TN        0\n",
      "NamGang_TP        0\n",
      "NamGang_Temp      0\n",
      "NamGang_Chl_a     0\n",
      "NamGang_Flow      0\n",
      "GyeaSung_DO       0\n",
      "GyeaSung_TP       0\n",
      "GyeaSung_Temp     0\n",
      "GyeaSung_Flow     0\n",
      "GwangRyeo_DO      0\n",
      "GwangRyeo_SS      0\n",
      "GwangRyeo_TP      0\n",
      "GwangRyeo_Temp    0\n",
      "ToPyeong_DO       0\n",
      "ToPyeong_BOD      0\n",
      "ToPyeong_TP       0\n",
      "ToPyeong_Chl_a    0\n",
      "ToPyeong_Flow     0\n",
      "ToPyeong_Temp     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#데이터 정규화\n",
    "UpData = pd.DataFrame(UpScaler.fit_transform(UpData), columns=UpData.columns, index=UpData.index)\n",
    "DownData = pd.DataFrame(DownScaler.fit_transform(DownData), columns=DownData.columns, index=DownData.index)\n",
    "\n",
    "print(UpData.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDSVZGiUdGYz"
   },
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hxm2moSudGQD"
   },
   "outputs": [],
   "source": [
    "class RiverDataset(Dataset):\n",
    "    def __init__(self, UpData, DownData, x_frames, y_frames, start, end):\n",
    "        \n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.UpData = UpData[start:end]\n",
    "        self.DownData = DownData[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.UpData) - (self.x_frames + self.y_frames) + 1\n",
    "    #데이터를 전처리 할때 UpData와 DownData의 길이가 동일해짐(날짜를 동일한것만 추출해야 하므로), 따라서 전체길이는 둘중 하나를 사용\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "\n",
    "        X = self.UpData.iloc[idx-self.x_frames:idx].values\n",
    "        Y = self.DownData.iloc[idx:idx+self.y_frames].values\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj80CahhdGG9"
   },
   "source": [
    "# 3. Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuBGEc51dF-V"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers) #\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_edit(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM_edit, self).__init__()\n",
    "        self.input_dim = 41\n",
    "        self.hidden_dim = 16\n",
    "        self.output_dim = 1\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.batch_size = 8\n",
    "        self.dropout = 0.1\n",
    "        self.use_bn = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers) #\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTpV_o6Wdglq"
   },
   "outputs": [],
   "source": [
    "# 정확도 : 예측확률을 100%로 봤을때 MAPE에 따른 오차비율을 빼줌 (100-MAPE) ##RMSE, MAPE 두개로 볼 수 있게\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    R2_score = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    return R2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GPb8H8-djwB"
   },
   "source": [
    "# 4. Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrT2W-pqdjh6"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(trainset,\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.transpose(0, 1).float().to(args.device)#파이토치는 순서가 달라서 바꿔줌\n",
    "        y_true = y[:, :, Colum].float().to(args.device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred.append(y_pred)\n",
    "        true.append(y_true)\n",
    "\n",
    "    # ========================================================================== #\n",
    "    for i in range(len(trainloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    train_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     train_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    return model, train_loss, train_acc1[0], train_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvAO-LVCdjgG"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(valset,\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        for i in range(len(valloader)):\n",
    "            tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "            tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "\n",
    "            for j in range(bat_siz):\n",
    "                value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "                value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "\n",
    "                pred_results.append(value1)\n",
    "                true_results.append(value2)\n",
    "        # ========================================================================== #   \n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    val_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     val_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    \n",
    "    return val_loss, val_acc1[0], val_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHWmu5EtdjXu"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # =================== test 데이터 시각화를 위해 x,y데이터 저장 =================== #\n",
    "    for i in range(len(testloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ======================================================================== #   \n",
    "\n",
    "    test_acc1 =  RMSE(np.array( true_results), np.array(pred_results))\n",
    "    test_acc2 =  R2(np.array( true_results), np.array(pred_results))\n",
    "#     test_acc3 =  (100 - MAPE(np.array( true_results), np.array(pred_results)))\n",
    "    \n",
    "    return test_acc1[0], test_acc2[0], pred_results, true_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edit(model):\n",
    "    testloader = DataLoader(testset,\n",
    "                           batch_size=8,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    print('start')\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = 8\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 ==== #\n",
    "    for i in range(len(testloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================== #   \n",
    "\n",
    "    test_acc1 =  RMSE(np.array( true_results), np.array(pred_results))\n",
    "    test_acc2 =  R2(np.array( true_results), np.array(pred_results))\n",
    "#     test_acc3 =  (100 - MAPE(np.array( true_results), np.array(pred_results)))\n",
    "    print('end')\n",
    "    \n",
    "    return test_acc1[0], test_acc2[0], pred_results, true_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkkF0-qmeOMq"
   },
   "source": [
    "# 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EOe2j_udjNv"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "#     model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model = LSTM_edit()\n",
    "    model.to(args.device)\n",
    "#     loss_fn = torch.nn.MSELoss() ##loss는 mse를 사용\n",
    "#     loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if args.loss == 'MSELoss':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif args.loss == 'L1Loss':\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif args.loss == 'PoissonNLLLoss':\n",
    "        loss_fn = torch.nn.PoissonNLLLoss()\n",
    "        loss_fn = nn.PoissonNLLLoss()\n",
    "    elif args.loss == 'KLDivLoss':\n",
    "        loss_fn = torch.nn.KLDivLoss()\n",
    "        loss_fn = nn.KLDivLoss()\n",
    "    elif args.loss == 'BCELoss':\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        loss_fn = nn.BCELoss()\n",
    "    elif args.loss == 'BCEWithLogitsLoss':\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError('In-valid LossFuction choice')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs_RMSE = []\n",
    "    train_accs_R2 = []\n",
    "    val_accs_RMSE = []\n",
    "    val_accs_R2 = []\n",
    "    axis = []\n",
    "    # ===================================== #\n",
    "    \n",
    "    ## model starting point ##    \n",
    "    ts = time.time()\n",
    "    model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "    val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "    te = time.time()\n",
    "\n",
    "    # ====== Add Epoch Data ====== #\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs_RMSE.append(train_acc_RMSE)\n",
    "    val_accs_RMSE.append(val_acc_RMSE)\n",
    "    train_accs_R2.append(train_acc_R2)\n",
    "    val_accs_R2.append(val_acc_R2)\n",
    "    # ============================ #\n",
    "\n",
    "    # # ===== Visdom visualizing ================================================================================== #\n",
    "    axis.append(0)\n",
    "    \n",
    "    plot1 = vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_loss'.format(num), legend=['train_loss','val_loss'], showlegend=True))\n",
    "    \n",
    "    plot2 = vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_acc_RMSE'.format(num), legend=['train_acc','val_acc'], showlegend=True))\n",
    "    \n",
    "    plot3 = vis.line(Y=torch.cat((torch.Tensor(train_accs_R2).view(-1,1), torch.Tensor(val_accs_R2).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_acc_R2'.format(num), legend=['train_acc','val_acc'], showlegend=True))    \n",
    "    # # =========================================================================================================== #\n",
    "    \n",
    "    print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "          .format(0, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "    \n",
    "    for epoch in range(args.epoch-1):  # loop over the dataset multiple times\n",
    "        \n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "\n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs_RMSE.append(train_acc_RMSE)\n",
    "        val_accs_RMSE.append(val_acc_RMSE)\n",
    "        train_accs_R2.append(train_acc_R2)\n",
    "        val_accs_R2.append(val_acc_R2)\n",
    "        # ============================ #\n",
    "\n",
    "        # # ===== Visdom visualizing ============================================================================== #\n",
    "        axis.append(epoch+1)\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot1, update='replace')\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot2, update='replace')\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_accs_R2).view(-1,1), torch.Tensor(val_accs_R2).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot3, update='replace')\n",
    "        # # ====================================================================================================== #\n",
    "        \n",
    "        print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch+1, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc_RMSE, test_acc_R2, Pred_data, True_data = test_edit(model)\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    \n",
    "    result['train_accs_RMSE'] = train_accs_RMSE\n",
    "    result['train_accs_R2'] = train_accs_R2\n",
    "    result['val_accs_RMSE'] = val_accs_RMSE\n",
    "    result['val_accs_R2'] = val_accs_R2\n",
    "#     result['train_acc'] = train_acc\n",
    "#     result['val_acc'] = val_acc\n",
    "    result['test_RMSE'] = test_acc_RMSE\n",
    "    result['test_R2'] = test_acc_R2\n",
    "    \n",
    "    result['test_pred'] = Pred_data\n",
    "    result['test_true'] = True_data\n",
    "    \n",
    "    network = model\n",
    "    torch.save(network.state_dict(),'lstm[{}].pt'.format(num))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNi-f5HyeJYi"
   },
   "source": [
    "# 6. LSTM Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4Bu6kFUdizj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size of trainset :280\n",
      " size of valset :61\n",
      " size of testset :61\n",
      " total_exp_num : 8\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 8\n",
    "args.UpData = UpData\n",
    "args.DownData = DownData\n",
    "args.x_frames = 4\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 16\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001\n",
    "args.dropout = 0.1 \n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam'  #SGD, RMSprop, Adam...\n",
    "args.loss = 'MSELoss'#'MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss'\n",
    "args.lr = 0.01\n",
    "args.epoch = 200\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'x_frames'\n",
    "list_var1 = [4]\n",
    "\n",
    "name_var2 = 'loss'\n",
    "list_var2 = ['MSELoss']\n",
    "\n",
    "name_var3 = 'optim'\n",
    "list_var3 = ['Adam']\n",
    "\n",
    "name_var4 = 'use_bn'\n",
    "list_var4 = [False]\n",
    "\n",
    "name_var5 = 'dropout'\n",
    "list_var5 = [0.1]\n",
    "\n",
    "name_var6 = 'batch_size'\n",
    "list_var6 = [8]\n",
    "\n",
    "name_var7 = 'hid_dim'\n",
    "list_var7 = [16]\n",
    "\n",
    "name_var8 = 'n_layers'\n",
    "list_var8 = [2]\n",
    "\n",
    "name_var9 = 'lr'\n",
    "list_var9 = [0.01]\n",
    "\n",
    "name_var10 = 'l2'\n",
    "list_var10 = [0.0001]\n",
    "\n",
    "name_var11 = 'epoch'\n",
    "list_var11 = [91,91,91,91,91,91,91,89]\n",
    "\n",
    "trainset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2018-06-30')\n",
    "valset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2018-07-01', '2019-12-31')\n",
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2018-07-01', '2019-12-31')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "print(' size of trainset :{}\\n'.format(len(trainset)),\n",
    "      'size of valset :{}\\n'.format(len(valset)),\n",
    "      'size of testset :{}'.format(len(testset)))\n",
    "\n",
    "list_vars = [list_var1, list_var2, list_var3, list_var4, list_var5, list_var6, list_var7, list_var8, list_var9, list_var10, list_var11]\n",
    "i = 1\n",
    "for lenth in list_vars:\n",
    "    x = len(lenth)\n",
    "    i *= x\n",
    "total_exp_num = i\n",
    "\n",
    "print(' total_exp_num : {}'.format(total_exp_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/model_save'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/model_save/TP_41\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/model_save/TP_41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Start #####\n",
      "\n",
      " exp_1\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.05594/0.03151. Took 0.36 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03395/0.03025. Took 0.36 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03483/0.02967. Took 0.36 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03595/0.03045. Took 0.36 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03657/0.02912. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03528/0.02766. Took 0.37 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03507/0.03100. Took 0.37 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03374/0.02592. Took 0.35 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03164/0.02419. Took 0.36 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02912/0.02614. Took 0.36 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03039/0.02367. Took 0.37 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02993/0.02304. Took 0.34 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02836/0.02203. Took 0.37 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02730/0.02787. Took 0.36 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03032/0.02703. Took 0.34 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03023/0.02368. Took 0.36 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02720/0.02327. Took 0.34 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02333/0.02112. Took 0.36 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02565/0.02318. Took 0.34 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02446/0.02299. Took 0.36 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02452/0.02170. Took 0.34 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02249/0.02156. Took 0.37 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02136/0.02135. Took 0.35 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02057/0.02139. Took 0.36 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01896/0.02019. Took 0.34 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01735/0.01940. Took 0.37 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01588/0.01898. Took 0.35 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01493/0.01847. Took 0.36 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01420/0.01817. Took 0.34 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01425/0.01853. Took 0.36 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01402/0.01791. Took 0.35 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01242/0.01834. Took 0.35 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01384/0.01969. Took 0.34 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01442/0.01873. Took 0.36 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01323/0.01817. Took 0.36 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01268/0.01820. Took 0.34 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01259/0.01776. Took 0.37 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01269/0.01879. Took 0.34 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01267/0.01761. Took 0.36 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.01821. Took 0.34 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01271/0.01769. Took 0.36 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01201/0.01754. Took 0.34 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01210/0.01813. Took 0.37 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01124/0.01755. Took 0.37 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01163/0.01873. Took 0.34 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01194/0.01793. Took 0.36 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01238/0.01748. Took 0.35 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01156/0.01952. Took 0.37 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01245/0.01742. Took 0.36 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01242/0.01914. Took 0.36 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01299/0.01737. Took 0.35 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01230/0.01771. Took 0.36 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01203/0.01737. Took 0.34 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01130/0.01793. Took 0.36 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01200/0.01729. Took 0.34 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01108/0.01821. Took 0.37 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01188/0.01749. Took 0.34 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01141/0.01816. Took 0.36 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01183/0.01818. Took 0.35 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01178/0.01712. Took 0.36 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01217/0.02003. Took 0.35 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01354/0.01896. Took 0.36 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01203/0.01856. Took 0.34 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01214/0.01751. Took 0.36 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01177/0.01830. Took 0.34 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.01787. Took 0.37 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01187/0.01908. Took 0.34 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01248/0.01744. Took 0.37 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01170/0.01801. Took 0.36 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01206/0.01924. Took 0.35 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01233/0.01768. Took 0.36 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01178/0.01799. Took 0.35 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01205/0.01967. Took 0.36 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01343/0.01847. Took 0.37 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01831. Took 0.35 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01173/0.01796. Took 0.37 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01184/0.01787. Took 0.34 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01163/0.01865. Took 0.36 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01195/0.01896. Took 0.35 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01203/0.01756. Took 0.37 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01113/0.01881. Took 0.35 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01194/0.01787. Took 0.36 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01184/0.01918. Took 0.34 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01190/0.01736. Took 0.37 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01104/0.01803. Took 0.36 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01113/0.01851. Took 0.34 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01208/0.01819. Took 0.37 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01177/0.01769. Took 0.34 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01149/0.01803. Took 0.37 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01130/0.01766. Took 0.34 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01136/0.01901. Took 0.37 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_2\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.04/0.02, Loss(train/val) 0.12273/0.03812. Took 0.37 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04936/0.03647. Took 0.35 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04606/0.03412. Took 0.40 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04494/0.03199. Took 0.39 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04076/0.03305. Took 0.35 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04192/0.03082. Took 0.36 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03854/0.03286. Took 0.35 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04229/0.03004. Took 0.37 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03725/0.03055. Took 0.35 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03875/0.03049. Took 0.38 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03660/0.02854. Took 0.36 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03259/0.02862. Took 0.36 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03367/0.02679. Took 0.35 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03061/0.02727. Took 0.37 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02898/0.02553. Took 0.36 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02822/0.02579. Took 0.36 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02513/0.02503. Took 0.35 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02677/0.02610. Took 0.36 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02738/0.02423. Took 0.36 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02551/0.02423. Took 0.37 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02518/0.02507. Took 0.36 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02563/0.02653. Took 0.37 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02413/0.02582. Took 0.35 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02549/0.02716. Took 0.37 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02332/0.02682. Took 0.35 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02334/0.02623. Took 0.36 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02339/0.02617. Took 0.36 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02377/0.02611. Took 0.37 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02395/0.02619. Took 0.35 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02311/0.02604. Took 0.37 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02424/0.02581. Took 0.35 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02363/0.02693. Took 0.37 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02427/0.02676. Took 0.36 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02354/0.02669. Took 0.36 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02328/0.02625. Took 0.36 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02334/0.02662. Took 0.37 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02322/0.02612. Took 0.35 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02297/0.02649. Took 0.36 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02340/0.02633. Took 0.36 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02316/0.02635. Took 0.37 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02243/0.02631. Took 0.35 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02322/0.02598. Took 0.37 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02309/0.02661. Took 0.35 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02265/0.02588. Took 0.37 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02312/0.02704. Took 0.36 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02275/0.02607. Took 0.36 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02303/0.02605. Took 0.35 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02178/0.02586. Took 0.37 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02271/0.02604. Took 0.35 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02058/0.02433. Took 0.36 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01902/0.02163. Took 0.35 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01857/0.02139. Took 0.36 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01793/0.02157. Took 0.36 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01701/0.02289. Took 0.36 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01715/0.02229. Took 0.36 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01730/0.02353. Took 0.36 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01688/0.02300. Took 0.35 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01639/0.02235. Took 0.36 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01570/0.02191. Took 0.36 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01542/0.02234. Took 0.38 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01548/0.02080. Took 0.36 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01493/0.02175. Took 0.36 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01464/0.02029. Took 0.35 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01418/0.02031. Took 0.37 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01417/0.02113. Took 0.35 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01477/0.02021. Took 0.37 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01483/0.02082. Took 0.36 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01432/0.01979. Took 0.37 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01406/0.01919. Took 0.35 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01348/0.01974. Took 0.36 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01437/0.02038. Took 0.36 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01398/0.01839. Took 0.37 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01341/0.01940. Took 0.35 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01325/0.01856. Took 0.37 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01330/0.01876. Took 0.35 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01258/0.01810. Took 0.36 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01429/0.01972. Took 0.35 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01279/0.01833. Took 0.37 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01308/0.01813. Took 0.36 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01314/0.01891. Took 0.36 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01260/0.01800. Took 0.36 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01260/0.02056. Took 0.37 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01286/0.01803. Took 0.35 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01235/0.01890. Took 0.37 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01216/0.01885. Took 0.36 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01824. Took 0.36 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01250/0.01924. Took 0.36 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01203/0.01884. Took 0.37 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01342/0.02020. Took 0.35 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01240/0.01848. Took 0.36 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01228/0.01843. Took 0.36 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_3\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04968/0.03323. Took 0.44 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04000/0.03287. Took 0.35 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04278/0.03193. Took 0.37 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03828/0.03197. Took 0.36 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03615/0.03066. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03589/0.02982. Took 0.34 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03008/0.02999. Took 0.36 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03031/0.02672. Took 0.37 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03268/0.02825. Took 0.35 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02864/0.02211. Took 0.39 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03190/0.02766. Took 0.37 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03030/0.02552. Took 0.35 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02840/0.02450. Took 0.39 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02818/0.02607. Took 0.37 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02926/0.02438. Took 0.35 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02670/0.02378. Took 0.37 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02687/0.02354. Took 0.37 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02416/0.02299. Took 0.36 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02396/0.02247. Took 0.37 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02315/0.02289. Took 0.37 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02216/0.02292. Took 0.35 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02032/0.02255. Took 0.39 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01874/0.02145. Took 0.38 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01842/0.02068. Took 0.35 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01657/0.01979. Took 0.37 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01592/0.02001. Took 0.38 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01493/0.01956. Took 0.36 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01459/0.01949. Took 0.37 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01458/0.02047. Took 0.37 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01377/0.02071. Took 0.36 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01492/0.02125. Took 0.37 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01397/0.01961. Took 0.38 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01345/0.01923. Took 0.34 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01362/0.02010. Took 0.38 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01374/0.02064. Took 0.38 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01362/0.02098. Took 0.39 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01368/0.01929. Took 0.39 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01270/0.01933. Took 0.41 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01252/0.01994. Took 0.36 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01201/0.01997. Took 0.37 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01274/0.01984. Took 0.37 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01215/0.01903. Took 0.37 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01241/0.01909. Took 0.36 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01237/0.01915. Took 0.37 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01241/0.01990. Took 0.35 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01229/0.02008. Took 0.37 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01190/0.01929. Took 0.37 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01229/0.02172. Took 0.35 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01252/0.01890. Took 0.37 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01278/0.02012. Took 0.37 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01245/0.01889. Took 0.35 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.02100. Took 0.38 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01243/0.02134. Took 0.37 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01245/0.01924. Took 0.36 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01191/0.01941. Took 0.38 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01986. Took 0.37 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.01951. Took 0.36 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01201/0.01948. Took 0.37 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01154/0.01991. Took 0.38 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01179/0.01921. Took 0.35 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01146/0.01958. Took 0.37 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01171/0.01972. Took 0.38 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01185/0.01949. Took 0.36 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01147/0.02034. Took 0.37 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01183/0.01924. Took 0.36 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01191/0.02011. Took 0.35 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01257/0.02079. Took 0.36 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01233/0.01879. Took 0.37 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01249/0.02046. Took 0.35 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01200/0.01931. Took 0.37 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01212/0.01945. Took 0.37 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01217/0.01987. Took 0.35 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01966. Took 0.37 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01940. Took 0.36 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01187/0.01990. Took 0.34 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01216/0.01957. Took 0.38 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01301/0.02011. Took 0.37 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01160/0.01926. Took 0.35 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01168/0.01943. Took 0.37 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01279/0.01989. Took 0.37 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.01896. Took 0.35 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01198/0.01898. Took 0.36 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01260/0.02064. Took 0.38 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01193/0.01937. Took 0.35 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01172/0.01896. Took 0.37 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01151/0.01920. Took 0.37 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01174/0.02010. Took 0.34 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01167/0.01877. Took 0.37 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01121/0.01906. Took 0.37 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.01873. Took 0.35 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01210/0.01995. Took 0.36 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_4\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03786/0.02959. Took 0.42 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03498/0.03095. Took 0.35 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03595/0.02412. Took 0.36 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03406/0.02693. Took 0.37 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03208/0.02558. Took 0.37 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03303/0.02876. Took 0.36 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03553/0.02670. Took 0.37 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03225/0.02179. Took 0.38 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03087/0.02162. Took 0.39 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02964/0.02229. Took 0.37 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02822/0.02250. Took 0.36 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02781/0.02192. Took 0.37 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02528/0.02169. Took 0.37 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02488/0.02197. Took 0.38 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02316/0.02219. Took 0.39 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02297/0.02293. Took 0.37 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02208/0.02213. Took 0.37 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02042/0.02169. Took 0.37 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02055/0.02194. Took 0.37 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01878/0.02052. Took 0.37 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01737/0.02069. Took 0.38 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01593/0.01942. Took 0.36 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01508/0.01940. Took 0.36 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01434/0.01833. Took 0.37 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01358/0.01934. Took 0.36 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01328/0.01878. Took 0.36 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01369/0.01973. Took 0.37 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01321/0.01913. Took 0.36 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01292/0.01861. Took 0.36 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01261/0.01936. Took 0.37 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01294/0.01889. Took 0.36 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01344/0.01848. Took 0.36 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01202/0.01922. Took 0.37 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01236/0.01841. Took 0.36 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01204/0.01907. Took 0.34 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01209/0.01852. Took 0.37 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01163/0.01870. Took 0.36 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01156/0.01913. Took 0.37 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01187/0.02083. Took 0.36 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01201/0.01842. Took 0.36 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01182/0.01826. Took 0.36 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01171/0.01906. Took 0.37 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01163/0.01783. Took 0.36 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01229/0.02001. Took 0.36 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01285/0.01801. Took 0.35 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01096/0.01877. Took 0.37 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01095/0.01780. Took 0.36 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01100/0.01847. Took 0.36 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01123/0.01872. Took 0.36 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01062/0.01774. Took 0.36 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01136/0.01889. Took 0.37 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01147/0.01843. Took 0.36 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01066/0.01798. Took 0.37 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01067/0.01917. Took 0.37 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01086/0.01826. Took 0.37 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01097/0.01790. Took 0.36 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01134/0.01927. Took 0.37 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01089/0.01792. Took 0.35 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01084/0.01825. Took 0.37 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01077/0.01764. Took 0.37 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01153/0.01874. Took 0.37 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01098/0.01785. Took 0.36 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01200/0.01833. Took 0.37 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01110/0.01851. Took 0.37 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01071/0.01791. Took 0.36 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01054/0.01769. Took 0.37 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01105/0.01822. Took 0.36 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01044/0.01772. Took 0.36 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01133/0.01920. Took 0.37 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01180/0.01871. Took 0.36 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01130/0.01857. Took 0.36 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01047/0.01796. Took 0.36 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00984/0.01791. Took 0.37 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01103/0.01881. Took 0.36 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01070/0.01766. Took 0.37 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01107/0.01860. Took 0.36 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01058/0.01765. Took 0.36 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01068/0.01846. Took 0.39 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01106/0.01931. Took 0.40 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01103/0.01768. Took 0.38 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00999/0.01792. Took 0.36 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01195/0.01869. Took 0.36 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01091/0.01782. Took 0.38 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01072/0.01779. Took 0.38 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01097/0.02030. Took 0.39 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01097/0.01770. Took 0.36 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01154/0.01988. Took 0.37 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01056/0.01786. Took 0.39 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01159/0.01879. Took 0.37 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01155/0.01935. Took 0.37 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01110/0.01801. Took 0.36 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_5\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.06745/0.03671. Took 0.42 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04334/0.03393. Took 0.39 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04415/0.03204. Took 0.39 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04144/0.03243. Took 0.37 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04071/0.03202. Took 0.38 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03797/0.03216. Took 0.37 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03820/0.03209. Took 0.37 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03692/0.03223. Took 0.37 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03481/0.03170. Took 0.39 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03384/0.03121. Took 0.36 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03355/0.03001. Took 0.37 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03398/0.03264. Took 0.36 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03821/0.03326. Took 0.36 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04039/0.03577. Took 0.37 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03265/0.02881. Took 0.35 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02566/0.02568. Took 0.36 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02701/0.02678. Took 0.36 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02849/0.02651. Took 0.36 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02717/0.02599. Took 0.36 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03064/0.02719. Took 0.34 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03077/0.02516. Took 0.36 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03026/0.02613. Took 0.36 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02692/0.02609. Took 0.36 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02683/0.02545. Took 0.37 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02680/0.02723. Took 0.35 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02583/0.02464. Took 0.37 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02686/0.02576. Took 0.37 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02567/0.02441. Took 0.37 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02558/0.02513. Took 0.36 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02550/0.02568. Took 0.34 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02573/0.02570. Took 0.37 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02593/0.02506. Took 0.38 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02552/0.02497. Took 0.38 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02567/0.02463. Took 0.37 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02445/0.02474. Took 0.35 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02466/0.02471. Took 0.37 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02439/0.02448. Took 0.37 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02419/0.02598. Took 0.37 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02451/0.02453. Took 0.38 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02374/0.02492. Took 0.35 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02427/0.02492. Took 0.37 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02336/0.02438. Took 0.38 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02413/0.02525. Took 0.37 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02402/0.02521. Took 0.38 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02468/0.02420. Took 0.37 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02309/0.02416. Took 0.37 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02349/0.02433. Took 0.37 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02286/0.02418. Took 0.37 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02146/0.02255. Took 0.37 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02016/0.02265. Took 0.39 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01876/0.02128. Took 0.38 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01829/0.02055. Took 0.37 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01715/0.01999. Took 0.39 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01603/0.01936. Took 0.37 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01483/0.01960. Took 0.36 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01468/0.01932. Took 0.37 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01393/0.01946. Took 0.37 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01452/0.01929. Took 0.38 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01459/0.01980. Took 0.37 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01396/0.01948. Took 0.37 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01360/0.01957. Took 0.37 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01377/0.01876. Took 0.38 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01258/0.01966. Took 0.35 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01329/0.01899. Took 0.35 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01321/0.01875. Took 0.37 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01271/0.01924. Took 0.36 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01312/0.01849. Took 0.35 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01250/0.01816. Took 0.35 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01215/0.01855. Took 0.35 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01257/0.01915. Took 0.34 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01286/0.01846. Took 0.35 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01195/0.01909. Took 0.34 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01295/0.01806. Took 0.34 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01222/0.01809. Took 0.35 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01245/0.01857. Took 0.34 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01266/0.01839. Took 0.36 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01359/0.01884. Took 0.35 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01315/0.01819. Took 0.36 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01243/0.01921. Took 0.37 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01242/0.01867. Took 0.37 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01290/0.01846. Took 0.37 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01231/0.01837. Took 0.36 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01194/0.01882. Took 0.38 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01218/0.01874. Took 0.37 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01277/0.01820. Took 0.36 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01182/0.01795. Took 0.36 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01187/0.01801. Took 0.35 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01157/0.01816. Took 0.34 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01203/0.01794. Took 0.34 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01140/0.01847. Took 0.34 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01275/0.01795. Took 0.34 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_6\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03585/0.02401. Took 0.46 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02815/0.02227. Took 0.38 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02675/0.02563. Took 0.37 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03372/0.02568. Took 0.38 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03430/0.02728. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03105/0.02865. Took 0.38 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03031/0.02508. Took 0.38 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03118/0.02582. Took 0.36 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02768/0.02709. Took 0.37 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02730/0.02503. Took 0.34 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02661/0.02599. Took 0.34 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02586/0.02537. Took 0.34 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02732/0.02607. Took 0.35 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02580/0.02435. Took 0.34 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02637/0.02578. Took 0.36 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02522/0.02474. Took 0.35 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02625/0.02601. Took 0.36 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02565/0.02514. Took 0.35 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02638/0.02594. Took 0.35 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02534/0.02579. Took 0.35 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02488/0.02591. Took 0.34 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02472/0.02557. Took 0.34 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02511/0.02614. Took 0.35 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02356/0.02579. Took 0.35 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02448/0.02548. Took 0.35 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02431/0.02593. Took 0.35 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02395/0.02554. Took 0.35 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02382/0.02570. Took 0.35 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02761/0.02674. Took 0.34 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02869/0.02702. Took 0.34 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02701/0.02756. Took 0.39 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02398/0.02610. Took 0.37 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02472/0.02711. Took 0.36 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02317/0.02654. Took 0.36 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02324/0.02590. Took 0.36 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02338/0.02591. Took 0.36 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02348/0.02626. Took 0.37 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02326/0.02654. Took 0.37 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02324/0.02702. Took 0.37 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02338/0.02611. Took 0.36 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02290/0.02693. Took 0.37 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02290/0.02677. Took 0.36 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02391/0.02659. Took 0.35 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02395/0.02690. Took 0.37 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02286/0.02653. Took 0.36 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02285/0.02722. Took 0.35 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02344/0.02650. Took 0.35 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02275/0.02782. Took 0.36 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02386/0.02484. Took 0.36 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02308/0.02720. Took 0.36 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02319/0.02677. Took 0.35 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02399/0.02934. Took 0.34 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02377/0.02579. Took 0.35 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01961/0.02344. Took 0.34 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01822/0.02141. Took 0.34 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01607/0.02495. Took 0.36 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01910/0.02409. Took 0.36 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01799/0.02090. Took 0.35 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01589/0.02269. Took 0.35 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01610/0.02310. Took 0.35 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01682/0.02255. Took 0.34 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01610/0.02242. Took 0.35 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01611/0.02170. Took 0.35 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01560/0.02115. Took 0.36 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01603/0.02000. Took 0.34 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01438/0.02071. Took 0.36 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01479/0.02002. Took 0.34 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01427/0.01974. Took 0.34 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01414/0.01983. Took 0.36 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01433/0.02018. Took 0.33 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01455/0.01964. Took 0.35 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01352/0.01964. Took 0.34 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01402/0.01919. Took 0.36 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01278/0.01942. Took 0.35 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01318/0.01968. Took 0.35 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01262/0.01964. Took 0.34 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01314/0.01808. Took 0.37 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01291/0.01874. Took 0.35 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01324/0.01816. Took 0.34 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01248/0.01903. Took 0.34 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01302/0.01886. Took 0.34 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01274/0.01863. Took 0.34 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01224/0.01888. Took 0.34 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01269/0.01911. Took 0.34 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01260/0.01865. Took 0.34 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01246/0.01947. Took 0.34 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01198/0.01858. Took 0.33 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.01939. Took 0.34 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01229/0.01796. Took 0.34 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01222/0.01858. Took 0.33 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01297/0.02036. Took 0.34 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_7\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=91, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.04/0.02, Loss(train/val) 0.09567/0.03267. Took 0.38 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03985/0.03569. Took 0.36 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04121/0.03581. Took 0.35 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.04125/0.03579. Took 0.34 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03974/0.03181. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03769/0.03020. Took 0.35 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03497/0.02910. Took 0.35 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03532/0.02905. Took 0.33 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03254/0.02719. Took 0.35 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03250/0.02782. Took 0.35 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03324/0.02661. Took 0.36 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03281/0.02573. Took 0.35 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03201/0.02588. Took 0.35 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02942/0.02503. Took 0.36 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03040/0.02509. Took 0.37 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.03008/0.02317. Took 0.35 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02824/0.02179. Took 0.34 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02619/0.02266. Took 0.36 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02606/0.02235. Took 0.35 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02391/0.02223. Took 0.34 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02259/0.02159. Took 0.35 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02092/0.02130. Took 0.34 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02037/0.02155. Took 0.35 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01988/0.02007. Took 0.35 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01795/0.01961. Took 0.34 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01679/0.01950. Took 0.35 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01647/0.01917. Took 0.36 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01492/0.01862. Took 0.34 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01445/0.01823. Took 0.35 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01326/0.01818. Took 0.35 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01337/0.01814. Took 0.35 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01293/0.01835. Took 0.34 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01361/0.01838. Took 0.36 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01253/0.01795. Took 0.35 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01236/0.01883. Took 0.35 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01298/0.01787. Took 0.35 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01208/0.01826. Took 0.35 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01277/0.01825. Took 0.35 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01215/0.01796. Took 0.38 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01208/0.01858. Took 0.37 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01252/0.01856. Took 0.39 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01256/0.01776. Took 0.35 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01964. Took 0.35 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01355/0.01767. Took 0.35 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01245/0.01809. Took 0.35 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01172/0.01760. Took 0.35 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01187/0.01820. Took 0.39 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01175/0.01784. Took 0.36 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01189/0.01826. Took 0.36 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01222/0.01761. Took 0.35 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01184/0.01850. Took 0.34 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01274/0.01834. Took 0.34 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01228/0.01798. Took 0.37 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01211/0.01862. Took 0.37 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01247/0.01906. Took 0.36 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01263/0.01808. Took 0.38 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01157/0.01789. Took 0.38 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01139/0.01841. Took 0.36 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01228/0.01859. Took 0.37 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.01864. Took 0.35 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01186/0.01764. Took 0.36 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01148/0.01791. Took 0.35 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01161/0.01882. Took 0.36 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01216/0.01995. Took 0.39 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01329/0.01846. Took 0.38 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01198/0.01872. Took 0.38 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01236/0.01829. Took 0.34 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01262/0.01836. Took 0.34 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01230/0.01803. Took 0.35 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.01903. Took 0.36 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01208/0.01795. Took 0.34 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01129/0.01799. Took 0.34 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01237/0.01945. Took 0.35 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01294/0.01817. Took 0.34 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01196/0.01766. Took 0.34 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.01772. Took 0.35 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01179/0.01835. Took 0.35 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.01886. Took 0.37 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01172/0.01784. Took 0.35 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01161/0.01857. Took 0.35 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01210/0.01803. Took 0.35 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01239/0.01894. Took 0.34 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01239/0.01839. Took 0.35 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.01875. Took 0.35 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01237/0.01780. Took 0.35 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01209/0.01838. Took 0.34 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01230/0.01799. Took 0.35 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01159/0.01832. Took 0.36 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01176/0.01792. Took 0.34 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01227/0.01814. Took 0.36 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01176/0.01813. Took 0.37 sec\n",
      "start\n",
      "end\n",
      "\n",
      " exp_8\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=16, epoch=89, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.06127/0.02848. Took 0.39 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03157/0.02721. Took 0.36 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02896/0.02260. Took 0.34 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02515/0.02137. Took 0.35 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02651/0.02560. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02841/0.02332. Took 0.36 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02697/0.02418. Took 0.34 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03022/0.02679. Took 0.35 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03148/0.02704. Took 0.35 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03159/0.02453. Took 0.34 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03116/0.02435. Took 0.35 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02956/0.02328. Took 0.34 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02977/0.02446. Took 0.34 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02698/0.02351. Took 0.34 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02689/0.02339. Took 0.34 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02492/0.02320. Took 0.34 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02351/0.02147. Took 0.33 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02165/0.02198. Took 0.34 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02142/0.02197. Took 0.34 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01909/0.02156. Took 0.35 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01904/0.02176. Took 0.36 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01840/0.02085. Took 0.35 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01683/0.01928. Took 0.35 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01413/0.01926. Took 0.34 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01379/0.01897. Took 0.34 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01292/0.01886. Took 0.35 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01234/0.01923. Took 0.34 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01344/0.01904. Took 0.34 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01230/0.01862. Took 0.34 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01303/0.01875. Took 0.34 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01177/0.01836. Took 0.35 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01320/0.01906. Took 0.34 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.01844. Took 0.34 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01212/0.01994. Took 0.34 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01206/0.01852. Took 0.34 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01313/0.01891. Took 0.35 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01225/0.01883. Took 0.35 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01229/0.01869. Took 0.35 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01190/0.01787. Took 0.35 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01172/0.01829. Took 0.34 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01198/0.01778. Took 0.35 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01273/0.01935. Took 0.35 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01191/0.01859. Took 0.35 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01273/0.01902. Took 0.35 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01282/0.01738. Took 0.35 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01129/0.01758. Took 0.34 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01156/0.01792. Took 0.36 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.01848. Took 0.38 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01205/0.01827. Took 0.35 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01135/0.02023. Took 0.35 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01455/0.01980. Took 0.34 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01479/0.01871. Took 0.34 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01191/0.02077. Took 0.38 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01423/0.01725. Took 0.35 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01175/0.01791. Took 0.34 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01119/0.01765. Took 0.34 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01217/0.01836. Took 0.38 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01343/0.01803. Took 0.35 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01271/0.01863. Took 0.34 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01336/0.01747. Took 0.36 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01218/0.01778. Took 0.35 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01191/0.01803. Took 0.35 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01348/0.01804. Took 0.36 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.01755. Took 0.36 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01105/0.01793. Took 0.35 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01158/0.01755. Took 0.36 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01165/0.01765. Took 0.36 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01243/0.01971. Took 0.35 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01262/0.01832. Took 0.35 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01080/0.01873. Took 0.35 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01141/0.01846. Took 0.35 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01046/0.01735. Took 0.35 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01065/0.01776. Took 0.36 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01038/0.01956. Took 0.36 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00999/0.01810. Took 0.36 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01105/0.02024. Took 0.35 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01090/0.01767. Took 0.35 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01121/0.02064. Took 0.36 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01184/0.01705. Took 0.36 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01082/0.01786. Took 0.35 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01092/0.01974. Took 0.34 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01190/0.02114. Took 0.35 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01099/0.01960. Took 0.35 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01065/0.01973. Took 0.35 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00997/0.02037. Took 0.35 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01065/0.01837. Took 0.34 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00957/0.02052. Took 0.33 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01026/0.01792. Took 0.36 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01055/0.01734. Took 0.34 sec\n",
      "start\n",
      "end\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print('##### Start #####')\n",
    "\n",
    "num = 0 #초기화\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        for var3 in list_var3:\n",
    "            for var4 in list_var4:\n",
    "                for var5 in list_var5:\n",
    "                    for var6 in list_var6:\n",
    "                        for var7 in list_var7:\n",
    "                            for var8 in list_var8:\n",
    "                                for var9 in list_var9:\n",
    "                                    for var10 in list_var10:\n",
    "                                        for var11 in list_var11:\n",
    "                                            ts = time.time()\n",
    "                                            num += 1\n",
    "                                            setattr(args, name_var1, var1)\n",
    "                                            setattr(args, name_var2, var2)\n",
    "                                            setattr(args, name_var3, var3)\n",
    "                                            setattr(args, name_var4, var4)\n",
    "                                            setattr(args, name_var5, var5)\n",
    "                                            setattr(args, name_var6, var6)\n",
    "                                            setattr(args, name_var7, var7)\n",
    "                                            setattr(args, name_var8, var8)\n",
    "                                            setattr(args, name_var9, var9)\n",
    "                                            setattr(args, name_var10, var10)\n",
    "                                            setattr(args, name_var11, var11)\n",
    "                                            print('\\n exp_{}'.format(num))\n",
    "                                            print('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                                                  .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn))     \n",
    "\n",
    "                                            result = experiment(partition, deepcopy(args))\n",
    "\n",
    "                                    #         print('train_acc_RMSE = {:2.2f}%, val_acc_RMSE = {:2.2f}%, test_RMSE = {:2.2f}%, test_R2 = {:2.2f}%'\n",
    "                                    #               .format(result['train_acc_RMSE'],result['val_acc_RMSE'],result['test_RMSE'],result['test_R2']))\n",
    "\n",
    "                                            vis.text('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                                                     .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn),\n",
    "                                                     opts=dict(title='exp_{}_text'.format(num)))\n",
    "                                            # 만든 모델의 test 데이터 예측 시각화\n",
    "\n",
    "                                            predict = torch.Tensor(result['test_pred']).view(-1,1)\n",
    "                                            truth = torch.Tensor(result['test_true']).view(-1,1)\n",
    "                                            axis = torch.Tensor(range(len(result['test_pred']))).view(-1,1)\n",
    "\n",
    "                                            Y_axis = torch.cat((predict, truth), -1)\n",
    "                                            X_axis = torch.cat((axis, axis), -1)\n",
    "\n",
    "                                            vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_RMSE[{:2.3f}]_R2[{:2.3f}]'.format(num,result['test_RMSE'],result['test_R2']),\n",
    "                                                                                       legend=['predict','true'],\n",
    "                                                                                       showlegend=True,\n",
    "                                                                                       layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/jinsungpark/Desktop/model_save/TP_41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm[8].pt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2019-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "numberis = 8\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "modell = LSTM_edit()\n",
    "modell.load_state_dict(torch.load('lstm[{}].pt'.format(numberis)))\n",
    "RMSE_1, R2_1, Pred_1, True_1 = test_edit(modell)\n",
    "test_data = pd.DataFrame()\n",
    "test_data['test_pred'] = Pred_1\n",
    "test_data['test_true'] = True_1\n",
    "test_data.to_csv('modell_result_[{}]_all.csv'.format(numberis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
