{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "from copy import deepcopy # Add Deepcopy for args\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Datareader Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jinsungpark/Desktop/Data_river/data04'\n",
      "/Users/jinsungpark/Desktop/jupyter\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynYVJoGrcnQS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/jupyter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTiRk82qctLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation_analysis.ipynb\r\n",
      "Data_Preprocessing.ipynb\r\n",
      "\u001b[34mData_river\u001b[m\u001b[m/\r\n",
      "Lab10_Stock_Price_Prediction_with_LSTM.ipynb\r\n",
      "Pandas_tutorial.ipynb\r\n",
      "\u001b[34mProject_Git\u001b[m\u001b[m/\r\n",
      "Project_River_Backup.ipynb\r\n",
      "Project_River_matplotlib.ipynb\r\n",
      "Project_River_matplotlib_01.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "\u001b[34mresults\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls #현재경로에 있는 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/jupyter/Data_river/data04\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/jupyter/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dln4LnKpc1CX"
   },
   "outputs": [],
   "source": [
    "UpStream_data = pd.read_excel('DS_Data_edit_log.xlsx', index = False)\n",
    "DownStream_data = pd.read_excel('NG_Data_edit_log.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GRAwoM_c068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date     DS_DO    DS_BOD    DS_COD     DS_SS     DS_TN     DS_TP  \\\n",
      "0    2013-01-07  2.782630  0.523025  1.439094  1.488231  1.187818 -3.488904   \n",
      "1    2013-01-14  2.785011  0.530628  1.435085  1.386294  1.187538 -3.506558   \n",
      "2    2013-01-21  2.740840  0.641854  1.504077  2.186051  1.230225 -3.575551   \n",
      "3    2013-01-28  2.681022  0.336472  1.458615  1.308333  1.298555 -3.270169   \n",
      "4    2013-02-04  2.694627 -0.356675  1.280934  1.629241  1.242424 -3.270169   \n",
      "..          ...       ...       ...       ...       ...       ...       ...   \n",
      "344  2019-09-02  2.197225  0.470004  1.648659  1.609438  0.251537 -3.442019   \n",
      "345  2019-09-09  2.186051  0.530628  1.648659  1.722767  0.121332 -3.912023   \n",
      "346  2019-09-16  2.041220  0.530628  1.722767  2.219203  0.131905 -3.442019   \n",
      "347  2019-09-24  2.219203  0.587787  1.916923  3.054001  0.723191 -2.128632   \n",
      "348  2019-09-30  2.240710  0.641854  2.054124  2.104134  0.909870 -2.385967   \n",
      "\n",
      "     DS_Chl_a  DS_Cells   GJ_Deep  ...  GumHo_DO  GumHo_BOD  GumHo_COD  \\\n",
      "0    3.024652  2.890372  2.377693  ...  2.708050  -0.356675   1.808289   \n",
      "1    3.025291  1.252763  2.371178  ...  2.564949   0.262364   1.856298   \n",
      "2    3.173878  1.252763  2.364620  ...  2.557227   1.280934   2.104134   \n",
      "3    2.424803  1.252763  2.365560  ...  2.624669   0.530628   1.960095   \n",
      "4    2.028148  1.252763  2.408745  ...  2.541602   0.788457   1.974081   \n",
      "..        ...       ...       ...  ...       ...        ...        ...   \n",
      "344  2.949688  9.148997  2.328253  ...  2.066863   0.405465   1.871802   \n",
      "345  3.317816  9.224243  2.330200  ...  2.041220   0.405465   1.856298   \n",
      "346  3.735286  9.268515  2.329227  ...  2.261763   0.741937   1.856298   \n",
      "347  2.041220  4.867534  2.330200  ...  2.174752   1.098612   2.459589   \n",
      "348  3.044522  6.961296  2.334084  ...  2.154913   0.866772   2.143823   \n",
      "\n",
      "     GumHo_SS  GumHo_TN  GumHo_TP  GumHo_Chl_a  GumHo_Flow    DS_Temp  \\\n",
      "0    0.955511  2.305281 -2.617296     1.481605    1.634521   2.570125   \n",
      "1    1.458615  2.227862 -2.476938     2.493205    2.827669   2.600000   \n",
      "2    2.041220  2.182111 -2.017406     2.970414    1.679524   2.300000   \n",
      "3    1.064711  2.022342 -2.577022     1.808289    1.862218   2.200000   \n",
      "4    1.960095  1.973525 -2.501036     3.034953    3.438461   1.900000   \n",
      "..        ...       ...       ...          ...         ...        ...   \n",
      "344  2.639057  1.430311 -2.419119     2.701361    3.886500  26.400000   \n",
      "345  1.722767  1.243290 -2.171557     2.091864    3.543854  26.000000   \n",
      "346  1.791759  1.335527 -2.513306     3.206803    2.987700  25.100000   \n",
      "347  3.737670  1.275083 -1.523260     3.218876    6.476557  21.600000   \n",
      "348  2.678838  1.283621 -2.004894     3.109321    4.682712  20.600000   \n",
      "\n",
      "     GumHo_Temp  \n",
      "0      2.500000  \n",
      "1      4.100000  \n",
      "2      6.600000  \n",
      "3      1.900000  \n",
      "4      5.500000  \n",
      "..          ...  \n",
      "344   23.300000  \n",
      "345   26.700000  \n",
      "346   25.700000  \n",
      "347   19.000000  \n",
      "348   20.286613  \n",
      "\n",
      "[349 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(UpStream_data)\n",
    "# print(DownStream_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [],
   "source": [
    "#날짜 인덱스화\n",
    "UpData = UpStream_data.set_index('Date')\n",
    "DownData = DownStream_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 43 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DS_DO              349 non-null    float64\n",
      " 1   DS_BOD             349 non-null    float64\n",
      " 2   DS_COD             349 non-null    float64\n",
      " 3   DS_SS              349 non-null    float64\n",
      " 4   DS_TN              349 non-null    float64\n",
      " 5   DS_TP              349 non-null    float64\n",
      " 6   DS_Chl_a           349 non-null    float64\n",
      " 7   DS_Cells           349 non-null    float64\n",
      " 8   GJ_Deep            349 non-null    float64\n",
      " 9   GJ_Level           349 non-null    float64\n",
      " 10  GJ_Outflow         349 non-null    float64\n",
      " 11  DaeGu_Rain         349 non-null    float64\n",
      " 12  DaeGu_Solar        349 non-null    float64\n",
      " 13  SeoBu_COD          349 non-null    float64\n",
      " 14  SeoBu_SS           349 non-null    float64\n",
      " 15  SeoBu_TN           349 non-null    float64\n",
      " 16  SeoBu_TP           349 non-null    float64\n",
      " 17  SeoBu_Flow_mean    349 non-null    float64\n",
      " 18  SeoBu_Flow_day     349 non-null    float64\n",
      " 19  SeoBu_COD_load     349 non-null    float64\n",
      " 20  SeoBu_SS_load      349 non-null    float64\n",
      " 21  SeoBu_TN_load      349 non-null    float64\n",
      " 22  SeoBu_TP_load      349 non-null    float64\n",
      " 23  SungSeo_COD        349 non-null    float64\n",
      " 24  SungSeo_SS         349 non-null    float64\n",
      " 25  SungSeo_TN         349 non-null    float64\n",
      " 26  SungSeo_TP         349 non-null    float64\n",
      " 27  SungSeo_Flow_mean  349 non-null    float64\n",
      " 28  SungSeo_Flow_day   349 non-null    float64\n",
      " 29  SungSeo_COD_load   349 non-null    float64\n",
      " 30  SungSeo_SS_load    349 non-null    float64\n",
      " 31  SungSeo_TN_load    349 non-null    float64\n",
      " 32  SungSeo_TP_load    349 non-null    float64\n",
      " 33  GumHo_DO           349 non-null    float64\n",
      " 34  GumHo_BOD          349 non-null    float64\n",
      " 35  GumHo_COD          349 non-null    float64\n",
      " 36  GumHo_SS           349 non-null    float64\n",
      " 37  GumHo_TN           349 non-null    float64\n",
      " 38  GumHo_TP           349 non-null    float64\n",
      " 39  GumHo_Chl_a        349 non-null    float64\n",
      " 40  GumHo_Flow         349 non-null    float64\n",
      " 41  DS_Temp            349 non-null    float64\n",
      " 42  GumHo_Temp         349 non-null    float64\n",
      "dtypes: float64(43)\n",
      "memory usage: 120.0+ KB\n"
     ]
    }
   ],
   "source": [
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DS_DO', 'DS_COD', 'DS_SS', 'DS_TP', 'GJ_Outflow', 'DaeGu_Rain',\n",
      "       'SungSeo_Flow_day', 'SungSeo_COD_load', 'GumHo_DO', 'GumHo_SS',\n",
      "       'GumHo_TP', 'GumHo_Flow', 'DS_Temp', 'GumHo_Temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#넣고싶은 상류 항목 컬럼 선택\n",
    "UpData = UpData.iloc[:,[0,2,3,5,10,11,28,29,33,36,38,40,41,42]]\n",
    "print(UpData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   NG_DO     349 non-null    float64\n",
      " 1   NG_BOD    349 non-null    float64\n",
      " 2   NG_COD    349 non-null    float64\n",
      " 3   NG_SS     349 non-null    float64\n",
      " 4   NG_TN     349 non-null    float64\n",
      " 5   NG_TP     349 non-null    float64\n",
      " 6   NG_Chl_a  349 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "DownData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG_TP\n"
     ]
    }
   ],
   "source": [
    "#알고싶은 하류 항목 컬럼 넘버 넣기('Date'항목이 인덱스화 돼서 컬럼 넘버가 -1씩 됨)\n",
    "Colum = 5\n",
    "print(DownData.columns[Colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Tp4o0Hc0ZL"
   },
   "source": [
    "### 1.2 Data Preprocessing(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChKYCAtTdGpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "UpScaler = MinMaxScaler() #상류데이터용\n",
    "DownScaler = MinMaxScaler() #하류데이터용\n",
    "\n",
    "#나중에 결과를 DeNormalizing 하기 위해 나누어 사용 하였다.\n",
    "\n",
    "def DeNormalize(Y, Data_name, column_num, Scaler_Type):\n",
    "    \n",
    "    data = Data_name\n",
    "    Scaler = Scaler_Type\n",
    "    \n",
    "    _max = Scaler.data_max_[column_num] # 역정규화 하려는 데이터의 컬럼 번호\n",
    "    _min = Scaler.data_min_[column_num] \n",
    "    \n",
    "    X = Y*(_max-_min) + _min\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_DO               0\n",
      "DS_COD              0\n",
      "DS_SS               0\n",
      "DS_TP               0\n",
      "GJ_Outflow          0\n",
      "DaeGu_Rain          0\n",
      "SungSeo_Flow_day    0\n",
      "SungSeo_COD_load    0\n",
      "GumHo_DO            0\n",
      "GumHo_SS            0\n",
      "GumHo_TP            0\n",
      "GumHo_Flow          0\n",
      "DS_Temp             0\n",
      "GumHo_Temp          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#데이터 정규화\n",
    "UpData = pd.DataFrame(UpScaler.fit_transform(UpData), columns=UpData.columns, index=UpData.index)\n",
    "DownData = pd.DataFrame(DownScaler.fit_transform(DownData), columns=DownData.columns, index=DownData.index)\n",
    "\n",
    "print(UpData.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiverDataset(Dataset):\n",
    "    def __init__(self, UpData, DownData, x_frames, y_frames, start, end):\n",
    "        \n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.UpData = UpData[start:end]\n",
    "        self.DownData = DownData[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.UpData) - (self.x_frames + self.y_frames) + 1\n",
    "    #데이터를 전처리 할때 UpData와 DownData의 길이가 동일해짐(날짜를 동일한것만 추출해야 하므로), 따라서 전체길이는 둘중 하나를 사용\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "\n",
    "        X = self.UpData.iloc[idx-self.x_frames:idx].values\n",
    "        y = self.DownData.iloc[idx:idx+self.y_frames].values\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metric(y_pred, y_true):\n",
    "#     perc_y_pred = y_pred.cpu().detach().numpy()\n",
    "#     perc_y_true = y_true.cpu().detach().numpy()\n",
    "#     mae = mean_absolute_error(perc_y_true, perc_y_pred, multioutput='raw_values')\n",
    "#     return mae*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metric(y_pred, y_true):\n",
    "#     perc_y_pred = y_pred.cpu().detach().numpy()\n",
    "#     perc_y_true = y_true.cpu().detach().numpy()\n",
    "#     mse = mean_squared_error(perc_y_true, perc_y_pred, multioutput='raw_values')\n",
    "#     return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_pred, y_true):\n",
    "    perc_y_pred = y_pred.cpu().detach().numpy()\n",
    "    perc_y_true = y_true.cpu().detach().numpy()\n",
    "    \n",
    "    perc_y_pred = np.exp(DeNormalize(perc_y_pred, DownData, Colum, DownScaler))\n",
    "    perc_y_true = np.exp(DeNormalize(perc_y_true, DownData, Colum, DownScaler))\n",
    "    \n",
    "    mse = mean_squared_error(perc_y_true, perc_y_pred, multioutput='raw_values')\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(partition['train'], \n",
    "                             batch_size=args.batch_size, \n",
    "                             shuffle=True, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.transpose(0, 1).float().to(args.device)\n",
    "        y_true = y[:, :, Colum].float().to(args.device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += metric(y_pred, y_true)[0]\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += metric(y_pred, y_true)[0]\n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc = val_acc / len(valloader)\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'], \n",
    "                           batch_size=args.batch_size, \n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            test_acc += metric(y_pred, y_true)[0]\n",
    "\n",
    "    test_acc = test_acc / len(testloader)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    # ===================================== #\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        # ============================ #\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc = test(model, partition, args)    \n",
    "\n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    print(type(result))\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(var1, var2, df):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
    "\n",
    "\n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp2_lr\"\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 2\n",
    "args.x_frames = 5\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 25\n",
    "args.n_layers = 1\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.00001\n",
    "args.dropout = 0.1\n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0001\n",
    "args.epoch = 250\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'hid_dim'\n",
    "name_var2 = 'n_layers'\n",
    "list_var1 = [25, 50, 75]\n",
    "list_var2 = [1,2,3]\n",
    "\n",
    "\n",
    "trainset = RiverDataset(UpData, DownData, args.x_frames, args.y_frames, '2013-01-01', '2017-12-31')\n",
    "valset = RiverDataset(UpData, DownData, args.x_frames, args.y_frames, '2018-01-01', '2018-12-31')\n",
    "testset = RiverDataset(UpData, DownData, args.x_frames, args.y_frames, '2019-01-01', '2019-12-31')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result = experiment(partition, deepcopy(args))\n",
    "        save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = 'hid_dim'\n",
    "var2 = 'n_layers'\n",
    "df = load_exp_result('exp1')\n",
    "\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
    "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
