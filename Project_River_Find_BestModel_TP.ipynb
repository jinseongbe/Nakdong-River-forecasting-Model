{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMKHTsedaum7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import argparse\n",
    "from copy import deepcopy #Add Deepcopy for args\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1gRd7qVcEwI"
   },
   "source": [
    "# 1. Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KShxDU_Jcj3u"
   },
   "source": [
    "### 1.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/jupyter/Data_river/data04\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/jupyter/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynYVJoGrcnQS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/jupyter/Data_river/data04'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTiRk82qctLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_Data_edit.xlsx      NG_Data_edit.xlsx      lstm02.pt\r\n",
      "DS_Data_edit_log.xlsx  NG_Data_edit_log.xlsx  \u001b[34mmodel_save\u001b[m\u001b[m/\r\n",
      "DS_data.xlsx           NG_data.xlsx           \u001b[34mresults\u001b[m\u001b[m/\r\n",
      "DS_to_NG.xlsx          UpData.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls #현재경로에 있는 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/jupyter/Data_river/data04\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/jupyter/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dln4LnKpc1CX"
   },
   "outputs": [],
   "source": [
    "UpStream_data = pd.read_excel('DS_Data_edit_log.xlsx')\n",
    "DownStream_data = pd.read_excel('NG_Data_edit_log.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GRAwoM_c068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'DS_DO', 'DS_BOD', 'DS_COD', 'DS_SS', 'DS_TN', 'DS_TP',\n",
      "       'DS_Chl_a', 'DS_Cells', 'GJ_Deep', 'GJ_Level', 'GJ_Outflow',\n",
      "       'DaeGu_Rain', 'DaeGu_Solar', 'SeoBu_COD', 'SeoBu_SS', 'SeoBu_TN',\n",
      "       'SeoBu_TP', 'SeoBu_Flow_mean', 'SeoBu_Flow_day', 'SeoBu_COD_load',\n",
      "       'SeoBu_SS_load', 'SeoBu_TN_load', 'SeoBu_TP_load', 'SungSeo_COD',\n",
      "       'SungSeo_SS', 'SungSeo_TN', 'SungSeo_TP', 'SungSeo_Flow_mean',\n",
      "       'SungSeo_Flow_day', 'SungSeo_COD_load', 'SungSeo_SS_load',\n",
      "       'SungSeo_TN_load', 'SungSeo_TP_load', 'GumHo_DO', 'GumHo_BOD',\n",
      "       'GumHo_COD', 'GumHo_SS', 'GumHo_TN', 'GumHo_TP', 'GumHo_Chl_a',\n",
      "       'GumHo_Flow', 'DS_Temp', 'GumHo_Temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(UpStream_data.columns)\n",
    "# print(DownStream_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [],
   "source": [
    "#날짜 인덱스화\n",
    "UpData = UpStream_data.set_index('Date')\n",
    "DownData = DownStream_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 43 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DS_DO              349 non-null    float64\n",
      " 1   DS_BOD             349 non-null    float64\n",
      " 2   DS_COD             349 non-null    float64\n",
      " 3   DS_SS              349 non-null    float64\n",
      " 4   DS_TN              349 non-null    float64\n",
      " 5   DS_TP              349 non-null    float64\n",
      " 6   DS_Chl_a           349 non-null    float64\n",
      " 7   DS_Cells           349 non-null    float64\n",
      " 8   GJ_Deep            349 non-null    float64\n",
      " 9   GJ_Level           349 non-null    float64\n",
      " 10  GJ_Outflow         349 non-null    float64\n",
      " 11  DaeGu_Rain         349 non-null    float64\n",
      " 12  DaeGu_Solar        349 non-null    float64\n",
      " 13  SeoBu_COD          349 non-null    float64\n",
      " 14  SeoBu_SS           349 non-null    float64\n",
      " 15  SeoBu_TN           349 non-null    float64\n",
      " 16  SeoBu_TP           349 non-null    float64\n",
      " 17  SeoBu_Flow_mean    349 non-null    float64\n",
      " 18  SeoBu_Flow_day     349 non-null    float64\n",
      " 19  SeoBu_COD_load     349 non-null    float64\n",
      " 20  SeoBu_SS_load      349 non-null    float64\n",
      " 21  SeoBu_TN_load      349 non-null    float64\n",
      " 22  SeoBu_TP_load      349 non-null    float64\n",
      " 23  SungSeo_COD        349 non-null    float64\n",
      " 24  SungSeo_SS         349 non-null    float64\n",
      " 25  SungSeo_TN         349 non-null    float64\n",
      " 26  SungSeo_TP         349 non-null    float64\n",
      " 27  SungSeo_Flow_mean  349 non-null    float64\n",
      " 28  SungSeo_Flow_day   349 non-null    float64\n",
      " 29  SungSeo_COD_load   349 non-null    float64\n",
      " 30  SungSeo_SS_load    349 non-null    float64\n",
      " 31  SungSeo_TN_load    349 non-null    float64\n",
      " 32  SungSeo_TP_load    349 non-null    float64\n",
      " 33  GumHo_DO           349 non-null    float64\n",
      " 34  GumHo_BOD          349 non-null    float64\n",
      " 35  GumHo_COD          349 non-null    float64\n",
      " 36  GumHo_SS           349 non-null    float64\n",
      " 37  GumHo_TN           349 non-null    float64\n",
      " 38  GumHo_TP           349 non-null    float64\n",
      " 39  GumHo_Chl_a        349 non-null    float64\n",
      " 40  GumHo_Flow         349 non-null    float64\n",
      " 41  DS_Temp            349 non-null    float64\n",
      " 42  GumHo_Temp         349 non-null    float64\n",
      "dtypes: float64(43)\n",
      "memory usage: 120.0+ KB\n"
     ]
    }
   ],
   "source": [
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [],
   "source": [
    "# #넣고싶은 상류 항목 컬럼 선택 - TP setting\n",
    "# UpData = UpData.iloc[:,[0,2,3,5,10,11,28,29,33,36,38,40,41,42]]\n",
    "# print(UpData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 33 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DS_DO              349 non-null    float64\n",
      " 1   DS_BOD             349 non-null    float64\n",
      " 2   DS_COD             349 non-null    float64\n",
      " 3   DS_SS              349 non-null    float64\n",
      " 4   DS_TN              349 non-null    float64\n",
      " 5   DS_TP              349 non-null    float64\n",
      " 6   DS_Chl_a           349 non-null    float64\n",
      " 7   DS_Cells           349 non-null    float64\n",
      " 8   GJ_Deep            349 non-null    float64\n",
      " 9   GJ_Level           349 non-null    float64\n",
      " 10  GJ_Outflow         349 non-null    float64\n",
      " 11  DaeGu_Rain         349 non-null    float64\n",
      " 12  DaeGu_Solar        349 non-null    float64\n",
      " 13  SeoBu_COD          349 non-null    float64\n",
      " 14  SeoBu_SS           349 non-null    float64\n",
      " 15  SeoBu_TN           349 non-null    float64\n",
      " 16  SeoBu_TP           349 non-null    float64\n",
      " 17  SeoBu_Flow_mean    349 non-null    float64\n",
      " 18  SungSeo_COD        349 non-null    float64\n",
      " 19  SungSeo_SS         349 non-null    float64\n",
      " 20  SungSeo_TN         349 non-null    float64\n",
      " 21  SungSeo_TP         349 non-null    float64\n",
      " 22  SungSeo_Flow_mean  349 non-null    float64\n",
      " 23  GumHo_DO           349 non-null    float64\n",
      " 24  GumHo_BOD          349 non-null    float64\n",
      " 25  GumHo_COD          349 non-null    float64\n",
      " 26  GumHo_SS           349 non-null    float64\n",
      " 27  GumHo_TN           349 non-null    float64\n",
      " 28  GumHo_TP           349 non-null    float64\n",
      " 29  GumHo_Chl_a        349 non-null    float64\n",
      " 30  GumHo_Flow         349 non-null    float64\n",
      " 31  DS_Temp            349 non-null    float64\n",
      " 32  GumHo_Temp         349 non-null    float64\n",
      "dtypes: float64(33)\n",
      "memory usage: 92.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#넣고싶은 상류 항목 컬럼 선택 - TP all setting\n",
    "UpData = UpData.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,23,24,25,26,27,33,34,35,36,37,38,39,40,41,42]]\n",
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   NG_DO     349 non-null    float64\n",
      " 1   NG_BOD    349 non-null    float64\n",
      " 2   NG_COD    349 non-null    float64\n",
      " 3   NG_SS     349 non-null    float64\n",
      " 4   NG_TN     349 non-null    float64\n",
      " 5   NG_TP     349 non-null    float64\n",
      " 6   NG_Chl_a  349 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "DownData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG_TP\n"
     ]
    }
   ],
   "source": [
    "#알고싶은 하류 항목 컬럼 넘버 넣기('Date'항목이 인덱스화 돼서 컬럼 넘버가 -1씩 됨)\n",
    "Colum = 5\n",
    "print(DownData.columns[Colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Tp4o0Hc0ZL"
   },
   "source": [
    "### 1.2 Data Preprocessing(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChKYCAtTdGpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "UpScaler = MinMaxScaler() #상류데이터용\n",
    "DownScaler = MinMaxScaler() #하류데이터용\n",
    "\n",
    "#나중에 결과를 DeNormalizing 하기 위해 나누어 사용 하였다.\n",
    "\n",
    "def DeNormalize(Y, Data_name, column_num, Scaler_Type):\n",
    "    \n",
    "    data = Data_name\n",
    "    Scaler = Scaler_Type\n",
    "    \n",
    "    _max = Scaler.data_max_[column_num] # 역정규화 하려는 데이터의 컬럼 번호\n",
    "    _min = Scaler.data_min_[column_num] \n",
    "    \n",
    "    X = Y*(_max-_min) + _min\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_DO                0\n",
      "DS_BOD               0\n",
      "DS_COD               0\n",
      "DS_SS                0\n",
      "DS_TN                0\n",
      "DS_TP                0\n",
      "DS_Chl_a             0\n",
      "DS_Cells             0\n",
      "GJ_Deep              0\n",
      "GJ_Level             0\n",
      "GJ_Outflow           0\n",
      "DaeGu_Rain           0\n",
      "DaeGu_Solar          0\n",
      "SeoBu_COD            0\n",
      "SeoBu_SS             0\n",
      "SeoBu_TN             0\n",
      "SeoBu_TP             0\n",
      "SeoBu_Flow_mean      0\n",
      "SungSeo_COD          0\n",
      "SungSeo_SS           0\n",
      "SungSeo_TN           0\n",
      "SungSeo_TP           0\n",
      "SungSeo_Flow_mean    0\n",
      "GumHo_DO             0\n",
      "GumHo_BOD            0\n",
      "GumHo_COD            0\n",
      "GumHo_SS             0\n",
      "GumHo_TN             0\n",
      "GumHo_TP             0\n",
      "GumHo_Chl_a          0\n",
      "GumHo_Flow           0\n",
      "DS_Temp              0\n",
      "GumHo_Temp           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#데이터 정규화\n",
    "UpData = pd.DataFrame(UpScaler.fit_transform(UpData), columns=UpData.columns, index=UpData.index)\n",
    "DownData = pd.DataFrame(DownScaler.fit_transform(DownData), columns=DownData.columns, index=DownData.index)\n",
    "\n",
    "print(UpData.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDSVZGiUdGYz"
   },
   "source": [
    "#2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hxm2moSudGQD"
   },
   "outputs": [],
   "source": [
    "class RiverDataset(Dataset):\n",
    "    def __init__(self, UpData, DownData, x_frames, y_frames, start, end):\n",
    "        \n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.UpData = UpData[start:end]\n",
    "        self.DownData = DownData[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.UpData) - (self.x_frames + self.y_frames) + 1\n",
    "    #데이터를 전처리 할때 UpData와 DownData의 길이가 동일해짐(날짜를 동일한것만 추출해야 하므로), 따라서 전체길이는 둘중 하나를 사용\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "\n",
    "        X = self.UpData.iloc[idx-self.x_frames:idx].values\n",
    "        Y = self.DownData.iloc[idx:idx+self.y_frames].values\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj80CahhdGG9"
   },
   "source": [
    "# 3. Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuBGEc51dF-V"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers) #\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTpV_o6Wdglq"
   },
   "outputs": [],
   "source": [
    "# 정확도 : 예측확률을 100%로 봤을때 MAPE에 따른 오차비율을 빼줌 (100-MAPE) ##RMSE, MAPE 두개로 볼 수 있게\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    R2_score = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    return R2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GPb8H8-djwB"
   },
   "source": [
    "# 4. Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrT2W-pqdjh6"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.transpose(0, 1).float().to(args.device)#파이토치는 순서가 달라서 바꿔줌\n",
    "        y_true = y[:, :, Colum].float().to(args.device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred.append(y_pred)\n",
    "        true.append(y_true)\n",
    "\n",
    "    # ========================================================================== #\n",
    "    for i in range(len(trainloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    train_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     train_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    return model, train_loss, train_acc1[0], train_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvAO-LVCdjgG"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "        # ========================================================================== #\n",
    "        for i in range(len(valloader)):\n",
    "            tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "            tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "\n",
    "            for j in range(bat_siz):\n",
    "                value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "                value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "\n",
    "                pred_results.append(value1)\n",
    "                true_results.append(value2)\n",
    "        # ========================================================================== #   \n",
    "\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    val_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     val_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    \n",
    "    return val_loss, val_acc1[0], val_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHWmu5EtdjXu"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # =================== test 데이터 시각화를 위해 x,y데이터 저장 =================== #\n",
    "    for i in range(len(testloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ======================================================================== #   \n",
    "\n",
    "    test_acc1 =  RMSE(np.array( true_results), np.array(pred_results))\n",
    "    test_acc2 =  R2(np.array( true_results), np.array(pred_results))\n",
    "#     test_acc3 =  (100 - MAPE(np.array( true_results), np.array(pred_results)))\n",
    "    \n",
    "    return test_acc1[0], test_acc2[0], pred_results, true_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkkF0-qmeOMq"
   },
   "source": [
    "# 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EOe2j_udjNv"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "\n",
    "    if args.loss == 'MSELoss':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif args.loss == 'L1Loss':\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif args.loss == 'PoissonNLLLoss':\n",
    "        loss_fn = torch.nn.PoissonNLLLoss()\n",
    "        loss_fn = nn.PoissonNLLLoss()\n",
    "    elif args.loss == 'KLDivLoss':\n",
    "        loss_fn = torch.nn.KLDivLoss()\n",
    "        loss_fn = nn.KLDivLoss()\n",
    "    elif args.loss == 'BCELoss':\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        loss_fn = nn.BCELoss()\n",
    "    elif args.loss == 'BCEWithLogitsLoss':\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError('In-valid LossFuction choice')\n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs_RMSE = []\n",
    "    train_accs_R2 = []\n",
    "    val_accs_RMSE = []\n",
    "    val_accs_R2 = []\n",
    "    result_info = pd.DataFrame()\n",
    "    result_data = pd.DataFrame()\n",
    "    \n",
    "    # ===================================== #\n",
    "    \n",
    "    ## model starting point ##    \n",
    "    ts = time.time()\n",
    "    model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "    val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "    te = time.time()\n",
    "\n",
    "    # ====== Add Epoch Data ====== #\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs_RMSE.append(train_acc_RMSE)\n",
    "    val_accs_RMSE.append(val_acc_RMSE)\n",
    "    train_accs_R2.append(train_acc_R2)\n",
    "    val_accs_R2.append(val_acc_R2)\n",
    "    # ============================ #\n",
    "\n",
    "\n",
    "#     print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "#           .format(0, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "    \n",
    "    for epoch in range(args.epoch-1):  # loop over the dataset multiple times\n",
    "        \n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "\n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs_RMSE.append(train_acc_RMSE)\n",
    "        val_accs_RMSE.append(val_acc_RMSE)\n",
    "        train_accs_R2.append(train_acc_R2)\n",
    "        val_accs_R2.append(val_acc_R2)\n",
    "        # ============================ #\n",
    "        \n",
    "        \n",
    "        if epoch == epoch:\n",
    "            trash_01, trash_02, Pred_save, True_save = test(model, partition, args)\n",
    "            result_data['test_pred_epoch({})'.format(epoch)] = Pred_save\n",
    "            result_data['test_true_epoch({})'.format(epoch)] = True_save\n",
    "\n",
    "\n",
    "#         print('Epoch {}, Acc_RMSE(train/val): {:2.4f}/{:2.4f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "#               .format(epoch+1, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc_RMSE, test_acc_R2, Pred_data, True_data = test(model, partition, args)    \n",
    "    \n",
    "    # ======= Add Result  ======= #\n",
    "    result_info['train_losses'] = train_losses\n",
    "    result_info['val_losses'] = val_losses\n",
    "    \n",
    "    result_info['train_accs_RMSE'] = train_accs_RMSE\n",
    "    result_info['train_accs_R2'] = train_accs_R2\n",
    "    result_info['val_accs_RMSE'] = val_accs_RMSE\n",
    "    result_info['val_accs_R2'] = val_accs_R2\n",
    "    \n",
    "#     result_info['train_acc'] = train_acc\n",
    "#     result_info['val_acc'] = val_acc\n",
    "    result_info['test_RMSE'] = test_acc_RMSE\n",
    "    result_info['test_R2'] = test_acc_R2\n",
    "\n",
    "#     result_data['test_pred'] = Pred_data\n",
    "#     result_data['test_true'] = True_data\n",
    "    \n",
    "    return result_info, result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNi-f5HyeJYi"
   },
   "source": [
    "# 6. LSTM Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4Bu6kFUdizj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size of trainset :178\n",
      " size of valset :39\n",
      " size of testset :39\n",
      " total_exp_num : 108\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 8\n",
    "args.UpData = UpData\n",
    "args.DownData = DownData\n",
    "args.x_frames = 5\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 16\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001\n",
    "args.dropout = 0.1 \n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam'  #SGD, RMSprop, Adam...\n",
    "args.loss = 'MSELoss'#'MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss'\n",
    "args.lr = 0.01\n",
    "args.epoch = 3\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'x_frames'\n",
    "list_var1 = [4]\n",
    "\n",
    "name_var2 = 'loss'\n",
    "list_var2 = ['MSELoss']\n",
    "\n",
    "name_var3 = 'optim'\n",
    "list_var3 = ['Adam']\n",
    "\n",
    "name_var4 = 'use_bn'\n",
    "list_var4 = [False]\n",
    "\n",
    "name_var5 = 'dropout'\n",
    "list_var5 = [0.1]\n",
    "\n",
    "name_var6 = 'batch_size'\n",
    "list_var6 = [4, 8]\n",
    "\n",
    "name_var7 = 'hid_dim'\n",
    "list_var7 = [32, 64]\n",
    "\n",
    "name_var8 = 'n_layers'\n",
    "list_var8 = [4, 3, 2]\n",
    "\n",
    "name_var9 = 'lr'\n",
    "list_var9 = [0.01, 0.001, 0.0001]\n",
    "\n",
    "name_var10 = 'l2'\n",
    "list_var10 = [0.01,0.001, 0.0001]\n",
    "\n",
    "name_var11 = 'epoch'\n",
    "list_var11 = [200]\n",
    "\n",
    "trainset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2016-07-15')\n",
    "valset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-07-16', '2017-05-19')\n",
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-07-16', '2017-05-19')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "print(' size of trainset :{}\\n'.format(len(trainset)),\n",
    "      'size of valset :{}\\n'.format(len(valset)),\n",
    "      'size of testset :{}'.format(len(testset)))\n",
    "\n",
    "list_vars = [list_var1, list_var2, list_var3, list_var4, list_var5, list_var6, list_var7, list_var8, list_var9, list_var10, list_var11]\n",
    "i = 1\n",
    "for lenth in list_vars:\n",
    "    x = len(lenth)\n",
    "    i *= x\n",
    "total_exp_num = i\n",
    "\n",
    "print(' total_exp_num : {}'.format(total_exp_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/jupyter/Data_river/data04'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/DSNG_ALL_TP\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/DSNG_ALL_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Start #####\n",
      "experiment_1/108, took 149.9sec, 0.93% done\n",
      "experiment_2/108, took 149.4sec, 1.85% done\n",
      "experiment_3/108, took 136.6sec, 2.78% done\n",
      "experiment_4/108, took 161.1sec, 3.70% done\n",
      "experiment_5/108, took 145.2sec, 4.63% done\n",
      "experiment_6/108, took 102.9sec, 5.56% done\n",
      "experiment_7/108, took 117.0sec, 6.48% done\n",
      "experiment_8/108, took 103.2sec, 7.41% done\n",
      "experiment_9/108, took 103.1sec, 8.33% done\n",
      "experiment_10/108, took 117.2sec, 9.26% done\n",
      "experiment_11/108, took 116.4sec, 10.19% done\n",
      "experiment_12/108, took 112.2sec, 11.11% done\n",
      "experiment_13/108, took 130.8sec, 12.04% done\n",
      "experiment_14/108, took 84.4sec, 12.96% done\n",
      "experiment_15/108, took 84.0sec, 13.89% done\n",
      "experiment_16/108, took 97.2sec, 14.81% done\n",
      "experiment_17/108, took 84.9sec, 15.74% done\n",
      "experiment_18/108, took 84.8sec, 16.67% done\n",
      "experiment_19/108, took 90.8sec, 17.59% done\n",
      "experiment_20/108, took 46.2sec, 18.52% done\n",
      "experiment_21/108, took 42.6sec, 19.44% done\n",
      "experiment_22/108, took 35.4sec, 20.37% done\n",
      "experiment_23/108, took 34.8sec, 21.30% done\n",
      "experiment_24/108, took 35.4sec, 22.22% done\n",
      "experiment_25/108, took 35.2sec, 23.15% done\n",
      "experiment_26/108, took 35.1sec, 24.07% done\n",
      "experiment_27/108, took 35.0sec, 25.00% done\n",
      "experiment_28/108, took 159.8sec, 25.93% done\n",
      "experiment_29/108, took 249.9sec, 26.85% done\n",
      "experiment_30/108, took 234.9sec, 27.78% done\n",
      "experiment_31/108, took 287.1sec, 28.70% done\n",
      "experiment_32/108, took 240.2sec, 29.63% done\n",
      "experiment_33/108, took 186.7sec, 30.56% done\n",
      "experiment_34/108, took 192.7sec, 31.48% done\n",
      "experiment_35/108, took 122.4sec, 32.41% done\n",
      "experiment_36/108, took 155.3sec, 33.33% done\n",
      "experiment_37/108, took 160.8sec, 34.26% done\n",
      "experiment_38/108, took 144.6sec, 35.19% done\n",
      "experiment_39/108, took 128.0sec, 36.11% done\n",
      "experiment_40/108, took 168.1sec, 37.04% done\n",
      "experiment_41/108, took 144.5sec, 37.96% done\n",
      "experiment_42/108, took 150.3sec, 38.89% done\n",
      "experiment_43/108, took 185.8sec, 39.81% done\n",
      "experiment_44/108, took 146.3sec, 40.74% done\n",
      "experiment_45/108, took 145.7sec, 41.67% done\n",
      "experiment_46/108, took 160.9sec, 42.59% done\n",
      "experiment_47/108, took 157.1sec, 43.52% done\n",
      "experiment_48/108, took 132.0sec, 44.44% done\n",
      "experiment_49/108, took 173.2sec, 45.37% done\n",
      "experiment_50/108, took 114.5sec, 46.30% done\n",
      "experiment_51/108, took 116.0sec, 47.22% done\n",
      "experiment_52/108, took 115.8sec, 48.15% done\n",
      "experiment_53/108, took 114.6sec, 49.07% done\n",
      "experiment_54/108, took 115.4sec, 50.00% done\n",
      "experiment_55/108, took 133.8sec, 50.93% done\n",
      "experiment_56/108, took 128.0sec, 51.85% done\n",
      "experiment_57/108, took 120.5sec, 52.78% done\n",
      "experiment_58/108, took 123.9sec, 53.70% done\n",
      "experiment_59/108, took 102.5sec, 54.63% done\n",
      "experiment_60/108, took 101.6sec, 55.56% done\n",
      "experiment_61/108, took 99.7sec, 56.48% done\n",
      "experiment_62/108, took 99.7sec, 57.41% done\n",
      "experiment_63/108, took 99.6sec, 58.33% done\n",
      "experiment_64/108, took 107.9sec, 59.26% done\n",
      "experiment_65/108, took 99.0sec, 60.19% done\n",
      "experiment_66/108, took 88.7sec, 61.11% done\n",
      "experiment_67/108, took 102.3sec, 62.04% done\n",
      "experiment_68/108, took 83.5sec, 62.96% done\n",
      "experiment_69/108, took 83.6sec, 63.89% done\n",
      "experiment_70/108, took 82.7sec, 64.81% done\n",
      "experiment_71/108, took 82.9sec, 65.74% done\n",
      "experiment_72/108, took 83.5sec, 66.67% done\n",
      "experiment_73/108, took 78.4sec, 67.59% done\n",
      "experiment_74/108, took 77.8sec, 68.52% done\n",
      "experiment_75/108, took 65.4sec, 69.44% done\n",
      "experiment_76/108, took 69.3sec, 70.37% done\n",
      "experiment_77/108, took 65.6sec, 71.30% done\n",
      "experiment_78/108, took 65.3sec, 72.22% done\n",
      "experiment_79/108, took 61.3sec, 73.15% done\n",
      "experiment_80/108, took 59.7sec, 74.07% done\n",
      "experiment_81/108, took 59.7sec, 75.00% done\n",
      "experiment_82/108, took 194.4sec, 75.93% done\n",
      "experiment_83/108, took 172.6sec, 76.85% done\n",
      "experiment_84/108, took 162.6sec, 77.78% done\n",
      "experiment_85/108, took 172.4sec, 78.70% done\n",
      "experiment_86/108, took 123.2sec, 79.63% done\n",
      "experiment_87/108, took 118.6sec, 80.56% done\n",
      "experiment_88/108, took 120.4sec, 81.48% done\n",
      "experiment_89/108, took 119.2sec, 82.41% done\n",
      "experiment_90/108, took 119.5sec, 83.33% done\n",
      "experiment_91/108, took 149.2sec, 84.26% done\n",
      "experiment_92/108, took 130.8sec, 85.19% done\n",
      "experiment_93/108, took 122.9sec, 86.11% done\n",
      "experiment_94/108, took 140.8sec, 87.04% done\n",
      "experiment_95/108, took 99.9sec, 87.96% done\n",
      "experiment_96/108, took 99.3sec, 88.89% done\n",
      "experiment_97/108, took 97.4sec, 89.81% done\n",
      "experiment_98/108, took 96.7sec, 90.74% done\n",
      "experiment_99/108, took 95.9sec, 91.67% done\n",
      "experiment_100/108, took 107.7sec, 92.59% done\n",
      "experiment_101/108, took 99.3sec, 93.52% done\n",
      "experiment_102/108, took 78.9sec, 94.44% done\n",
      "experiment_103/108, took 102.5sec, 95.37% done\n",
      "experiment_104/108, took 74.8sec, 96.30% done\n",
      "experiment_105/108, took 74.3sec, 97.22% done\n",
      "experiment_106/108, took 76.6sec, 98.15% done\n",
      "experiment_107/108, took 79.6sec, 99.07% done\n",
      "experiment_108/108, took 76.8sec, 100.00% done\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "print('##### Start #####')\n",
    "\n",
    "num = 0 #초기화\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        for var3 in list_var3:\n",
    "            for var4 in list_var4:\n",
    "                for var5 in list_var5:\n",
    "                    for var6 in list_var6:\n",
    "                        for var7 in list_var7:\n",
    "                            for var8 in list_var8:\n",
    "                                for var9 in list_var9:\n",
    "                                    for var10 in list_var10:\n",
    "                                        for var11 in list_var11:\n",
    "                                            ts = time.time()\n",
    "                                            num += 1\n",
    "                                            setattr(args, name_var1, var1)\n",
    "                                            setattr(args, name_var2, var2)\n",
    "                                            setattr(args, name_var3, var3)\n",
    "                                            setattr(args, name_var4, var4)\n",
    "                                            setattr(args, name_var5, var5)\n",
    "                                            setattr(args, name_var6, var6)\n",
    "                                            setattr(args, name_var7, var7)\n",
    "                                            setattr(args, name_var8, var8)\n",
    "                                            setattr(args, name_var9, var9)\n",
    "                                            setattr(args, name_var10, var10)\n",
    "                                            setattr(args, name_var11, var11)\n",
    "\n",
    "#                                             print('experiment_{}/{} : x_frames = {}, loss={}, optim={}, use_bn={}, dropout={}, batch_size={}, hid_dim={}, n_layers={}, lr={}, l2={}, epoch={}'\n",
    "#                                                   .format(num,total_exp_num,args.x_frames,args.loss,args.optim,args.use_bn,args.dropout,args.batch_size,args.hid_dim,args.n_layers,args.lr,args.l2,args.epoch))\n",
    "                                            result_info, result_data = experiment(partition, deepcopy(args))\n",
    "    \n",
    "                                            min_RMSE = min(result_info['val_accs_RMSE'])\n",
    "                                            max_R2 = max(result_info['val_accs_R2'])\n",
    "#                                             RMSE_ = result_info['val_accs_RMSE']\n",
    "#                                             R2_ = result_info['val_accs_R2']\n",
    "                                            epoch = args.epoch\n",
    "                                            \n",
    "                                            result_info.to_csv('exp{:03d}_info_RMSE[{:2.4f}]_R2[{:2.4f}].csv'.format(num,min_RMSE,max_R2))\n",
    "                                            result_data.to_csv('exp{:03d}_data_RMSE[{:2.4f}]_R2[{:2.4f}].csv'.format(num,min_RMSE,max_R2))\n",
    "                \n",
    "                                            file=open('exp_index.txt','a')\n",
    "                                            file.write('experiment {:03d}/{} : x_frames = {}, loss={}, optim={}, use_bn={}, dropout={}, batch_size={}, hid_dim={}, n_layers={}, lr={}, l2={}, epoch={}\\n'\n",
    "                                                       .format(num,total_exp_num,args.x_frames,args.loss,args.optim,args.use_bn,args.dropout,args.batch_size,args.hid_dim,args.n_layers,args.lr,args.l2,args.epoch))\n",
    "                                            file.close()\n",
    "\n",
    "                                            te = time.time()\n",
    "                                \n",
    "                                            print('experiment_{}/{}, took {:2.1f}sec, {:2.2f}% done'\n",
    "                                                  .format(num,total_exp_num,te-ts,(num/total_exp_num)*100))\n",
    "                                    \n",
    "#                                             network = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "#                                             torch.save(network.state_dict(),'lstm1.pt')\n",
    "                            \n",
    "                                       \n",
    "#                                             print('train_acc = {:2.2f}%, val_acc = {:2.2f}%, test_acc = {:2.2f}%'\n",
    "#                                                   .format(result['train_acc'],result['val_acc'],result['test_acc']))\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CPU random seed: 666\n"
     ]
    }
   ],
   "source": [
    "# print('Current CPU random seed:',torch.initial_seed())\n",
    "# print('Current CUDA random seed:',torch.cuda.initial_seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 666\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# network = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 666\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# torch.save(network.state_dict(),'lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(UpData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 666\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# modell = LSTM(14, 16, 1, 2, 8, 0.1, False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 666\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# modell.load_state_dict(torch.load('lstm1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 666\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# RMSE_1, R2_1, Pred_1, True_1 = test(modell, partition, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.DataFrame()\n",
    "# test_data['test_pred'] = Pred_1\n",
    "# test_data['test_true'] = True_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.to_csv('modell_result_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 8\n",
    "args.UpData = UpData\n",
    "args.DownData = DownData\n",
    "args.x_frames = 5\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 16\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001\n",
    "args.dropout = 0.1 \n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam'  #SGD, RMSprop, Adam...\n",
    "args.loss = 'MSELoss'#'MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss'\n",
    "args.lr = 0.01\n",
    "args.epoch = 3\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'x_frames'\n",
    "list_var1 = [2]\n",
    "\n",
    "name_var2 = 'loss'\n",
    "list_var2 = ['MSELoss']\n",
    "\n",
    "name_var3 = 'optim'\n",
    "list_var3 = ['Adam']\n",
    "\n",
    "name_var4 = 'use_bn'\n",
    "list_var4 = [False]\n",
    "\n",
    "name_var5 = 'dropout'\n",
    "list_var5 = [0.1]\n",
    "\n",
    "name_var6 = 'batch_size'\n",
    "list_var6 = [4]\n",
    "\n",
    "name_var7 = 'hid_dim'\n",
    "list_var7 = [32, 64]\n",
    "\n",
    "name_var8 = 'n_layers'\n",
    "list_var8 = [4, 3, 2]\n",
    "\n",
    "name_var9 = 'lr'\n",
    "list_var9 = [0.0007, 0.001, 0.003]\n",
    "\n",
    "name_var10 = 'l2'\n",
    "list_var10 = [0.00007,0.0001, 0.0003]\n",
    "\n",
    "name_var11 = 'epoch'\n",
    "list_var11 = [200]\n",
    "\n",
    "trainset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2016-07-15')\n",
    "valset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-07-16', '2017-05-19')\n",
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-07-16', '2017-05-19')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "print(' size of trainset :{}\\n'.format(len(trainset)),\n",
    "      'size of valset :{}\\n'.format(len(valset)),\n",
    "      'size of testset :{}'.format(len(testset)))\n",
    "\n",
    "list_vars = [list_var1, list_var2, list_var3, list_var4, list_var5, list_var6, list_var7, list_var8, list_var9, list_var10, list_var11]\n",
    "i = 1\n",
    "for lenth in list_vars:\n",
    "    x = len(lenth)\n",
    "    i *= x\n",
    "total_exp_num = i\n",
    "\n",
    "print(' total_exp_num : {}'.format(total_exp_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
