{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMKHTsedaum7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import argparse\n",
    "from copy import deepcopy #Add Deepcopy for args\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1gRd7qVcEwI"
   },
   "source": [
    "# 1. Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KShxDU_Jcj3u"
   },
   "source": [
    "### 1.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jinsungpark/Desktop/Data_river/data04'\n",
      "/Users/jinsungpark/Desktop/jupyter\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynYVJoGrcnQS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/jupyter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTiRk82qctLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation_analysis.ipynb\r\n",
      "Data_Preprocessing.ipynb\r\n",
      "\u001b[34mData_river\u001b[m\u001b[m/\r\n",
      "Lab10_Stock_Price_Prediction_with_LSTM.ipynb\r\n",
      "Pandas_tutorial.ipynb\r\n",
      "\u001b[34mProject_Git\u001b[m\u001b[m/\r\n",
      "Project_River_Backup.ipynb\r\n",
      "Project_River_Backup_change_TP.ipynb\r\n",
      "Project_River_Find_BestModel.ipynb\r\n",
      "Project_River_Find_BestModel_01.ipynb\r\n",
      "Project_River_matplotlib.ipynb\r\n",
      "Project_River_matplotlib_01.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "Untitled1.ipynb\r\n",
      "\u001b[34mresults\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls #현재경로에 있는 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/jupyter/Data_river/data04\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/jupyter/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dln4LnKpc1CX"
   },
   "outputs": [],
   "source": [
    "UpStream_data = pd.read_excel('DS_Data_edit_log.xlsx')\n",
    "DownStream_data = pd.read_excel('NG_Data_edit_log.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GRAwoM_c068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'DS_DO', 'DS_BOD', 'DS_COD', 'DS_SS', 'DS_TN', 'DS_TP',\n",
      "       'DS_Chl_a', 'DS_Cells', 'GJ_Deep', 'GJ_Level', 'GJ_Outflow',\n",
      "       'DaeGu_Rain', 'DaeGu_Solar', 'SeoBu_COD', 'SeoBu_SS', 'SeoBu_TN',\n",
      "       'SeoBu_TP', 'SeoBu_Flow_mean', 'SeoBu_Flow_day', 'SeoBu_COD_load',\n",
      "       'SeoBu_SS_load', 'SeoBu_TN_load', 'SeoBu_TP_load', 'SungSeo_COD',\n",
      "       'SungSeo_SS', 'SungSeo_TN', 'SungSeo_TP', 'SungSeo_Flow_mean',\n",
      "       'SungSeo_Flow_day', 'SungSeo_COD_load', 'SungSeo_SS_load',\n",
      "       'SungSeo_TN_load', 'SungSeo_TP_load', 'GumHo_DO', 'GumHo_BOD',\n",
      "       'GumHo_COD', 'GumHo_SS', 'GumHo_TN', 'GumHo_TP', 'GumHo_Chl_a',\n",
      "       'GumHo_Flow', 'DS_Temp', 'GumHo_Temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(UpStream_data.columns)\n",
    "# print(DownStream_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [],
   "source": [
    "#날짜 인덱스화\n",
    "UpData = UpStream_data.set_index('Date')\n",
    "DownData = DownStream_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 43 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DS_DO              349 non-null    float64\n",
      " 1   DS_BOD             349 non-null    float64\n",
      " 2   DS_COD             349 non-null    float64\n",
      " 3   DS_SS              349 non-null    float64\n",
      " 4   DS_TN              349 non-null    float64\n",
      " 5   DS_TP              349 non-null    float64\n",
      " 6   DS_Chl_a           349 non-null    float64\n",
      " 7   DS_Cells           349 non-null    float64\n",
      " 8   GJ_Deep            349 non-null    float64\n",
      " 9   GJ_Level           349 non-null    float64\n",
      " 10  GJ_Outflow         349 non-null    float64\n",
      " 11  DaeGu_Rain         349 non-null    float64\n",
      " 12  DaeGu_Solar        349 non-null    float64\n",
      " 13  SeoBu_COD          349 non-null    float64\n",
      " 14  SeoBu_SS           349 non-null    float64\n",
      " 15  SeoBu_TN           349 non-null    float64\n",
      " 16  SeoBu_TP           349 non-null    float64\n",
      " 17  SeoBu_Flow_mean    349 non-null    float64\n",
      " 18  SeoBu_Flow_day     349 non-null    float64\n",
      " 19  SeoBu_COD_load     349 non-null    float64\n",
      " 20  SeoBu_SS_load      349 non-null    float64\n",
      " 21  SeoBu_TN_load      349 non-null    float64\n",
      " 22  SeoBu_TP_load      349 non-null    float64\n",
      " 23  SungSeo_COD        349 non-null    float64\n",
      " 24  SungSeo_SS         349 non-null    float64\n",
      " 25  SungSeo_TN         349 non-null    float64\n",
      " 26  SungSeo_TP         349 non-null    float64\n",
      " 27  SungSeo_Flow_mean  349 non-null    float64\n",
      " 28  SungSeo_Flow_day   349 non-null    float64\n",
      " 29  SungSeo_COD_load   349 non-null    float64\n",
      " 30  SungSeo_SS_load    349 non-null    float64\n",
      " 31  SungSeo_TN_load    349 non-null    float64\n",
      " 32  SungSeo_TP_load    349 non-null    float64\n",
      " 33  GumHo_DO           349 non-null    float64\n",
      " 34  GumHo_BOD          349 non-null    float64\n",
      " 35  GumHo_COD          349 non-null    float64\n",
      " 36  GumHo_SS           349 non-null    float64\n",
      " 37  GumHo_TN           349 non-null    float64\n",
      " 38  GumHo_TP           349 non-null    float64\n",
      " 39  GumHo_Chl_a        349 non-null    float64\n",
      " 40  GumHo_Flow         349 non-null    float64\n",
      " 41  DS_Temp            349 non-null    float64\n",
      " 42  GumHo_Temp         349 non-null    float64\n",
      "dtypes: float64(43)\n",
      "memory usage: 120.0+ KB\n"
     ]
    }
   ],
   "source": [
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   DS_DO        349 non-null    float64\n",
      " 1   DS_TN        349 non-null    float64\n",
      " 2   DS_Cells     349 non-null    float64\n",
      " 3   GJ_Outflow   349 non-null    float64\n",
      " 4   DaeGu_Rain   349 non-null    float64\n",
      " 5   DaeGu_Solar  349 non-null    float64\n",
      " 6   SeoBu_TN     349 non-null    float64\n",
      " 7   GumHo_DO     349 non-null    float64\n",
      " 8   GumHo_COD    349 non-null    float64\n",
      " 9   GumHo_TN     349 non-null    float64\n",
      " 10  GumHo_TP     349 non-null    float64\n",
      " 11  GumHo_Chl_a  349 non-null    float64\n",
      " 12  DS_Temp      349 non-null    float64\n",
      " 13  GumHo_Temp   349 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 40.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#넣고싶은 상류 항목 컬럼 선택\n",
    "UpData = UpData.iloc[:,[0,4,7,10,11,12,15,33,35,37,38,39,41,42]]\n",
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   NG_DO     349 non-null    float64\n",
      " 1   NG_BOD    349 non-null    float64\n",
      " 2   NG_COD    349 non-null    float64\n",
      " 3   NG_SS     349 non-null    float64\n",
      " 4   NG_TN     349 non-null    float64\n",
      " 5   NG_TP     349 non-null    float64\n",
      " 6   NG_Chl_a  349 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "DownData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG_TN\n"
     ]
    }
   ],
   "source": [
    "#알고싶은 하류 항목 컬럼 넘버 넣기('Date'항목이 인덱스화 돼서 컬럼 넘버가 -1씩 됨)\n",
    "Colum = 4\n",
    "print(DownData.columns[Colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Tp4o0Hc0ZL"
   },
   "source": [
    "### 1.2 Data Preprocessing(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChKYCAtTdGpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "UpScaler = MinMaxScaler() #상류데이터용\n",
    "DownScaler = MinMaxScaler() #하류데이터용\n",
    "\n",
    "#나중에 결과를 DeNormalizing 하기 위해 나누어 사용 하였다.\n",
    "\n",
    "def DeNormalize(Y, Data_name, column_num, Scaler_Type):\n",
    "    \n",
    "    data = Data_name\n",
    "    Scaler = Scaler_Type\n",
    "    \n",
    "    _max = Scaler.data_max_[column_num] # 역정규화 하려는 데이터의 컬럼 번호\n",
    "    _min = Scaler.data_min_[column_num] \n",
    "    \n",
    "    X = Y*(_max-_min) + _min\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_DO          0\n",
      "DS_TN          0\n",
      "DS_Cells       0\n",
      "GJ_Outflow     0\n",
      "DaeGu_Rain     0\n",
      "DaeGu_Solar    0\n",
      "SeoBu_TN       0\n",
      "GumHo_DO       0\n",
      "GumHo_COD      0\n",
      "GumHo_TN       0\n",
      "GumHo_TP       0\n",
      "GumHo_Chl_a    0\n",
      "DS_Temp        0\n",
      "GumHo_Temp     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#데이터 정규화\n",
    "UpData = pd.DataFrame(UpScaler.fit_transform(UpData), columns=UpData.columns, index=UpData.index)\n",
    "DownData = pd.DataFrame(DownScaler.fit_transform(DownData), columns=DownData.columns, index=DownData.index)\n",
    "\n",
    "print(UpData.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDSVZGiUdGYz"
   },
   "source": [
    "#2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hxm2moSudGQD"
   },
   "outputs": [],
   "source": [
    "class RiverDataset(Dataset):\n",
    "    def __init__(self, UpData, DownData, x_frames, y_frames, start, end):\n",
    "        \n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.UpData = UpData[start:end]\n",
    "        self.DownData = DownData[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.UpData) - (self.x_frames + self.y_frames) + 1\n",
    "    #데이터를 전처리 할때 UpData와 DownData의 길이가 동일해짐(날짜를 동일한것만 추출해야 하므로), 따라서 전체길이는 둘중 하나를 사용\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "\n",
    "        X = self.UpData.iloc[idx-self.x_frames:idx].values\n",
    "        Y = self.DownData.iloc[idx:idx+self.y_frames].values\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj80CahhdGG9"
   },
   "source": [
    "# 3. Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuBGEc51dF-V"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers) #\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTpV_o6Wdglq"
   },
   "outputs": [],
   "source": [
    "# 정확도 : 예측확률을 100%로 봤을때 MAPE에 따른 오차비율을 빼줌 (100-MAPE) ##RMSE, MAPE 두개로 볼 수 있게\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    R2_score = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    return R2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GPb8H8-djwB"
   },
   "source": [
    "# 4. Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrT2W-pqdjh6"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.transpose(0, 1).float().to(args.device)#파이토치는 순서가 달라서 바꿔줌\n",
    "        y_true = y[:, :, Colum].float().to(args.device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred.append(y_pred)\n",
    "        true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 (배치사이즈의 텐서들을 한개씩 추가해주기) ==== #\n",
    "    for i in range(len(trainloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    train_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     train_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    return model, train_loss, train_acc1[0], train_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvAO-LVCdjgG"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 (배치사이즈의 텐서들을 한개씩 추가해주기) ==== #\n",
    "    for i in range(len(valloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "    \n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    val_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     val_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    \n",
    "    return val_loss, val_acc1[0], val_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHWmu5EtdjXu"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 ==== #\n",
    "    for i in range(len(testloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================== #   \n",
    "\n",
    "    test_acc1 =  RMSE(np.array( true_results), np.array(pred_results))\n",
    "    test_acc2 =  R2(np.array( true_results), np.array(pred_results))\n",
    "#     test_acc3 =  (100 - MAPE(np.array( true_results), np.array(pred_results)))\n",
    "    \n",
    "    return test_acc1[0], test_acc2[0], pred_results, true_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkkF0-qmeOMq"
   },
   "source": [
    "# 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EOe2j_udjNv"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "#     loss_fn = torch.nn.MSELoss() ##loss는 mse를 사용\n",
    "#     loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if args.loss == 'MSELoss':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif args.loss == 'L1Loss':\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif args.loss == 'PoissonNLLLoss':\n",
    "        loss_fn = torch.nn.PoissonNLLLoss()\n",
    "        loss_fn = nn.PoissonNLLLoss()\n",
    "    elif args.loss == 'KLDivLoss':\n",
    "        loss_fn = torch.nn.KLDivLoss()\n",
    "        loss_fn = nn.KLDivLoss()\n",
    "    elif args.loss == 'BCELoss':\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        loss_fn = nn.BCELoss()\n",
    "    elif args.loss == 'BCEWithLogitsLoss':\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError('In-valid LossFuction choice')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs_RMSE = []\n",
    "    train_accs_R2 = []\n",
    "    val_accs_RMSE = []\n",
    "    val_accs_R2 = []\n",
    "    axis = []\n",
    "    # ===================================== #\n",
    "    \n",
    "    ## model starting point ##    \n",
    "    ts = time.time()\n",
    "    model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "    val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "    te = time.time()\n",
    "\n",
    "    # ====== Add Epoch Data ====== #\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs_RMSE.append(train_acc_RMSE)\n",
    "    val_accs_RMSE.append(val_acc_RMSE)\n",
    "    train_accs_R2.append(train_acc_R2)\n",
    "    val_accs_R2.append(val_acc_R2)\n",
    "    # ============================ #\n",
    "\n",
    "    # # ===== Visdom visualizing ================================================================================== #\n",
    "    axis.append(0)\n",
    "    \n",
    "    plot1 = vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_loss'.format(num), legend=['train_loss','val_loss'], showlegend=True))\n",
    "    \n",
    "    plot2 = vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_acc_RMSE'.format(num), legend=['train_acc','val_acc'], showlegend=True))\n",
    "    # # =========================================================================================================== #\n",
    "    \n",
    "    print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "          .format(0, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "    \n",
    "    for epoch in range(args.epoch-1):  # loop over the dataset multiple times\n",
    "        \n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "\n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs_RMSE.append(train_acc_RMSE)\n",
    "        val_accs_RMSE.append(val_acc_RMSE)\n",
    "        train_accs_R2.append(train_acc_R2)\n",
    "        val_accs_R2.append(val_acc_R2)\n",
    "        # ============================ #\n",
    "\n",
    "        # # ===== Visdom visualizing ============================================================================== #\n",
    "        axis.append(epoch+1)\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot1, update='replace')\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot2, update='replace')\n",
    "        # # ====================================================================================================== #\n",
    "        \n",
    "        print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch+1, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc_RMSE, test_acc_R2, Pred_data, True_data = test(model, partition, args)\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    \n",
    "    result['train_accs_RMSE'] = train_accs_RMSE\n",
    "    result['train_accs_R2'] = train_accs_R2\n",
    "    result['val_accs_RMSE'] = val_accs_RMSE\n",
    "    result['val_accs_R2'] = val_accs_R2\n",
    "#     result['train_acc'] = train_acc\n",
    "#     result['val_acc'] = val_acc\n",
    "    result['test_RMSE'] = test_acc_RMSE\n",
    "    result['test_R2'] = test_acc_R2\n",
    "    \n",
    "    result['test_pred'] = Pred_data\n",
    "    result['test_true'] = True_data\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNi-f5HyeJYi"
   },
   "source": [
    "# 6. LSTM Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4Bu6kFUdizj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset :181 size of valset :38 size of testset :38\n",
      "\n",
      " exp_1\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=2, n_layers=3, batch_size=2, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.58/0.52, Loss(train/val) 0.09125/0.00970. Took 0.64 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.23/0.51, Loss(train/val) 0.04599/0.00930. Took 0.65 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.22/0.49, Loss(train/val) 0.04512/0.00856. Took 0.65 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.17/0.47, Loss(train/val) 0.04195/0.00810. Took 0.64 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.17/0.49, Loss(train/val) 0.04066/0.00860. Took 0.65 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 1.14/0.48, Loss(train/val) 0.03911/0.00777. Took 0.64 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.98/0.94, Loss(train/val) 0.02756/0.03382. Took 0.64 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 1.17/0.50, Loss(train/val) 0.04118/0.00867. Took 0.64 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 1.10/0.51, Loss(train/val) 0.03634/0.00881. Took 0.63 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 1.09/0.50, Loss(train/val) 0.03578/0.00873. Took 0.64 sec\n",
      "\n",
      " exp_2\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=2, n_layers=3, batch_size=2, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.06/0.88, Loss(train/val) 0.03164/0.02928. Took 0.61 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.69/0.90, Loss(train/val) 0.01546/0.03017. Took 0.62 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.73/0.91, Loss(train/val) 0.01672/0.03156. Took 0.62 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.73/0.88, Loss(train/val) 0.01723/0.02878. Took 0.61 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.83/0.82, Loss(train/val) 0.01913/0.02420. Took 0.61 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.80/0.82, Loss(train/val) 0.01918/0.02435. Took 0.61 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.85/0.73, Loss(train/val) 0.02254/0.01889. Took 0.61 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.90/0.69, Loss(train/val) 0.02471/0.01617. Took 0.61 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.91/0.67, Loss(train/val) 0.02500/0.01537. Took 0.61 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.91/0.67, Loss(train/val) 0.02470/0.01520. Took 0.61 sec\n",
      "\n",
      " exp_3\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=2, n_layers=3, batch_size=4, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 2.02/0.94, Loss(train/val) 0.20058/0.02753. Took 0.36 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.20/0.54, Loss(train/val) 0.04332/0.00957. Took 0.36 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.13/0.59, Loss(train/val) 0.03817/0.01168. Took 0.36 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.13/0.55, Loss(train/val) 0.03847/0.01004. Took 0.36 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.10/0.47, Loss(train/val) 0.03626/0.00728. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.92/0.82, Loss(train/val) 0.02420/0.01850. Took 0.36 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.91/0.58, Loss(train/val) 0.02340/0.01040. Took 0.36 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.79/0.38, Loss(train/val) 0.01706/0.00519. Took 0.36 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.81/0.37, Loss(train/val) 0.01824/0.00514. Took 0.36 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.86/0.38, Loss(train/val) 0.02048/0.00516. Took 0.36 sec\n",
      "\n",
      " exp_4\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=2, n_layers=3, batch_size=4, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.08/0.73, Loss(train/val) 0.03101/0.01864. Took 0.34 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.03/0.61, Loss(train/val) 0.03189/0.01229. Took 0.34 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.05/0.56, Loss(train/val) 0.03273/0.01047. Took 0.34 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.04/0.55, Loss(train/val) 0.03257/0.01020. Took 0.34 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.10/0.58, Loss(train/val) 0.03468/0.01129. Took 0.34 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 1.03/0.53, Loss(train/val) 0.03179/0.00934. Took 0.34 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 1.03/0.51, Loss(train/val) 0.03166/0.00876. Took 0.34 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.94/0.42, Loss(train/val) 0.02611/0.00577. Took 0.34 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.78/0.53, Loss(train/val) 0.01847/0.01072. Took 0.34 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.69/0.49, Loss(train/val) 0.01420/0.00728. Took 0.34 sec\n",
      "\n",
      " exp_5\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=2, n_layers=3, batch_size=8, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 2.02/1.11, Loss(train/val) 0.16080/0.04914. Took 0.20 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.24/0.67, Loss(train/val) 0.04691/0.01518. Took 0.21 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.14/0.50, Loss(train/val) 0.03835/0.00838. Took 0.20 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.10/0.47, Loss(train/val) 0.03614/0.00710. Took 0.20 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.97/0.32, Loss(train/val) 0.02699/0.00397. Took 0.20 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.84/0.30, Loss(train/val) 0.01928/0.00314. Took 0.21 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.79/0.40, Loss(train/val) 0.01865/0.00660. Took 0.21 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.88/0.47, Loss(train/val) 0.02019/0.00842. Took 0.20 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.83/0.50, Loss(train/val) 0.01855/0.01035. Took 0.21 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.91/0.49, Loss(train/val) 0.02155/0.00926. Took 0.21 sec\n",
      "\n",
      " exp_6\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=2, n_layers=3, batch_size=8, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.23/0.76, Loss(train/val) 0.04858/0.01970. Took 0.19 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.11/0.56, Loss(train/val) 0.03648/0.01022. Took 0.20 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.12/0.55, Loss(train/val) 0.03722/0.00977. Took 0.20 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.10/0.52, Loss(train/val) 0.03553/0.00874. Took 0.20 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.03/0.49, Loss(train/val) 0.03124/0.00782. Took 0.20 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.84/0.33, Loss(train/val) 0.02030/0.00380. Took 0.20 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.71/0.42, Loss(train/val) 0.01415/0.00547. Took 0.20 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.66/0.40, Loss(train/val) 0.01244/0.00517. Took 0.20 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.64/0.44, Loss(train/val) 0.01141/0.00592. Took 0.20 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.61/0.61, Loss(train/val) 0.01114/0.00965. Took 0.20 sec\n",
      "\n",
      " exp_7\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=3, n_layers=3, batch_size=2, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.78/0.54, Loss(train/val) 0.11681/0.01064. Took 0.65 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.28/0.59, Loss(train/val) 0.04948/0.01243. Took 0.64 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.31/0.56, Loss(train/val) 0.05264/0.01139. Took 0.64 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.26/0.40, Loss(train/val) 0.04823/0.00582. Took 0.64 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.11/0.49, Loss(train/val) 0.03688/0.00865. Took 0.65 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 1.09/0.83, Loss(train/val) 0.03496/0.02547. Took 0.64 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.89/0.67, Loss(train/val) 0.02301/0.01123. Took 0.65 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.71/1.11, Loss(train/val) 0.01428/0.02866. Took 0.65 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.82/0.76, Loss(train/val) 0.01725/0.01426. Took 0.64 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.71/0.75, Loss(train/val) 0.01400/0.01435. Took 0.64 sec\n",
      "\n",
      " exp_8\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=3, n_layers=3, batch_size=2, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 0.79/0.91, Loss(train/val) 0.02001/0.03139. Took 0.61 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.85/0.69, Loss(train/val) 0.02248/0.01660. Took 0.61 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.78/0.65, Loss(train/val) 0.02005/0.01449. Took 0.61 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.70/0.78, Loss(train/val) 0.01509/0.02205. Took 0.61 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.67/0.59, Loss(train/val) 0.01429/0.01225. Took 0.61 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.63/0.55, Loss(train/val) 0.01213/0.01046. Took 0.61 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.61/0.46, Loss(train/val) 0.01162/0.00731. Took 0.61 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.59/0.68, Loss(train/val) 0.01184/0.01243. Took 0.61 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.67/0.44, Loss(train/val) 0.01246/0.00649. Took 0.61 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Acc_RMSE(train/val): 0.59/0.45, Loss(train/val) 0.01125/0.00683. Took 0.61 sec\n",
      "\n",
      " exp_9\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=3, n_layers=3, batch_size=4, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.51/0.57, Loss(train/val) 0.07668/0.01067. Took 0.36 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.15/0.56, Loss(train/val) 0.04035/0.01022. Took 0.36 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.14/0.46, Loss(train/val) 0.03936/0.00715. Took 0.35 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.07/0.36, Loss(train/val) 0.03467/0.00511. Took 0.35 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.87/0.90, Loss(train/val) 0.02273/0.02129. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.87/0.35, Loss(train/val) 0.02041/0.00414. Took 0.36 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.70/0.40, Loss(train/val) 0.01405/0.00502. Took 0.35 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.74/0.32, Loss(train/val) 0.01499/0.00350. Took 0.35 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.79/0.28, Loss(train/val) 0.01688/0.00273. Took 0.36 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.75/0.30, Loss(train/val) 0.01538/0.00321. Took 0.36 sec\n",
      "\n",
      " exp_10\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=3, n_layers=3, batch_size=4, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.54/0.68, Loss(train/val) 0.03926/0.01563. Took 0.34 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.98/0.63, Loss(train/val) 0.02869/0.01325. Took 0.34 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.76/0.70, Loss(train/val) 0.01773/0.01753. Took 0.34 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.69/0.46, Loss(train/val) 0.01487/0.00623. Took 0.34 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.64/0.46, Loss(train/val) 0.01261/0.00644. Took 0.34 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.63/0.51, Loss(train/val) 0.01220/0.00720. Took 0.34 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.62/0.36, Loss(train/val) 0.01161/0.00454. Took 0.34 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.61/0.44, Loss(train/val) 0.01141/0.00585. Took 0.34 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.62/0.50, Loss(train/val) 0.01178/0.00722. Took 0.34 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.59/0.39, Loss(train/val) 0.01109/0.00523. Took 0.34 sec\n",
      "\n",
      " exp_11\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=3, n_layers=3, batch_size=8, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.64/0.59, Loss(train/val) 0.11639/0.01150. Took 0.21 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.12/0.50, Loss(train/val) 0.03794/0.00837. Took 0.22 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.11/0.50, Loss(train/val) 0.03707/0.00841. Took 0.21 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.08/0.51, Loss(train/val) 0.03464/0.00872. Took 0.23 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.08/0.46, Loss(train/val) 0.03444/0.00693. Took 0.21 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.99/0.45, Loss(train/val) 0.02712/0.00584. Took 0.22 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.79/0.32, Loss(train/val) 0.01874/0.00404. Took 0.21 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.78/0.39, Loss(train/val) 0.01588/0.00593. Took 0.21 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.88/0.34, Loss(train/val) 0.02090/0.00437. Took 0.21 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.83/0.31, Loss(train/val) 0.01857/0.00355. Took 0.21 sec\n",
      "\n",
      " exp_12\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=3, n_layers=3, batch_size=8, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 2.32/0.69, Loss(train/val) 0.07788/0.01576. Took 0.22 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.12/0.53, Loss(train/val) 0.03690/0.00937. Took 0.21 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.08/0.50, Loss(train/val) 0.03457/0.00828. Took 0.20 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.05/0.49, Loss(train/val) 0.03233/0.00783. Took 0.23 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.91/0.32, Loss(train/val) 0.02354/0.00361. Took 0.21 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.81/0.33, Loss(train/val) 0.01875/0.00365. Took 0.21 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.70/0.38, Loss(train/val) 0.01393/0.00459. Took 0.21 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.67/0.45, Loss(train/val) 0.01266/0.00613. Took 0.20 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.63/0.41, Loss(train/val) 0.01184/0.00545. Took 0.20 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.63/0.56, Loss(train/val) 0.01157/0.00851. Took 0.21 sec\n",
      "\n",
      " exp_13\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=3, batch_size=2, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.33/0.50, Loss(train/val) 0.05655/0.00873. Took 0.68 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.13/0.51, Loss(train/val) 0.03852/0.00899. Took 0.65 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.12/0.51, Loss(train/val) 0.03756/0.00893. Took 0.67 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.11/0.51, Loss(train/val) 0.03688/0.00886. Took 0.68 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.10/0.51, Loss(train/val) 0.03643/0.00882. Took 0.66 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 1.10/0.51, Loss(train/val) 0.03619/0.00875. Took 0.65 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 1.10/0.50, Loss(train/val) 0.03605/0.00874. Took 0.66 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 1.09/0.50, Loss(train/val) 0.03592/0.00874. Took 0.65 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 1.09/0.50, Loss(train/val) 0.03587/0.00873. Took 0.65 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 1.09/0.50, Loss(train/val) 0.03587/0.00872. Took 0.66 sec\n",
      "\n",
      " exp_14\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=4, n_layers=3, batch_size=2, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.23/0.90, Loss(train/val) 0.03320/0.03096. Took 0.62 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.74/0.89, Loss(train/val) 0.01662/0.03014. Took 0.64 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.76/0.86, Loss(train/val) 0.01804/0.02737. Took 0.69 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.78/0.85, Loss(train/val) 0.01923/0.02656. Took 0.69 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.82/0.80, Loss(train/val) 0.02088/0.02278. Took 0.66 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.84/0.77, Loss(train/val) 0.02175/0.02112. Took 0.62 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.88/0.74, Loss(train/val) 0.02352/0.01909. Took 0.68 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.86/0.77, Loss(train/val) 0.02216/0.02119. Took 0.66 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.88/0.72, Loss(train/val) 0.02364/0.01821. Took 0.62 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.88/0.72, Loss(train/val) 0.02331/0.01817. Took 0.61 sec\n",
      "\n",
      " exp_15\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=3, batch_size=4, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.21/0.55, Loss(train/val) 0.04474/0.01019. Took 0.35 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.11/0.50, Loss(train/val) 0.03691/0.00860. Took 0.36 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.11/0.53, Loss(train/val) 0.03672/0.00943. Took 0.36 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.08/0.50, Loss(train/val) 0.03480/0.00847. Took 0.36 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.07/0.51, Loss(train/val) 0.03427/0.00871. Took 0.36 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 1.04/0.40, Loss(train/val) 0.03231/0.00585. Took 0.35 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.99/0.49, Loss(train/val) 0.02639/0.00811. Took 0.36 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.91/0.84, Loss(train/val) 0.02494/0.01643. Took 0.36 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.81/1.06, Loss(train/val) 0.01954/0.02809. Took 0.37 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.94/0.35, Loss(train/val) 0.02417/0.00387. Took 0.36 sec\n",
      "\n",
      " exp_16\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=4, n_layers=3, batch_size=4, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.24/0.88, Loss(train/val) 0.05317/0.02865. Took 0.34 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.96/0.79, Loss(train/val) 0.02809/0.02251. Took 0.34 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.94/0.81, Loss(train/val) 0.02658/0.02360. Took 0.34 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.78/0.82, Loss(train/val) 0.01826/0.02500. Took 0.35 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.61/0.62, Loss(train/val) 0.01269/0.01200. Took 0.34 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.66/0.72, Loss(train/val) 0.01358/0.01470. Took 0.35 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.65/0.69, Loss(train/val) 0.01296/0.01332. Took 0.34 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.63/0.58, Loss(train/val) 0.01215/0.01079. Took 0.34 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.65/0.49, Loss(train/val) 0.01264/0.00916. Took 0.35 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Acc_RMSE(train/val): 0.59/0.45, Loss(train/val) 0.01145/0.00772. Took 0.34 sec\n",
      "\n",
      " exp_17\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=3, batch_size=8, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 1.56/0.98, Loss(train/val) 0.07649/0.03674. Took 0.20 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.19/0.50, Loss(train/val) 0.04342/0.00821. Took 0.21 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.08/0.49, Loss(train/val) 0.03440/0.00795. Took 0.20 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.03/0.35, Loss(train/val) 0.03082/0.00380. Took 0.20 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.85/0.30, Loss(train/val) 0.01930/0.00316. Took 0.20 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.78/0.48, Loss(train/val) 0.01680/0.00875. Took 0.21 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.84/0.47, Loss(train/val) 0.01906/0.00781. Took 0.20 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.85/0.36, Loss(train/val) 0.01993/0.00526. Took 0.22 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.82/0.29, Loss(train/val) 0.01876/0.00298. Took 0.21 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.79/0.31, Loss(train/val) 0.01673/0.00348. Took 0.20 sec\n",
      "\n",
      " exp_18\n",
      "loseFunc = MSELoss, optim=RMSprop, x_frames=4, n_layers=3, batch_size=8, hid_dim=16, epoch=10, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 2.10/0.76, Loss(train/val) 0.12542/0.02006. Took 0.20 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 1.14/0.62, Loss(train/val) 0.03875/0.01274. Took 0.20 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 1.14/0.60, Loss(train/val) 0.03905/0.01180. Took 0.20 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 1.13/0.54, Loss(train/val) 0.03776/0.00937. Took 0.20 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 1.12/0.61, Loss(train/val) 0.03669/0.01234. Took 0.20 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 1.03/0.64, Loss(train/val) 0.03160/0.01350. Took 0.20 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.89/0.44, Loss(train/val) 0.02341/0.00665. Took 0.20 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.74/0.41, Loss(train/val) 0.01533/0.00599. Took 0.20 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.69/0.42, Loss(train/val) 0.01368/0.00607. Took 0.20 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.67/0.40, Loss(train/val) 0.01347/0.00597. Took 0.20 sec\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 8 # 2~8\n",
    "args.UpData = UpData\n",
    "args.DownData = DownData\n",
    "args.x_frames = 4\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 16\n",
    "args.n_layers = 3\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001 \n",
    "args.dropout = 0.1 \n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #SGD, RMSprop, Adam...\n",
    "args.loss = 'MSELoss'#'MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss'\n",
    "args.lr = 0.01\n",
    "args.epoch = 180\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'x_frames'\n",
    "name_var2 = 'batch_size'\n",
    "name_var3 = 'optim'\n",
    "list_var1 = [2, 3, 4]\n",
    "list_var2 = [2, 4, 8]\n",
    "list_var3 = ['Adam','RMSprop']\n",
    "num = 0 #초기화\n",
    "\n",
    "\n",
    "trainset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2016-07-31')\n",
    "valset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-08-01', '2017-05-19')\n",
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-08-01', '2017-05-19')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "\n",
    "print('size of trainset :{}'.format(len(trainset)),\n",
    "      'size of valset :{}'.format(len(valset)),\n",
    "      'size of testset :{}'.format(len(testset)))\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        for var3 in list_var3:\n",
    "            num += 1\n",
    "            setattr(args, name_var1, var1)\n",
    "            setattr(args, name_var2, var2)\n",
    "            setattr(args, name_var3, var3)\n",
    "            print('\\n exp_{}'.format(num))\n",
    "            print('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                  .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn))     \n",
    "\n",
    "            result = experiment(partition, deepcopy(args))\n",
    "\n",
    "    #         print('train_acc_RMSE = {:2.2f}%, val_acc_RMSE = {:2.2f}%, test_RMSE = {:2.2f}%, test_R2 = {:2.2f}%'\n",
    "    #               .format(result['train_acc_RMSE'],result['val_acc_RMSE'],result['test_RMSE'],result['test_R2']))\n",
    "\n",
    "            vis.text('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                     .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn),\n",
    "                     opts=dict(title='exp_{}_text'.format(num)))\n",
    "            # 만든 모델의 test 데이터 예측 시각화\n",
    "\n",
    "            predict = torch.Tensor(result['test_pred']).view(-1,1)\n",
    "            truth = torch.Tensor(result['test_true']).view(-1,1)\n",
    "            axis = torch.Tensor(range(len(result['test_pred']))).view(-1,1)\n",
    "\n",
    "            Y_axis = torch.cat((predict, truth), -1)\n",
    "            X_axis = torch.cat((axis, axis), -1)\n",
    "\n",
    "            vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_RMSE[{:2.3f}]_R2[{:2.3f}]'.format(num,result['test_RMSE'],result['test_R2']),\n",
    "                                                       legend=['predict','true'],\n",
    "                                                       showlegend=True,\n",
    "                                                       layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
