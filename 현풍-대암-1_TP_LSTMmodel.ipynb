{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMKHTsedaum7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['seed']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import argparse\n",
    "from copy import deepcopy #Add Deepcopy for args\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1gRd7qVcEwI"
   },
   "source": [
    "# 1. Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KShxDU_Jcj3u"
   },
   "source": [
    "### 1.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/jupyter/Data_river/original02/HP_DA\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/jupyter/Data_river/original02/HP_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynYVJoGrcnQS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/jupyter/Data_river/original02/HP_DA'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTiRk82qctLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA_Down_data.xlsx  DA_Down_log.xlsx   HP_Up_data.xlsx    HP_Up_log.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "ls #현재경로에 있는 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dln4LnKpc1CX"
   },
   "outputs": [],
   "source": [
    "UpStream_data = pd.read_excel('HP_Up_log.xlsx')\n",
    "DownStream_data = pd.read_excel('DA_Down_log.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GRAwoM_c068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'HP_DO', 'HP_BOD', 'HP_COD', 'HP_SS', 'HP_TN', 'HP_TP',\n",
      "       'HP_Chl_a', 'HP_Flow', 'ChaCh_DO', 'ChaCh_BOD', 'ChaCh_COD', 'ChaCh_SS',\n",
      "       'ChaCh_TN', 'ChaCh_TP', 'ChaCh_Chl_a', 'ChaCh_Flow', 'UiR_Rain',\n",
      "       'UiR_Solar', 'HP_Temp', 'ChaCh_Temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(UpStream_data.columns)\n",
    "# print(DownStream_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [],
   "source": [
    "#날짜 인덱스화\n",
    "UpData = UpStream_data.set_index('Date')\n",
    "DownData = DownStream_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 265 entries, 2013-01-08 to 2019-09-23\n",
      "Data columns (total 20 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   HP_DO        265 non-null    float64\n",
      " 1   HP_BOD       265 non-null    float64\n",
      " 2   HP_COD       265 non-null    float64\n",
      " 3   HP_SS        265 non-null    float64\n",
      " 4   HP_TN        265 non-null    float64\n",
      " 5   HP_TP        265 non-null    float64\n",
      " 6   HP_Chl_a     265 non-null    float64\n",
      " 7   HP_Flow      265 non-null    float64\n",
      " 8   ChaCh_DO     265 non-null    float64\n",
      " 9   ChaCh_BOD    265 non-null    float64\n",
      " 10  ChaCh_COD    265 non-null    float64\n",
      " 11  ChaCh_SS     265 non-null    float64\n",
      " 12  ChaCh_TN     265 non-null    float64\n",
      " 13  ChaCh_TP     265 non-null    float64\n",
      " 14  ChaCh_Chl_a  265 non-null    float64\n",
      " 15  ChaCh_Flow   265 non-null    float64\n",
      " 16  UiR_Rain     265 non-null    float64\n",
      " 17  UiR_Solar    265 non-null    float64\n",
      " 18  HP_Temp      265 non-null    float64\n",
      " 19  ChaCh_Temp   265 non-null    float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 43.5 KB\n"
     ]
    }
   ],
   "source": [
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HP_DO', 'HP_COD', 'HP_SS', 'HP_TN', 'HP_TP', 'HP_Temp', 'HP_Flow',\n",
      "       'ChaCh_DO', 'ChaCh_COD', 'ChaCh_TN', 'ChaCh_TP', 'ChaCh_Temp',\n",
      "       'ChaCh_Flow', 'UiR_Rain'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#넣고싶은 상류 항목 컬럼 선택 - TP setting\n",
    "UpData = UpData.iloc[:,[0,2,3,4,5,18,7,8,10,12,13,19,15,16]]\n",
    "print(UpData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 265 entries, 2013-01-08 to 2019-09-23\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   DA_DO     265 non-null    float64\n",
      " 1   DA_BOD    265 non-null    float64\n",
      " 2   DA_COD    265 non-null    float64\n",
      " 3   DA_SS     265 non-null    float64\n",
      " 4   DA_TN     265 non-null    float64\n",
      " 5   DA_TP     265 non-null    float64\n",
      " 6   DA_Chl_a  265 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 16.6 KB\n"
     ]
    }
   ],
   "source": [
    "DownData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA_TP\n"
     ]
    }
   ],
   "source": [
    "#알고싶은 하류 항목 컬럼 넘버 넣기('Date'항목이 인덱스화 돼서 컬럼 넘버가 -1씩 됨)\n",
    "Colum = 5\n",
    "print(DownData.columns[Colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Tp4o0Hc0ZL"
   },
   "source": [
    "### 1.2 Data Preprocessing(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChKYCAtTdGpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "UpScaler = MinMaxScaler() #상류데이터용\n",
    "DownScaler = MinMaxScaler() #하류데이터용\n",
    "\n",
    "#나중에 결과를 DeNormalizing 하기 위해 나누어 사용 하였다.\n",
    "\n",
    "def DeNormalize(Y, Data_name, column_num, Scaler_Type):\n",
    "    \n",
    "    data = Data_name\n",
    "    Scaler = Scaler_Type\n",
    "    \n",
    "    _max = Scaler.data_max_[column_num] # 역정규화 하려는 데이터의 컬럼 번호\n",
    "    _min = Scaler.data_min_[column_num] \n",
    "    \n",
    "    X = Y*(_max-_min) + _min\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP_DO         0\n",
      "HP_COD        0\n",
      "HP_SS         0\n",
      "HP_TN         0\n",
      "HP_TP         0\n",
      "HP_Temp       0\n",
      "HP_Flow       0\n",
      "ChaCh_DO      0\n",
      "ChaCh_COD     0\n",
      "ChaCh_TN      0\n",
      "ChaCh_TP      0\n",
      "ChaCh_Temp    0\n",
      "ChaCh_Flow    0\n",
      "UiR_Rain      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#데이터 정규화\n",
    "UpData = pd.DataFrame(UpScaler.fit_transform(UpData), columns=UpData.columns, index=UpData.index)\n",
    "DownData = pd.DataFrame(DownScaler.fit_transform(DownData), columns=DownData.columns, index=DownData.index)\n",
    "\n",
    "print(UpData.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDSVZGiUdGYz"
   },
   "source": [
    "## 2.Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hxm2moSudGQD"
   },
   "outputs": [],
   "source": [
    "class RiverDataset(Dataset):\n",
    "    def __init__(self, UpData, DownData, x_frames, y_frames, start, end):\n",
    "        \n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.UpData = UpData[start:end]\n",
    "        self.DownData = DownData[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.UpData) - (self.x_frames + self.y_frames) + 1\n",
    "    #데이터를 전처리 할때 UpData와 DownData의 길이가 동일해짐(날짜를 동일한것만 추출해야 하므로), 따라서 전체길이는 둘중 하나를 사용\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "\n",
    "        X = self.UpData.iloc[idx-self.x_frames:idx].values\n",
    "        Y = self.DownData.iloc[idx:idx+self.y_frames].values\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj80CahhdGG9"
   },
   "source": [
    "# 3. Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuBGEc51dF-V"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers) #\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTpV_o6Wdglq"
   },
   "outputs": [],
   "source": [
    "# 정확도 : 예측확률을 100%로 봤을때 MAPE에 따른 오차비율을 빼줌 (100-MAPE) ##RMSE, MAPE 두개로 볼 수 있게\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    R2_score = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    return R2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GPb8H8-djwB"
   },
   "source": [
    "# 4. Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrT2W-pqdjh6"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.transpose(0, 1).float().to(args.device)#파이토치는 순서가 달라서 바꿔줌\n",
    "        y_true = y[:, :, Colum].float().to(args.device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred.append(y_pred)\n",
    "        true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 (배치사이즈의 텐서들을 한개씩 추가해주기) ==== #\n",
    "    for i in range(len(trainloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    train_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     train_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    return model, train_loss, train_acc1[0], train_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvAO-LVCdjgG"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 (배치사이즈의 텐서들을 한개씩 추가해주기) ==== #\n",
    "    for i in range(len(valloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "    \n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    val_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     val_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    \n",
    "    return val_loss, val_acc1[0], val_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHWmu5EtdjXu"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 ==== #\n",
    "    for i in range(len(testloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================== #   \n",
    "\n",
    "    test_acc1 =  RMSE(np.array( true_results), np.array(pred_results))\n",
    "    test_acc2 =  R2(np.array( true_results), np.array(pred_results))\n",
    "#     test_acc3 =  (100 - MAPE(np.array( true_results), np.array(pred_results)))\n",
    "    \n",
    "    return test_acc1[0], test_acc2[0], pred_results, true_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkkF0-qmeOMq"
   },
   "source": [
    "# 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EOe2j_udjNv"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "#     loss_fn = torch.nn.MSELoss() ##loss는 mse를 사용\n",
    "#     loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if args.loss == 'MSELoss':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif args.loss == 'L1Loss':\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif args.loss == 'PoissonNLLLoss':\n",
    "        loss_fn = torch.nn.PoissonNLLLoss()\n",
    "        loss_fn = nn.PoissonNLLLoss()\n",
    "    elif args.loss == 'KLDivLoss':\n",
    "        loss_fn = torch.nn.KLDivLoss()\n",
    "        loss_fn = nn.KLDivLoss()\n",
    "    elif args.loss == 'BCELoss':\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        loss_fn = nn.BCELoss()\n",
    "    elif args.loss == 'BCEWithLogitsLoss':\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError('In-valid LossFuction choice')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs_RMSE = []\n",
    "    train_accs_R2 = []\n",
    "    val_accs_RMSE = []\n",
    "    val_accs_R2 = []\n",
    "    axis = []\n",
    "    # ===================================== #\n",
    "    \n",
    "    ## model starting point ##    \n",
    "    ts = time.time()\n",
    "    model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "    val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "    te = time.time()\n",
    "\n",
    "    # ====== Add Epoch Data ====== #\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs_RMSE.append(train_acc_RMSE)\n",
    "    val_accs_RMSE.append(val_acc_RMSE)\n",
    "    train_accs_R2.append(train_acc_R2)\n",
    "    val_accs_R2.append(val_acc_R2)\n",
    "    # ============================ #\n",
    "\n",
    "    # # ===== Visdom visualizing ================================================================================== #\n",
    "    axis.append(0)\n",
    "    \n",
    "    plot1 = vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_loss'.format(num), legend=['train_loss','val_loss'], showlegend=True))\n",
    "    \n",
    "    plot2 = vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_acc_RMSE'.format(num), legend=['train_acc','val_acc'], showlegend=True))\n",
    "    # # =========================================================================================================== #\n",
    "    \n",
    "    print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "          .format(0, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "    \n",
    "    for epoch in range(args.epoch-1):  # loop over the dataset multiple times\n",
    "        \n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "\n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs_RMSE.append(train_acc_RMSE)\n",
    "        val_accs_RMSE.append(val_acc_RMSE)\n",
    "        train_accs_R2.append(train_acc_R2)\n",
    "        val_accs_R2.append(val_acc_R2)\n",
    "        # ============================ #\n",
    "\n",
    "        # # ===== Visdom visualizing ============================================================================== #\n",
    "        axis.append(epoch+1)\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot1, update='replace')\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot2, update='replace')\n",
    "        # # ====================================================================================================== #\n",
    "        \n",
    "        print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch+1, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc_RMSE, test_acc_R2, Pred_data, True_data = test(model, partition, args)\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    \n",
    "    result['train_accs_RMSE'] = train_accs_RMSE\n",
    "    result['train_accs_R2'] = train_accs_R2\n",
    "    result['val_accs_RMSE'] = val_accs_RMSE\n",
    "    result['val_accs_R2'] = val_accs_R2\n",
    "#     result['train_acc'] = train_acc\n",
    "#     result['val_acc'] = val_acc\n",
    "    result['test_RMSE'] = test_acc_RMSE\n",
    "    result['test_R2'] = test_acc_R2\n",
    "    \n",
    "    result['test_pred'] = Pred_data\n",
    "    result['test_true'] = True_data\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNi-f5HyeJYi"
   },
   "source": [
    "# 6. LSTM Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4Bu6kFUdizj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset :214 size of valset :47 size of testset :47\n",
      "\n",
      " exp_1\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=2, n_layers=4, batch_size=8, hid_dim=32, epoch=200, lr=0.0001, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 0.05/0.05, Loss(train/val) 0.26715/0.23590. Took 0.27 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.05/0.05, Loss(train/val) 0.25462/0.22425. Took 0.27 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.05/0.05, Loss(train/val) 0.24032/0.20935. Took 0.27 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.05/0.05, Loss(train/val) 0.22451/0.19428. Took 0.28 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.05/0.04, Loss(train/val) 0.20758/0.17831. Took 0.28 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.05/0.04, Loss(train/val) 0.19057/0.16088. Took 0.27 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.05/0.04, Loss(train/val) 0.17146/0.14334. Took 0.28 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.15104/0.12397. Took 0.28 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.12850/0.10245. Took 0.29 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.10349/0.07984. Took 0.29 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.04/0.03, Loss(train/val) 0.07771/0.05906. Took 0.27 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05617/0.04694. Took 0.27 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04455/0.04404. Took 0.28 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03990/0.04440. Took 0.27 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03962/0.04464. Took 0.28 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03871/0.04457. Took 0.28 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03742/0.04457. Took 0.27 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03829/0.04447. Took 0.28 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03860/0.04438. Took 0.27 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03848/0.04420. Took 0.27 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03773/0.04412. Took 0.27 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03744/0.04419. Took 0.27 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03697/0.04420. Took 0.29 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03674/0.04413. Took 0.30 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03828/0.04401. Took 0.28 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03846/0.04389. Took 0.27 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03693/0.04371. Took 0.27 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03598/0.04355. Took 0.27 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03724/0.04363. Took 0.27 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03560/0.04364. Took 0.27 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03600/0.04364. Took 0.29 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03693/0.04348. Took 0.27 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03707/0.04323. Took 0.28 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03641/0.04316. Took 0.28 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03690/0.04314. Took 0.28 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03732/0.04310. Took 0.28 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03712/0.04294. Took 0.28 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03789/0.04289. Took 0.30 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03543/0.04278. Took 0.29 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03585/0.04278. Took 0.31 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03664/0.04266. Took 0.27 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03703/0.04252. Took 0.28 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03675/0.04254. Took 0.29 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03600/0.04245. Took 0.27 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03714/0.04230. Took 0.27 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03686/0.04219. Took 0.27 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03646/0.04204. Took 0.26 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03636/0.04200. Took 0.27 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03593/0.04195. Took 0.27 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03428/0.04184. Took 0.27 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03574/0.04173. Took 0.27 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03488/0.04157. Took 0.27 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03482/0.04145. Took 0.27 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03464/0.04142. Took 0.27 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03582/0.04133. Took 0.27 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03445/0.04119. Took 0.27 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03482/0.04091. Took 0.27 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03368/0.04084. Took 0.27 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03399/0.04072. Took 0.27 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03527/0.04061. Took 0.27 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03464/0.04041. Took 0.27 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03436/0.04026. Took 0.27 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03490/0.04012. Took 0.27 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03519/0.03991. Took 0.27 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03395/0.03981. Took 0.27 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03511/0.03963. Took 0.27 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03407/0.03949. Took 0.27 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03416/0.03922. Took 0.27 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03439/0.03904. Took 0.27 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03371/0.03886. Took 0.26 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03237/0.03872. Took 0.27 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03376/0.03868. Took 0.27 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03405/0.03843. Took 0.27 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03358/0.03822. Took 0.27 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03113/0.03798. Took 0.27 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03286/0.03779. Took 0.27 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03200/0.03757. Took 0.27 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03291/0.03744. Took 0.27 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03221/0.03728. Took 0.27 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03045/0.03706. Took 0.27 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03262/0.03689. Took 0.27 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03318/0.03653. Took 0.27 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03076/0.03637. Took 0.27 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03108/0.03624. Took 0.27 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03247/0.03603. Took 0.27 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03054/0.03577. Took 0.27 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03258/0.03533. Took 0.27 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03173/0.03515. Took 0.26 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03020/0.03502. Took 0.27 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03150/0.03462. Took 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03022/0.03430. Took 0.27 sec\n",
      "Epoch 91, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03012/0.03419. Took 0.26 sec\n",
      "Epoch 92, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03038/0.03395. Took 0.27 sec\n",
      "Epoch 93, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02884/0.03373. Took 0.27 sec\n",
      "Epoch 94, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02997/0.03353. Took 0.27 sec\n",
      "Epoch 95, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03075/0.03319. Took 0.27 sec\n",
      "Epoch 96, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03045/0.03289. Took 0.27 sec\n",
      "Epoch 97, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02917/0.03263. Took 0.27 sec\n",
      "Epoch 98, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02828/0.03237. Took 0.27 sec\n",
      "Epoch 99, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02875/0.03217. Took 0.27 sec\n",
      "Epoch 100, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03055/0.03192. Took 0.27 sec\n",
      "Epoch 101, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02762/0.03167. Took 0.27 sec\n",
      "Epoch 102, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02810/0.03139. Took 0.27 sec\n",
      "Epoch 103, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02797/0.03123. Took 0.27 sec\n",
      "Epoch 104, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02879/0.03095. Took 0.27 sec\n",
      "Epoch 105, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02680/0.03070. Took 0.27 sec\n",
      "Epoch 106, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02847/0.03063. Took 0.27 sec\n",
      "Epoch 107, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02707/0.03040. Took 0.27 sec\n",
      "Epoch 108, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02814/0.02994. Took 0.27 sec\n",
      "Epoch 109, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02740/0.02983. Took 0.27 sec\n",
      "Epoch 110, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02751/0.02965. Took 0.27 sec\n",
      "Epoch 111, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02855/0.02945. Took 0.27 sec\n",
      "Epoch 112, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02681/0.02933. Took 0.27 sec\n",
      "Epoch 113, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02572/0.02906. Took 0.27 sec\n",
      "Epoch 114, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02749/0.02886. Took 0.27 sec\n",
      "Epoch 115, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02708/0.02847. Took 0.29 sec\n",
      "Epoch 116, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02733/0.02838. Took 0.29 sec\n",
      "Epoch 117, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02672/0.02805. Took 0.27 sec\n",
      "Epoch 118, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02648/0.02782. Took 0.27 sec\n",
      "Epoch 119, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02703/0.02773. Took 0.27 sec\n",
      "Epoch 120, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02716/0.02765. Took 0.27 sec\n",
      "Epoch 121, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02820/0.02737. Took 0.27 sec\n",
      "Epoch 122, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02581/0.02719. Took 0.27 sec\n",
      "Epoch 123, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02533/0.02708. Took 0.28 sec\n",
      "Epoch 124, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02476/0.02687. Took 0.27 sec\n",
      "Epoch 125, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02704/0.02679. Took 0.27 sec\n",
      "Epoch 126, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02513/0.02683. Took 0.27 sec\n",
      "Epoch 127, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02539/0.02654. Took 0.27 sec\n",
      "Epoch 128, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02478/0.02647. Took 0.27 sec\n",
      "Epoch 129, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02564/0.02628. Took 0.27 sec\n",
      "Epoch 130, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02619/0.02613. Took 0.27 sec\n",
      "Epoch 131, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02506/0.02610. Took 0.27 sec\n",
      "Epoch 132, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02602/0.02584. Took 0.27 sec\n",
      "Epoch 133, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02488/0.02579. Took 0.27 sec\n",
      "Epoch 134, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02522/0.02572. Took 0.27 sec\n",
      "Epoch 135, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02336/0.02550. Took 0.27 sec\n",
      "Epoch 136, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02487/0.02546. Took 0.28 sec\n",
      "Epoch 137, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02524/0.02533. Took 0.28 sec\n",
      "Epoch 138, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02468/0.02517. Took 0.27 sec\n",
      "Epoch 139, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02618/0.02520. Took 0.29 sec\n",
      "Epoch 140, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02630/0.02511. Took 0.30 sec\n",
      "Epoch 141, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02494/0.02508. Took 0.29 sec\n",
      "Epoch 142, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02532/0.02509. Took 0.27 sec\n",
      "Epoch 143, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02382/0.02493. Took 0.30 sec\n",
      "Epoch 144, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02481/0.02482. Took 0.30 sec\n",
      "Epoch 145, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02549/0.02486. Took 0.33 sec\n",
      "Epoch 146, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02417/0.02468. Took 0.37 sec\n",
      "Epoch 147, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02458/0.02472. Took 0.29 sec\n",
      "Epoch 148, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02489/0.02458. Took 0.29 sec\n",
      "Epoch 149, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02443/0.02455. Took 0.28 sec\n",
      "Epoch 150, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02464/0.02455. Took 0.28 sec\n",
      "Epoch 151, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02471/0.02453. Took 0.29 sec\n",
      "Epoch 152, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02395/0.02442. Took 0.30 sec\n",
      "Epoch 153, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02460/0.02448. Took 0.28 sec\n",
      "Epoch 154, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02457/0.02437. Took 0.27 sec\n",
      "Epoch 155, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02396/0.02441. Took 0.28 sec\n",
      "Epoch 156, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02535/0.02442. Took 0.27 sec\n",
      "Epoch 157, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02464/0.02438. Took 0.27 sec\n",
      "Epoch 158, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02408/0.02432. Took 0.27 sec\n",
      "Epoch 159, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02498/0.02428. Took 0.28 sec\n",
      "Epoch 160, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02465/0.02432. Took 0.27 sec\n",
      "Epoch 161, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02426/0.02423. Took 0.27 sec\n",
      "Epoch 162, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02499/0.02410. Took 0.28 sec\n",
      "Epoch 163, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02408/0.02423. Took 0.27 sec\n",
      "Epoch 164, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02560/0.02416. Took 0.27 sec\n",
      "Epoch 165, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02522/0.02423. Took 0.27 sec\n",
      "Epoch 166, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02444/0.02419. Took 0.28 sec\n",
      "Epoch 167, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02503/0.02396. Took 0.28 sec\n",
      "Epoch 168, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02465/0.02406. Took 0.30 sec\n",
      "Epoch 169, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02434/0.02410. Took 0.28 sec\n",
      "Epoch 170, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02544/0.02395. Took 0.30 sec\n",
      "Epoch 171, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02377/0.02394. Took 0.27 sec\n",
      "Epoch 172, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02442/0.02410. Took 0.28 sec\n",
      "Epoch 173, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02380/0.02400. Took 0.28 sec\n",
      "Epoch 174, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02475/0.02393. Took 0.28 sec\n",
      "Epoch 175, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02410/0.02385. Took 0.28 sec\n",
      "Epoch 176, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02373/0.02393. Took 0.27 sec\n",
      "Epoch 177, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02439/0.02400. Took 0.28 sec\n",
      "Epoch 178, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02373/0.02415. Took 0.28 sec\n",
      "Epoch 179, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02420/0.02396. Took 0.27 sec\n",
      "Epoch 180, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02458/0.02399. Took 0.27 sec\n",
      "Epoch 181, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02362/0.02393. Took 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02430/0.02400. Took 0.27 sec\n",
      "Epoch 183, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02354/0.02396. Took 0.27 sec\n",
      "Epoch 184, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02412/0.02389. Took 0.27 sec\n",
      "Epoch 185, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02387/0.02384. Took 0.27 sec\n",
      "Epoch 186, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02477/0.02392. Took 0.28 sec\n",
      "Epoch 187, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02389/0.02387. Took 0.27 sec\n",
      "Epoch 188, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02213/0.02392. Took 0.27 sec\n",
      "Epoch 189, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02456/0.02409. Took 0.28 sec\n",
      "Epoch 190, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02458/0.02391. Took 0.27 sec\n",
      "Epoch 191, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02416/0.02377. Took 0.27 sec\n",
      "Epoch 192, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02376/0.02381. Took 0.28 sec\n",
      "Epoch 193, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02397/0.02379. Took 0.28 sec\n",
      "Epoch 194, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02368/0.02377. Took 0.27 sec\n",
      "Epoch 195, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02291/0.02378. Took 0.27 sec\n",
      "Epoch 196, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02251/0.02372. Took 0.27 sec\n",
      "Epoch 197, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02389/0.02372. Took 0.27 sec\n",
      "Epoch 198, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02254/0.02375. Took 0.27 sec\n",
      "Epoch 199, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02435/0.02369. Took 0.27 sec\n",
      "\n",
      " exp_2\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=2, n_layers=4, batch_size=8, hid_dim=32, epoch=300, lr=0.0001, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.09608/0.08408. Took 0.27 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.08956/0.07858. Took 0.27 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.08330/0.07366. Took 0.28 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.07785/0.06943. Took 0.28 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.07268/0.06590. Took 0.27 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.06913/0.06334. Took 0.27 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.04/0.04, Loss(train/val) 0.06615/0.06095. Took 0.27 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.04/0.03, Loss(train/val) 0.06311/0.05873. Took 0.27 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.04/0.03, Loss(train/val) 0.06054/0.05675. Took 0.27 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.04/0.03, Loss(train/val) 0.05777/0.05488. Took 0.28 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05529/0.05328. Took 0.28 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05302/0.05190. Took 0.27 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05139/0.05029. Took 0.27 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04882/0.04882. Took 0.27 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04634/0.04758. Took 0.27 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04473/0.04664. Took 0.27 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04317/0.04600. Took 0.27 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04178/0.04556. Took 0.27 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04104/0.04533. Took 0.27 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03984/0.04525. Took 0.27 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03951/0.04528. Took 0.27 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03869/0.04536. Took 0.27 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03898/0.04546. Took 0.27 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03849/0.04555. Took 0.27 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03835/0.04564. Took 0.28 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03843/0.04571. Took 0.27 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03818/0.04577. Took 0.27 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03814/0.04581. Took 0.27 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03812/0.04583. Took 0.27 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03793/0.04587. Took 0.27 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03795/0.04589. Took 0.27 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03744/0.04591. Took 0.27 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03819/0.04589. Took 0.27 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03775/0.04588. Took 0.27 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03803/0.04587. Took 0.27 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03794/0.04581. Took 0.27 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03773/0.04579. Took 0.27 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03736/0.04578. Took 0.27 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03802/0.04574. Took 0.27 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03833/0.04570. Took 0.28 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03719/0.04567. Took 0.27 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03761/0.04565. Took 0.27 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03771/0.04560. Took 0.27 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03795/0.04557. Took 0.27 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03744/0.04557. Took 0.27 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03738/0.04550. Took 0.27 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03801/0.04546. Took 0.27 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03724/0.04543. Took 0.27 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03804/0.04538. Took 0.27 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03734/0.04537. Took 0.27 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03710/0.04535. Took 0.27 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03742/0.04530. Took 0.27 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03756/0.04527. Took 0.27 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03714/0.04525. Took 0.28 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03771/0.04522. Took 0.27 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03712/0.04518. Took 0.27 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03722/0.04513. Took 0.28 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03705/0.04511. Took 0.27 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03753/0.04508. Took 0.28 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03690/0.04504. Took 0.27 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03733/0.04498. Took 0.28 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03691/0.04492. Took 0.27 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03639/0.04490. Took 0.27 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03696/0.04488. Took 0.27 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03683/0.04483. Took 0.27 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03710/0.04475. Took 0.27 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03724/0.04466. Took 0.27 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03629/0.04458. Took 0.27 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03661/0.04449. Took 0.27 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03653/0.04442. Took 0.27 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03624/0.04437. Took 0.27 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03619/0.04433. Took 0.27 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03605/0.04428. Took 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03598/0.04418. Took 0.27 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03616/0.04407. Took 0.27 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03576/0.04397. Took 0.27 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03583/0.04385. Took 0.27 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03522/0.04378. Took 0.27 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03593/0.04361. Took 0.27 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03545/0.04348. Took 0.27 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03553/0.04333. Took 0.27 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03581/0.04317. Took 0.28 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03513/0.04301. Took 0.27 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03527/0.04285. Took 0.27 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03518/0.04269. Took 0.27 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03517/0.04249. Took 0.27 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03476/0.04227. Took 0.27 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03427/0.04207. Took 0.27 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03436/0.04184. Took 0.27 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03454/0.04161. Took 0.27 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03377/0.04138. Took 0.27 sec\n",
      "Epoch 91, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03369/0.04113. Took 0.28 sec\n",
      "Epoch 92, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03349/0.04084. Took 0.28 sec\n",
      "Epoch 93, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03343/0.04053. Took 0.27 sec\n",
      "Epoch 94, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03309/0.04021. Took 0.27 sec\n",
      "Epoch 95, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03292/0.03984. Took 0.27 sec\n",
      "Epoch 96, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03323/0.03950. Took 0.27 sec\n",
      "Epoch 97, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03222/0.03916. Took 0.27 sec\n",
      "Epoch 98, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03219/0.03881. Took 0.28 sec\n",
      "Epoch 99, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03233/0.03840. Took 0.27 sec\n",
      "Epoch 100, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03079/0.03794. Took 0.27 sec\n",
      "Epoch 101, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03111/0.03746. Took 0.27 sec\n",
      "Epoch 102, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03066/0.03690. Took 0.27 sec\n",
      "Epoch 103, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03033/0.03631. Took 0.27 sec\n",
      "Epoch 104, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03010/0.03567. Took 0.27 sec\n",
      "Epoch 105, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02998/0.03497. Took 0.28 sec\n",
      "Epoch 106, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02968/0.03430. Took 0.27 sec\n",
      "Epoch 107, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02870/0.03368. Took 0.27 sec\n",
      "Epoch 108, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02891/0.03308. Took 0.28 sec\n",
      "Epoch 109, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02839/0.03246. Took 0.27 sec\n",
      "Epoch 110, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02792/0.03187. Took 0.28 sec\n",
      "Epoch 111, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02723/0.03134. Took 0.27 sec\n",
      "Epoch 112, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02755/0.03075. Took 0.27 sec\n",
      "Epoch 113, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02737/0.03025. Took 0.27 sec\n",
      "Epoch 114, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02711/0.02967. Took 0.27 sec\n",
      "Epoch 115, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02595/0.02922. Took 0.28 sec\n",
      "Epoch 116, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02652/0.02884. Took 0.28 sec\n",
      "Epoch 117, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02538/0.02839. Took 0.27 sec\n",
      "Epoch 118, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02613/0.02794. Took 0.27 sec\n",
      "Epoch 119, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02514/0.02753. Took 0.27 sec\n",
      "Epoch 120, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02565/0.02719. Took 0.27 sec\n",
      "Epoch 121, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02559/0.02682. Took 0.27 sec\n",
      "Epoch 122, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02493/0.02652. Took 0.27 sec\n",
      "Epoch 123, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02459/0.02623. Took 0.27 sec\n",
      "Epoch 124, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02468/0.02599. Took 0.27 sec\n",
      "Epoch 125, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02452/0.02580. Took 0.27 sec\n",
      "Epoch 126, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02439/0.02556. Took 0.27 sec\n",
      "Epoch 127, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02477/0.02537. Took 0.27 sec\n",
      "Epoch 128, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02466/0.02516. Took 0.27 sec\n",
      "Epoch 129, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02426/0.02502. Took 0.27 sec\n",
      "Epoch 130, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02459/0.02485. Took 0.27 sec\n",
      "Epoch 131, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02451/0.02473. Took 0.27 sec\n",
      "Epoch 132, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02403/0.02463. Took 0.27 sec\n",
      "Epoch 133, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02383/0.02453. Took 0.27 sec\n",
      "Epoch 134, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02380/0.02444. Took 0.28 sec\n",
      "Epoch 135, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02398/0.02440. Took 0.27 sec\n",
      "Epoch 136, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02395/0.02433. Took 0.27 sec\n",
      "Epoch 137, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02372/0.02427. Took 0.27 sec\n",
      "Epoch 138, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02383/0.02423. Took 0.27 sec\n",
      "Epoch 139, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02338/0.02417. Took 0.27 sec\n",
      "Epoch 140, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02369/0.02410. Took 0.27 sec\n",
      "Epoch 141, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02365/0.02408. Took 0.27 sec\n",
      "Epoch 142, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02387/0.02406. Took 0.27 sec\n",
      "Epoch 143, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02276/0.02403. Took 0.27 sec\n",
      "Epoch 144, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02342/0.02400. Took 0.28 sec\n",
      "Epoch 145, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02312/0.02398. Took 0.28 sec\n",
      "Epoch 146, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02305/0.02398. Took 0.29 sec\n",
      "Epoch 147, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02348/0.02398. Took 0.28 sec\n",
      "Epoch 148, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02362/0.02397. Took 0.28 sec\n",
      "Epoch 149, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02317/0.02395. Took 0.27 sec\n",
      "Epoch 150, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02384/0.02394. Took 0.29 sec\n",
      "Epoch 151, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02346/0.02394. Took 0.27 sec\n",
      "Epoch 152, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02347/0.02394. Took 0.27 sec\n",
      "Epoch 153, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02346/0.02392. Took 0.27 sec\n",
      "Epoch 154, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02356/0.02391. Took 0.28 sec\n",
      "Epoch 155, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02364/0.02391. Took 0.27 sec\n",
      "Epoch 156, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02308/0.02392. Took 0.28 sec\n",
      "Epoch 157, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02331/0.02391. Took 0.27 sec\n",
      "Epoch 158, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02339/0.02390. Took 0.27 sec\n",
      "Epoch 159, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02332/0.02388. Took 0.27 sec\n",
      "Epoch 160, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02264/0.02390. Took 0.27 sec\n",
      "Epoch 161, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02329/0.02391. Took 0.27 sec\n",
      "Epoch 162, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02331/0.02391. Took 0.28 sec\n",
      "Epoch 163, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02283/0.02389. Took 0.28 sec\n",
      "Epoch 164, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02356/0.02389. Took 0.28 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02319/0.02388. Took 0.28 sec\n",
      "Epoch 166, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02305/0.02390. Took 0.27 sec\n",
      "Epoch 167, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02303/0.02390. Took 0.27 sec\n",
      "Epoch 168, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02383/0.02390. Took 0.27 sec\n",
      "Epoch 169, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02327/0.02389. Took 0.27 sec\n",
      "Epoch 170, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02344/0.02390. Took 0.27 sec\n",
      "Epoch 171, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02344/0.02390. Took 0.27 sec\n",
      "Epoch 172, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02291/0.02393. Took 0.27 sec\n",
      "Epoch 173, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02260/0.02392. Took 0.27 sec\n",
      "Epoch 174, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02286/0.02392. Took 0.28 sec\n",
      "Epoch 175, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02295/0.02391. Took 0.27 sec\n",
      "Epoch 176, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02203/0.02389. Took 0.27 sec\n",
      "Epoch 177, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02312/0.02388. Took 0.27 sec\n",
      "Epoch 178, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02245/0.02389. Took 0.27 sec\n",
      "Epoch 179, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02348/0.02390. Took 0.27 sec\n",
      "Epoch 180, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02302/0.02392. Took 0.27 sec\n",
      "Epoch 181, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02209/0.02392. Took 0.27 sec\n",
      "Epoch 182, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02309/0.02392. Took 0.28 sec\n",
      "Epoch 183, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02330/0.02392. Took 0.28 sec\n",
      "Epoch 184, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02257/0.02392. Took 0.28 sec\n",
      "Epoch 185, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02280/0.02393. Took 0.27 sec\n",
      "Epoch 186, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02258/0.02394. Took 0.27 sec\n",
      "Epoch 187, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02319/0.02392. Took 0.27 sec\n",
      "Epoch 188, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02255/0.02390. Took 0.27 sec\n",
      "Epoch 189, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02245/0.02391. Took 0.27 sec\n",
      "Epoch 190, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02248/0.02393. Took 0.27 sec\n",
      "Epoch 191, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02293/0.02394. Took 0.27 sec\n",
      "Epoch 192, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02267/0.02393. Took 0.27 sec\n",
      "Epoch 193, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02282/0.02394. Took 0.27 sec\n",
      "Epoch 194, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02253/0.02395. Took 0.28 sec\n",
      "Epoch 195, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02294/0.02395. Took 0.28 sec\n",
      "Epoch 196, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02263/0.02394. Took 0.27 sec\n",
      "Epoch 197, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02311/0.02395. Took 0.27 sec\n",
      "Epoch 198, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02284/0.02395. Took 0.27 sec\n",
      "Epoch 199, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02291/0.02396. Took 0.27 sec\n",
      "Epoch 200, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02326/0.02395. Took 0.27 sec\n",
      "Epoch 201, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02289/0.02396. Took 0.27 sec\n",
      "Epoch 202, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02228/0.02398. Took 0.27 sec\n",
      "Epoch 203, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02270/0.02398. Took 0.27 sec\n",
      "Epoch 204, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02228/0.02397. Took 0.27 sec\n",
      "Epoch 205, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02216/0.02398. Took 0.27 sec\n",
      "Epoch 206, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02230/0.02398. Took 0.27 sec\n",
      "Epoch 207, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02261/0.02398. Took 0.27 sec\n",
      "Epoch 208, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02288/0.02399. Took 0.28 sec\n",
      "Epoch 209, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02304/0.02398. Took 0.28 sec\n",
      "Epoch 210, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02244/0.02400. Took 0.27 sec\n",
      "Epoch 211, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02271/0.02400. Took 0.27 sec\n",
      "Epoch 212, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02241/0.02399. Took 0.27 sec\n",
      "Epoch 213, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02248/0.02398. Took 0.27 sec\n",
      "Epoch 214, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02168/0.02397. Took 0.27 sec\n",
      "Epoch 215, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02359/0.02398. Took 0.27 sec\n",
      "Epoch 216, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02209/0.02397. Took 0.27 sec\n",
      "Epoch 217, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02234/0.02397. Took 0.27 sec\n",
      "Epoch 218, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02218/0.02396. Took 0.28 sec\n",
      "Epoch 219, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02281/0.02395. Took 0.28 sec\n",
      "Epoch 220, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02210/0.02396. Took 0.29 sec\n",
      "Epoch 221, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02278/0.02397. Took 0.28 sec\n",
      "Epoch 222, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02269/0.02397. Took 0.28 sec\n",
      "Epoch 223, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02260/0.02398. Took 0.28 sec\n",
      "Epoch 224, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02257/0.02401. Took 0.28 sec\n",
      "Epoch 225, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02218/0.02402. Took 0.28 sec\n",
      "Epoch 226, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02258/0.02402. Took 0.28 sec\n",
      "Epoch 227, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02184/0.02404. Took 0.28 sec\n",
      "Epoch 228, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02215/0.02404. Took 0.29 sec\n",
      "Epoch 229, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02301/0.02403. Took 0.28 sec\n",
      "Epoch 230, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02211/0.02402. Took 0.28 sec\n",
      "Epoch 231, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02250/0.02406. Took 0.27 sec\n",
      "Epoch 232, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02248/0.02404. Took 0.27 sec\n",
      "Epoch 233, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02239/0.02405. Took 0.27 sec\n",
      "Epoch 234, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02188/0.02407. Took 0.27 sec\n",
      "Epoch 235, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02208/0.02409. Took 0.27 sec\n",
      "Epoch 236, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02192/0.02406. Took 0.27 sec\n",
      "Epoch 237, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02253/0.02406. Took 0.27 sec\n",
      "Epoch 238, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02209/0.02407. Took 0.28 sec\n",
      "Epoch 239, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02184/0.02408. Took 0.28 sec\n",
      "Epoch 240, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02223/0.02408. Took 0.27 sec\n",
      "Epoch 241, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02238/0.02403. Took 0.27 sec\n",
      "Epoch 242, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02226/0.02403. Took 0.27 sec\n",
      "Epoch 243, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02289/0.02405. Took 0.27 sec\n",
      "Epoch 244, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02247/0.02406. Took 0.27 sec\n",
      "Epoch 245, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02228/0.02406. Took 0.27 sec\n",
      "Epoch 246, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02192/0.02408. Took 0.27 sec\n",
      "Epoch 247, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02116/0.02410. Took 0.27 sec\n",
      "Epoch 248, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02190/0.02409. Took 0.28 sec\n",
      "Epoch 249, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02203/0.02406. Took 0.28 sec\n",
      "Epoch 250, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02167/0.02407. Took 0.27 sec\n",
      "Epoch 251, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02244/0.02407. Took 0.27 sec\n",
      "Epoch 252, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02209/0.02405. Took 0.27 sec\n",
      "Epoch 253, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02163/0.02406. Took 0.27 sec\n",
      "Epoch 254, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02199/0.02406. Took 0.28 sec\n",
      "Epoch 255, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02207/0.02407. Took 0.27 sec\n",
      "Epoch 256, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02215/0.02408. Took 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02154/0.02408. Took 0.27 sec\n",
      "Epoch 258, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02237/0.02407. Took 0.27 sec\n",
      "Epoch 259, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02215/0.02407. Took 0.27 sec\n",
      "Epoch 260, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02210/0.02406. Took 0.28 sec\n",
      "Epoch 261, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02168/0.02406. Took 0.27 sec\n",
      "Epoch 262, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02205/0.02408. Took 0.27 sec\n",
      "Epoch 263, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02169/0.02412. Took 0.27 sec\n",
      "Epoch 264, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02214/0.02411. Took 0.27 sec\n",
      "Epoch 265, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02111/0.02411. Took 0.27 sec\n",
      "Epoch 266, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02214/0.02411. Took 0.27 sec\n",
      "Epoch 267, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02187/0.02412. Took 0.27 sec\n",
      "Epoch 268, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02167/0.02414. Took 0.27 sec\n",
      "Epoch 269, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02164/0.02414. Took 0.27 sec\n",
      "Epoch 270, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02190/0.02415. Took 0.27 sec\n",
      "Epoch 271, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02144/0.02413. Took 0.27 sec\n",
      "Epoch 272, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02212/0.02416. Took 0.27 sec\n",
      "Epoch 273, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02171/0.02418. Took 0.27 sec\n",
      "Epoch 274, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02194/0.02416. Took 0.28 sec\n",
      "Epoch 275, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02175/0.02418. Took 0.27 sec\n",
      "Epoch 276, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02145/0.02415. Took 0.27 sec\n",
      "Epoch 277, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02203/0.02416. Took 0.27 sec\n",
      "Epoch 278, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02166/0.02416. Took 0.27 sec\n",
      "Epoch 279, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02155/0.02419. Took 0.27 sec\n",
      "Epoch 280, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02168/0.02416. Took 0.27 sec\n",
      "Epoch 281, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02124/0.02416. Took 0.27 sec\n",
      "Epoch 282, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02118/0.02416. Took 0.28 sec\n",
      "Epoch 283, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02158/0.02419. Took 0.28 sec\n",
      "Epoch 284, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02130/0.02420. Took 0.27 sec\n",
      "Epoch 285, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02178/0.02417. Took 0.27 sec\n",
      "Epoch 286, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02415. Took 0.27 sec\n",
      "Epoch 287, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02183/0.02418. Took 0.27 sec\n",
      "Epoch 288, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02172/0.02421. Took 0.27 sec\n",
      "Epoch 289, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02135/0.02418. Took 0.27 sec\n",
      "Epoch 290, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02138/0.02420. Took 0.27 sec\n",
      "Epoch 291, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02166/0.02419. Took 0.27 sec\n",
      "Epoch 292, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02131/0.02419. Took 0.27 sec\n",
      "Epoch 293, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02130/0.02423. Took 0.27 sec\n",
      "Epoch 294, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02136/0.02421. Took 0.27 sec\n",
      "Epoch 295, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02106/0.02421. Took 0.27 sec\n",
      "Epoch 296, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02150/0.02420. Took 0.28 sec\n",
      "Epoch 297, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02176/0.02421. Took 0.27 sec\n",
      "Epoch 298, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02116/0.02424. Took 0.27 sec\n",
      "Epoch 299, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02144/0.02424. Took 0.27 sec\n",
      "\n",
      " exp_3\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=2, n_layers=4, batch_size=8, hid_dim=32, epoch=400, lr=0.0001, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05453/0.05211. Took 0.27 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05039/0.04974. Took 0.27 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04703/0.04789. Took 0.27 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04482/0.04663. Took 0.27 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04273/0.04585. Took 0.27 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04210/0.04547. Took 0.28 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.04068/0.04535. Took 0.28 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03937/0.04535. Took 0.28 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03950/0.04544. Took 0.27 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03882/0.04556. Took 0.27 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03920/0.04568. Took 0.27 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03831/0.04581. Took 0.27 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03855/0.04592. Took 0.27 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03829/0.04601. Took 0.27 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03866/0.04608. Took 0.27 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03829/0.04614. Took 0.28 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03808/0.04620. Took 0.27 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03790/0.04627. Took 0.28 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03815/0.04632. Took 0.28 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03842/0.04637. Took 0.28 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03832/0.04639. Took 0.27 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03756/0.04642. Took 0.27 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03836/0.04644. Took 0.27 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03812/0.04645. Took 0.27 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03821/0.04646. Took 0.27 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03764/0.04647. Took 0.27 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03765/0.04647. Took 0.27 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03769/0.04646. Took 0.27 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03785/0.04647. Took 0.27 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03854/0.04646. Took 0.27 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03795/0.04645. Took 0.27 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03823/0.04646. Took 0.28 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03838/0.04645. Took 0.28 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03821/0.04644. Took 0.27 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03819/0.04645. Took 0.27 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03812/0.04645. Took 0.27 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03770/0.04645. Took 0.27 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03802/0.04648. Took 0.27 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03830/0.04648. Took 0.27 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03836/0.04645. Took 0.27 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03798/0.04645. Took 0.27 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03810/0.04643. Took 0.27 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03831/0.04642. Took 0.27 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03822/0.04643. Took 0.27 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03753/0.04643. Took 0.27 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03796/0.04643. Took 0.28 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03831/0.04643. Took 0.27 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03844/0.04642. Took 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03813/0.04643. Took 0.27 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03827/0.04642. Took 0.27 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03813/0.04640. Took 0.27 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03824/0.04641. Took 0.27 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03804/0.04641. Took 0.28 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03789/0.04640. Took 0.28 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03822/0.04640. Took 0.27 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03828/0.04639. Took 0.27 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03775/0.04638. Took 0.27 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03837/0.04638. Took 0.27 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03761/0.04639. Took 0.27 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03801/0.04636. Took 0.27 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03803/0.04635. Took 0.27 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03823/0.04637. Took 0.27 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03834/0.04636. Took 0.27 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03812/0.04634. Took 0.27 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03811/0.04634. Took 0.27 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03846/0.04633. Took 0.27 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03844/0.04633. Took 0.28 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03779/0.04632. Took 0.27 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03745/0.04631. Took 0.27 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03783/0.04630. Took 0.27 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03831/0.04628. Took 0.28 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03835/0.04627. Took 0.27 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03831/0.04625. Took 0.27 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03802/0.04624. Took 0.27 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03793/0.04623. Took 0.27 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03755/0.04621. Took 0.27 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03789/0.04621. Took 0.27 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03778/0.04620. Took 0.27 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03795/0.04619. Took 0.27 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03817/0.04617. Took 0.27 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03771/0.04615. Took 0.27 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03827/0.04612. Took 0.27 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03787/0.04610. Took 0.27 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03801/0.04607. Took 0.27 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03790/0.04606. Took 0.27 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03781/0.04604. Took 0.27 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03785/0.04599. Took 0.28 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03792/0.04594. Took 0.28 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03818/0.04587. Took 0.27 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03743/0.04579. Took 0.27 sec\n",
      "Epoch 90, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03743/0.04574. Took 0.27 sec\n",
      "Epoch 91, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03761/0.04567. Took 0.27 sec\n",
      "Epoch 92, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03744/0.04560. Took 0.27 sec\n",
      "Epoch 93, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03741/0.04550. Took 0.27 sec\n",
      "Epoch 94, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03728/0.04539. Took 0.27 sec\n",
      "Epoch 95, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03729/0.04525. Took 0.27 sec\n",
      "Epoch 96, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03673/0.04508. Took 0.27 sec\n",
      "Epoch 97, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03676/0.04491. Took 0.27 sec\n",
      "Epoch 98, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03635/0.04470. Took 0.27 sec\n",
      "Epoch 99, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03719/0.04443. Took 0.27 sec\n",
      "Epoch 100, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03629/0.04410. Took 0.27 sec\n",
      "Epoch 101, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03583/0.04371. Took 0.28 sec\n",
      "Epoch 102, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03648/0.04324. Took 0.27 sec\n",
      "Epoch 103, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03574/0.04268. Took 0.27 sec\n",
      "Epoch 104, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03480/0.04201. Took 0.27 sec\n",
      "Epoch 105, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03437/0.04123. Took 0.27 sec\n",
      "Epoch 106, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03358/0.04031. Took 0.27 sec\n",
      "Epoch 107, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03337/0.03926. Took 0.27 sec\n",
      "Epoch 108, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03239/0.03804. Took 0.27 sec\n",
      "Epoch 109, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03126/0.03679. Took 0.27 sec\n",
      "Epoch 110, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03111/0.03545. Took 0.28 sec\n",
      "Epoch 111, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02957/0.03409. Took 0.27 sec\n",
      "Epoch 112, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.02899/0.03269. Took 0.27 sec\n",
      "Epoch 113, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02838/0.03134. Took 0.27 sec\n",
      "Epoch 114, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02748/0.03001. Took 0.28 sec\n",
      "Epoch 115, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02771/0.02892. Took 0.28 sec\n",
      "Epoch 116, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02629/0.02795. Took 0.28 sec\n",
      "Epoch 117, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02669/0.02710. Took 0.27 sec\n",
      "Epoch 118, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02627/0.02640. Took 0.27 sec\n",
      "Epoch 119, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02598/0.02588. Took 0.27 sec\n",
      "Epoch 120, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02522/0.02545. Took 0.28 sec\n",
      "Epoch 121, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02535/0.02510. Took 0.27 sec\n",
      "Epoch 122, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02567/0.02482. Took 0.27 sec\n",
      "Epoch 123, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02479/0.02460. Took 0.27 sec\n",
      "Epoch 124, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02494/0.02436. Took 0.27 sec\n",
      "Epoch 125, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02446/0.02418. Took 0.27 sec\n",
      "Epoch 126, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02463/0.02405. Took 0.27 sec\n",
      "Epoch 127, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02430/0.02395. Took 0.27 sec\n",
      "Epoch 128, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02452/0.02386. Took 0.28 sec\n",
      "Epoch 129, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02510/0.02377. Took 0.28 sec\n",
      "Epoch 130, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02407/0.02374. Took 0.27 sec\n",
      "Epoch 131, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02371/0.02371. Took 0.27 sec\n",
      "Epoch 132, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02442/0.02368. Took 0.27 sec\n",
      "Epoch 133, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02465/0.02363. Took 0.27 sec\n",
      "Epoch 134, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02446/0.02361. Took 0.27 sec\n",
      "Epoch 135, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02378/0.02357. Took 0.27 sec\n",
      "Epoch 136, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02404/0.02355. Took 0.27 sec\n",
      "Epoch 137, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02404/0.02353. Took 0.27 sec\n",
      "Epoch 138, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02376/0.02353. Took 0.28 sec\n",
      "Epoch 139, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02391/0.02352. Took 0.28 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02437/0.02353. Took 0.27 sec\n",
      "Epoch 141, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02385/0.02352. Took 0.27 sec\n",
      "Epoch 142, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02413/0.02353. Took 0.27 sec\n",
      "Epoch 143, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02351/0.02351. Took 0.27 sec\n",
      "Epoch 144, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02370/0.02353. Took 0.27 sec\n",
      "Epoch 145, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02384/0.02351. Took 0.27 sec\n",
      "Epoch 146, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02336/0.02351. Took 0.27 sec\n",
      "Epoch 147, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02400/0.02354. Took 0.27 sec\n",
      "Epoch 148, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02383/0.02355. Took 0.27 sec\n",
      "Epoch 149, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02314/0.02355. Took 0.27 sec\n",
      "Epoch 150, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02443/0.02357. Took 0.27 sec\n",
      "Epoch 151, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02378/0.02359. Took 0.27 sec\n",
      "Epoch 152, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02380/0.02361. Took 0.27 sec\n",
      "Epoch 153, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02366/0.02361. Took 0.28 sec\n",
      "Epoch 154, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02357/0.02362. Took 0.27 sec\n",
      "Epoch 155, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02405/0.02363. Took 0.27 sec\n",
      "Epoch 156, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02333/0.02363. Took 0.27 sec\n",
      "Epoch 157, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02361/0.02364. Took 0.27 sec\n",
      "Epoch 158, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02376/0.02365. Took 0.27 sec\n",
      "Epoch 159, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02381/0.02366. Took 0.27 sec\n",
      "Epoch 160, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02321/0.02368. Took 0.27 sec\n",
      "Epoch 161, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02338/0.02369. Took 0.27 sec\n",
      "Epoch 162, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02390/0.02370. Took 0.27 sec\n",
      "Epoch 163, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02319/0.02369. Took 0.27 sec\n",
      "Epoch 164, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02359/0.02369. Took 0.27 sec\n",
      "Epoch 165, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02325/0.02367. Took 0.27 sec\n",
      "Epoch 166, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02372/0.02367. Took 0.28 sec\n",
      "Epoch 167, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02324/0.02368. Took 0.28 sec\n",
      "Epoch 168, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02347/0.02366. Took 0.27 sec\n",
      "Epoch 169, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02351/0.02367. Took 0.27 sec\n",
      "Epoch 170, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02420/0.02369. Took 0.27 sec\n",
      "Epoch 171, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02361/0.02368. Took 0.27 sec\n",
      "Epoch 172, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02358/0.02367. Took 0.27 sec\n",
      "Epoch 173, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02295/0.02367. Took 0.27 sec\n",
      "Epoch 174, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02297/0.02367. Took 0.28 sec\n",
      "Epoch 175, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02353/0.02366. Took 0.27 sec\n",
      "Epoch 176, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02333/0.02368. Took 0.27 sec\n",
      "Epoch 177, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02298/0.02369. Took 0.27 sec\n",
      "Epoch 178, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02323/0.02368. Took 0.28 sec\n",
      "Epoch 179, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02314/0.02369. Took 0.27 sec\n",
      "Epoch 180, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02365/0.02370. Took 0.27 sec\n",
      "Epoch 181, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02302/0.02370. Took 0.27 sec\n",
      "Epoch 182, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02338/0.02369. Took 0.27 sec\n",
      "Epoch 183, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02366/0.02371. Took 0.27 sec\n",
      "Epoch 184, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02294/0.02372. Took 0.27 sec\n",
      "Epoch 185, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02334/0.02372. Took 0.27 sec\n",
      "Epoch 186, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02360/0.02371. Took 0.27 sec\n",
      "Epoch 187, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02311/0.02370. Took 0.27 sec\n",
      "Epoch 188, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02290/0.02371. Took 0.27 sec\n",
      "Epoch 189, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02276/0.02372. Took 0.27 sec\n",
      "Epoch 190, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02366/0.02372. Took 0.27 sec\n",
      "Epoch 191, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02325/0.02372. Took 0.27 sec\n",
      "Epoch 192, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02333/0.02374. Took 0.27 sec\n",
      "Epoch 193, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02322/0.02375. Took 0.27 sec\n",
      "Epoch 194, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02335/0.02375. Took 0.27 sec\n",
      "Epoch 195, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02341/0.02377. Took 0.27 sec\n",
      "Epoch 196, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02247/0.02376. Took 0.27 sec\n",
      "Epoch 197, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02312/0.02377. Took 0.27 sec\n",
      "Epoch 198, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02356/0.02377. Took 0.27 sec\n",
      "Epoch 199, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02364/0.02376. Took 0.27 sec\n",
      "Epoch 200, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02328/0.02376. Took 0.27 sec\n",
      "Epoch 201, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02287/0.02378. Took 0.27 sec\n",
      "Epoch 202, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02326/0.02378. Took 0.27 sec\n",
      "Epoch 203, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02358/0.02377. Took 0.27 sec\n",
      "Epoch 204, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02302/0.02379. Took 0.27 sec\n",
      "Epoch 205, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02333/0.02379. Took 0.28 sec\n",
      "Epoch 206, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02266/0.02379. Took 0.28 sec\n",
      "Epoch 207, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02301/0.02379. Took 0.27 sec\n",
      "Epoch 208, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02319/0.02378. Took 0.27 sec\n",
      "Epoch 209, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02255/0.02378. Took 0.27 sec\n",
      "Epoch 210, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02282/0.02378. Took 0.27 sec\n",
      "Epoch 211, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02325/0.02378. Took 0.27 sec\n",
      "Epoch 212, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02314/0.02379. Took 0.27 sec\n",
      "Epoch 213, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02289/0.02380. Took 0.27 sec\n",
      "Epoch 214, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02304/0.02380. Took 0.27 sec\n",
      "Epoch 215, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02309/0.02381. Took 0.27 sec\n",
      "Epoch 216, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02259/0.02382. Took 0.27 sec\n",
      "Epoch 217, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02242/0.02382. Took 0.27 sec\n",
      "Epoch 218, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02245/0.02381. Took 0.27 sec\n",
      "Epoch 219, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02291/0.02380. Took 0.27 sec\n",
      "Epoch 220, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02348/0.02380. Took 0.27 sec\n",
      "Epoch 221, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02290/0.02380. Took 0.27 sec\n",
      "Epoch 222, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02247/0.02379. Took 0.27 sec\n",
      "Epoch 223, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02338/0.02379. Took 0.27 sec\n",
      "Epoch 224, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02325/0.02380. Took 0.27 sec\n",
      "Epoch 225, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02300/0.02378. Took 0.27 sec\n",
      "Epoch 226, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02285/0.02378. Took 0.28 sec\n",
      "Epoch 227, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02286/0.02376. Took 0.27 sec\n",
      "Epoch 228, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02330/0.02376. Took 0.27 sec\n",
      "Epoch 229, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02306/0.02376. Took 0.27 sec\n",
      "Epoch 230, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02325/0.02376. Took 0.27 sec\n",
      "Epoch 231, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02246/0.02376. Took 0.27 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02275/0.02376. Took 0.27 sec\n",
      "Epoch 233, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02274/0.02378. Took 0.27 sec\n",
      "Epoch 234, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02293/0.02380. Took 0.27 sec\n",
      "Epoch 235, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02279/0.02380. Took 0.27 sec\n",
      "Epoch 236, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02260/0.02381. Took 0.27 sec\n",
      "Epoch 237, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02369/0.02381. Took 0.27 sec\n",
      "Epoch 238, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02273/0.02383. Took 0.27 sec\n",
      "Epoch 239, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02313/0.02383. Took 0.27 sec\n",
      "Epoch 240, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02314/0.02384. Took 0.28 sec\n",
      "Epoch 241, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02273/0.02383. Took 0.28 sec\n",
      "Epoch 242, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02266/0.02384. Took 0.27 sec\n",
      "Epoch 243, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02266/0.02383. Took 0.27 sec\n",
      "Epoch 244, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02227/0.02382. Took 0.27 sec\n",
      "Epoch 245, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02283/0.02383. Took 0.27 sec\n",
      "Epoch 246, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02208/0.02383. Took 0.27 sec\n",
      "Epoch 247, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02269/0.02385. Took 0.27 sec\n",
      "Epoch 248, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02265/0.02384. Took 0.27 sec\n",
      "Epoch 249, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02271/0.02383. Took 0.28 sec\n",
      "Epoch 250, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02228/0.02384. Took 0.27 sec\n",
      "Epoch 251, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02256/0.02382. Took 0.27 sec\n",
      "Epoch 252, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02241/0.02382. Took 0.27 sec\n",
      "Epoch 253, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02249/0.02384. Took 0.27 sec\n",
      "Epoch 254, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02262/0.02382. Took 0.27 sec\n",
      "Epoch 255, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02277/0.02382. Took 0.27 sec\n",
      "Epoch 256, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02271/0.02381. Took 0.27 sec\n",
      "Epoch 257, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02270/0.02382. Took 0.28 sec\n",
      "Epoch 258, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02277/0.02381. Took 0.28 sec\n",
      "Epoch 259, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02261/0.02382. Took 0.27 sec\n",
      "Epoch 260, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02239/0.02384. Took 0.27 sec\n",
      "Epoch 261, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02226/0.02383. Took 0.27 sec\n",
      "Epoch 262, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02245/0.02382. Took 0.27 sec\n",
      "Epoch 263, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02238/0.02382. Took 0.27 sec\n",
      "Epoch 264, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02250/0.02382. Took 0.27 sec\n",
      "Epoch 265, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02229/0.02380. Took 0.27 sec\n",
      "Epoch 266, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02196/0.02378. Took 0.27 sec\n",
      "Epoch 267, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02242/0.02381. Took 0.27 sec\n",
      "Epoch 268, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02232/0.02381. Took 0.27 sec\n",
      "Epoch 269, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02221/0.02381. Took 0.27 sec\n",
      "Epoch 270, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02264/0.02381. Took 0.27 sec\n",
      "Epoch 271, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02235/0.02381. Took 0.27 sec\n",
      "Epoch 272, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02247/0.02380. Took 0.27 sec\n",
      "Epoch 273, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02249/0.02383. Took 0.27 sec\n",
      "Epoch 274, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02240/0.02384. Took 0.28 sec\n",
      "Epoch 275, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02191/0.02381. Took 0.27 sec\n",
      "Epoch 276, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02234/0.02382. Took 0.28 sec\n",
      "Epoch 277, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02258/0.02382. Took 0.27 sec\n",
      "Epoch 278, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02268/0.02382. Took 0.27 sec\n",
      "Epoch 279, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02262/0.02382. Took 0.27 sec\n",
      "Epoch 280, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02221/0.02383. Took 0.27 sec\n",
      "Epoch 281, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02246/0.02382. Took 0.27 sec\n",
      "Epoch 282, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02240/0.02382. Took 0.27 sec\n",
      "Epoch 283, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02243/0.02383. Took 0.27 sec\n",
      "Epoch 284, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02200/0.02382. Took 0.27 sec\n",
      "Epoch 285, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02225/0.02381. Took 0.27 sec\n",
      "Epoch 286, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02209/0.02380. Took 0.27 sec\n",
      "Epoch 287, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02207/0.02380. Took 0.27 sec\n",
      "Epoch 288, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02282/0.02380. Took 0.27 sec\n",
      "Epoch 289, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02251/0.02380. Took 0.27 sec\n",
      "Epoch 290, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02222/0.02381. Took 0.27 sec\n",
      "Epoch 291, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02264/0.02382. Took 0.27 sec\n",
      "Epoch 292, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02222/0.02382. Took 0.28 sec\n",
      "Epoch 293, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02202/0.02382. Took 0.27 sec\n",
      "Epoch 294, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02208/0.02384. Took 0.27 sec\n",
      "Epoch 295, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02198/0.02384. Took 0.27 sec\n",
      "Epoch 296, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02228/0.02382. Took 0.27 sec\n",
      "Epoch 297, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02289/0.02381. Took 0.27 sec\n",
      "Epoch 298, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02225/0.02380. Took 0.27 sec\n",
      "Epoch 299, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02280/0.02380. Took 0.27 sec\n",
      "Epoch 300, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02238/0.02381. Took 0.27 sec\n",
      "Epoch 301, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02211/0.02381. Took 0.27 sec\n",
      "Epoch 302, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02230/0.02382. Took 0.27 sec\n",
      "Epoch 303, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02267/0.02382. Took 0.27 sec\n",
      "Epoch 304, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02220/0.02382. Took 0.28 sec\n",
      "Epoch 305, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02199/0.02382. Took 0.27 sec\n",
      "Epoch 306, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02237/0.02383. Took 0.27 sec\n",
      "Epoch 307, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02200/0.02383. Took 0.27 sec\n",
      "Epoch 308, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02195/0.02382. Took 0.27 sec\n",
      "Epoch 309, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02194/0.02381. Took 0.27 sec\n",
      "Epoch 310, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02211/0.02382. Took 0.28 sec\n",
      "Epoch 311, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02177/0.02383. Took 0.28 sec\n",
      "Epoch 312, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02244/0.02383. Took 0.28 sec\n",
      "Epoch 313, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02170/0.02382. Took 0.27 sec\n",
      "Epoch 314, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02163/0.02383. Took 0.27 sec\n",
      "Epoch 315, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02166/0.02384. Took 0.27 sec\n",
      "Epoch 316, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02188/0.02385. Took 0.27 sec\n",
      "Epoch 317, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02179/0.02385. Took 0.27 sec\n",
      "Epoch 318, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02230/0.02385. Took 0.27 sec\n",
      "Epoch 319, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02230/0.02383. Took 0.27 sec\n",
      "Epoch 320, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02203/0.02383. Took 0.27 sec\n",
      "Epoch 321, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02384. Took 0.27 sec\n",
      "Epoch 322, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02213/0.02384. Took 0.27 sec\n",
      "Epoch 323, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02222/0.02385. Took 0.28 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02161/0.02385. Took 0.28 sec\n",
      "Epoch 325, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02228/0.02386. Took 0.27 sec\n",
      "Epoch 326, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02170/0.02384. Took 0.27 sec\n",
      "Epoch 327, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02178/0.02384. Took 0.27 sec\n",
      "Epoch 328, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02211/0.02383. Took 0.27 sec\n",
      "Epoch 329, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02169/0.02383. Took 0.27 sec\n",
      "Epoch 330, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02174/0.02382. Took 0.27 sec\n",
      "Epoch 331, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02156/0.02381. Took 0.27 sec\n",
      "Epoch 332, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02164/0.02382. Took 0.27 sec\n",
      "Epoch 333, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02182/0.02383. Took 0.28 sec\n",
      "Epoch 334, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02241/0.02382. Took 0.28 sec\n",
      "Epoch 335, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02197/0.02381. Took 0.27 sec\n",
      "Epoch 336, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02194/0.02382. Took 0.27 sec\n",
      "Epoch 337, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02184/0.02383. Took 0.28 sec\n",
      "Epoch 338, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02190/0.02383. Took 0.27 sec\n",
      "Epoch 339, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02209/0.02383. Took 0.27 sec\n",
      "Epoch 340, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02253/0.02383. Took 0.27 sec\n",
      "Epoch 341, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02212/0.02384. Took 0.27 sec\n",
      "Epoch 342, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02139/0.02384. Took 0.27 sec\n",
      "Epoch 343, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02170/0.02386. Took 0.27 sec\n",
      "Epoch 344, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02232/0.02383. Took 0.27 sec\n",
      "Epoch 345, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02232/0.02384. Took 0.27 sec\n",
      "Epoch 346, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02160/0.02386. Took 0.27 sec\n",
      "Epoch 347, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02175/0.02386. Took 0.27 sec\n",
      "Epoch 348, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02203/0.02382. Took 0.27 sec\n",
      "Epoch 349, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02175/0.02382. Took 0.27 sec\n",
      "Epoch 350, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02179/0.02385. Took 0.27 sec\n",
      "Epoch 351, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02168/0.02384. Took 0.28 sec\n",
      "Epoch 352, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02208/0.02383. Took 0.27 sec\n",
      "Epoch 353, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02384. Took 0.27 sec\n",
      "Epoch 354, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02082/0.02384. Took 0.27 sec\n",
      "Epoch 355, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02154/0.02385. Took 0.27 sec\n",
      "Epoch 356, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02150/0.02384. Took 0.27 sec\n",
      "Epoch 357, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02188/0.02383. Took 0.27 sec\n",
      "Epoch 358, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02173/0.02382. Took 0.27 sec\n",
      "Epoch 359, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02193/0.02382. Took 0.27 sec\n",
      "Epoch 360, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02212/0.02382. Took 0.28 sec\n",
      "Epoch 361, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02210/0.02380. Took 0.28 sec\n",
      "Epoch 362, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02161/0.02379. Took 0.27 sec\n",
      "Epoch 363, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02171/0.02379. Took 0.27 sec\n",
      "Epoch 364, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02171/0.02379. Took 0.27 sec\n",
      "Epoch 365, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02205/0.02381. Took 0.27 sec\n",
      "Epoch 366, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02172/0.02381. Took 0.27 sec\n",
      "Epoch 367, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02379. Took 0.27 sec\n",
      "Epoch 368, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02110/0.02379. Took 0.27 sec\n",
      "Epoch 369, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02173/0.02379. Took 0.27 sec\n",
      "Epoch 370, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02152/0.02379. Took 0.27 sec\n",
      "Epoch 371, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02379. Took 0.27 sec\n",
      "Epoch 372, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02172/0.02379. Took 0.27 sec\n",
      "Epoch 373, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02141/0.02381. Took 0.27 sec\n",
      "Epoch 374, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02097/0.02379. Took 0.28 sec\n",
      "Epoch 375, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02140/0.02379. Took 0.27 sec\n",
      "Epoch 376, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02203/0.02381. Took 0.27 sec\n",
      "Epoch 377, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02189/0.02380. Took 0.27 sec\n",
      "Epoch 378, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02167/0.02380. Took 0.27 sec\n",
      "Epoch 379, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02145/0.02380. Took 0.27 sec\n",
      "Epoch 380, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02134/0.02380. Took 0.27 sec\n",
      "Epoch 381, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02186/0.02379. Took 0.27 sec\n",
      "Epoch 382, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02148/0.02380. Took 0.28 sec\n",
      "Epoch 383, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02381. Took 0.27 sec\n",
      "Epoch 384, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02095/0.02381. Took 0.27 sec\n",
      "Epoch 385, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02195/0.02380. Took 0.27 sec\n",
      "Epoch 386, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02179/0.02380. Took 0.27 sec\n",
      "Epoch 387, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02145/0.02383. Took 0.27 sec\n",
      "Epoch 388, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02156/0.02386. Took 0.27 sec\n",
      "Epoch 389, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02144/0.02384. Took 0.27 sec\n",
      "Epoch 390, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02165/0.02384. Took 0.27 sec\n",
      "Epoch 391, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02152/0.02380. Took 0.27 sec\n",
      "Epoch 392, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02121/0.02381. Took 0.27 sec\n",
      "Epoch 393, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02185/0.02382. Took 0.27 sec\n",
      "Epoch 394, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02127/0.02382. Took 0.27 sec\n",
      "Epoch 395, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02091/0.02380. Took 0.27 sec\n",
      "Epoch 396, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02130/0.02380. Took 0.27 sec\n",
      "Epoch 397, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02156/0.02382. Took 0.27 sec\n",
      "Epoch 398, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02093/0.02383. Took 0.28 sec\n",
      "Epoch 399, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02162/0.02381. Took 0.28 sec\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 8 # 2~8\n",
    "args.UpData = UpData\n",
    "args.DownData = DownData\n",
    "args.x_frames = 2\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 32\n",
    "args.n_layers = 4\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001\n",
    "args.dropout = 0.1 \n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam'  #SGD, RMSprop, Adam...\n",
    "args.loss = 'MSELoss'#'MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss'\n",
    "args.lr = 0.0001\n",
    "args.epoch = 200\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'x_frames'\n",
    "name_var2 = 'batch_size'\n",
    "name_var3 = 'optim'\n",
    "name_var4 = 'lr'\n",
    "name_var5 = 'epoch'\n",
    "list_var1 = [2]\n",
    "list_var2 = [8]\n",
    "list_var3 = ['Adam']\n",
    "list_var4 = [0.0001]\n",
    "list_var5 = [200,300,400]\n",
    "\n",
    "\n",
    "num = 0 #초기화\n",
    "\n",
    "\n",
    "trainset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2018-06-30')\n",
    "valset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2018-07-01', '2019-12-31')\n",
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2018-07-01', '2019-12-31')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "\n",
    "print('size of trainset :{}'.format(len(trainset)),\n",
    "      'size of valset :{}'.format(len(valset)),\n",
    "      'size of testset :{}'.format(len(testset)))\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        for var3 in list_var3:\n",
    "            for var4 in list_var4:\n",
    "                for var5 in list_var5:\n",
    "                    num += 1\n",
    "                    setattr(args, name_var1, var1)\n",
    "                    setattr(args, name_var2, var2)\n",
    "                    setattr(args, name_var3, var3)\n",
    "                    setattr(args, name_var4, var4)\n",
    "                    setattr(args, name_var5, var5)\n",
    "                    print('\\n exp_{}'.format(num))\n",
    "                    print('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                          .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn))     \n",
    "\n",
    "                    result = experiment(partition, deepcopy(args))\n",
    "\n",
    "            #         print('train_acc_RMSE = {:2.2f}%, val_acc_RMSE = {:2.2f}%, test_RMSE = {:2.2f}%, test_R2 = {:2.2f}%'\n",
    "            #               .format(result['train_acc_RMSE'],result['val_acc_RMSE'],result['test_RMSE'],result['test_R2']))\n",
    "\n",
    "                    vis.text('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                             .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn),\n",
    "                             opts=dict(title='exp_{}_text'.format(num)))\n",
    "                    # 만든 모델의 test 데이터 예측 시각화\n",
    "\n",
    "                    predict = torch.Tensor(result['test_pred']).view(-1,1)\n",
    "                    truth = torch.Tensor(result['test_true']).view(-1,1)\n",
    "                    axis = torch.Tensor(range(len(result['test_pred']))).view(-1,1)\n",
    "\n",
    "                    Y_axis = torch.cat((predict, truth), -1)\n",
    "                    X_axis = torch.cat((axis, axis), -1)\n",
    "\n",
    "                    vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_RMSE[{:2.3f}]_R2[{:2.3f}]'.format(num,result['test_RMSE'],result['test_R2']),\n",
    "                                                               legend=['predict','true'],\n",
    "                                                               showlegend=True,\n",
    "                                                               layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current CPU random seed: 666\n"
     ]
    }
   ],
   "source": [
    "print('Current CPU random seed:',torch.initial_seed())\n",
    "# print('Current CUDA random seed:',torch.cuda.initial_seed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.text('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "#                              .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn),\n",
    "#                              opts=dict(title='exp_{}_text'.format(num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
