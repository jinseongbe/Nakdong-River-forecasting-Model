{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMKHTsedaum7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import argparse\n",
    "from copy import deepcopy #Add Deepcopy for args\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(torch.__version__)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1gRd7qVcEwI"
   },
   "source": [
    "# 1. Data loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KShxDU_Jcj3u"
   },
   "source": [
    "### 1.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jinsungpark/Desktop/Data_river/data04'\n",
      "/Users/jinsungpark/Desktop/jupyter\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynYVJoGrcnQS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jinsungpark/Desktop/jupyter'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd #현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTiRk82qctLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation_analysis.ipynb\r\n",
      "Data_Preprocessing.ipynb\r\n",
      "\u001b[34mData_river\u001b[m\u001b[m/\r\n",
      "Lab10_Stock_Price_Prediction_with_LSTM.ipynb\r\n",
      "Pandas_tutorial.ipynb\r\n",
      "\u001b[34mProject_Git\u001b[m\u001b[m/\r\n",
      "Project_River_Backup.ipynb\r\n",
      "Project_River_Backup_change_TN.ipynb\r\n",
      "Project_River_Backup_change_TP.ipynb\r\n",
      "Project_River_Backup_change_test.ver.ipynb\r\n",
      "Project_River_Find_BestModel.ipynb\r\n",
      "Project_River_Find_BestModel_01.ipynb\r\n",
      "Project_River_matplotlib.ipynb\r\n",
      "Project_River_matplotlib_01.ipynb\r\n",
      "Untitled.ipynb\r\n",
      "\u001b[34mresults\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls #현재경로에 있는 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f39evpcvc1KK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jinsungpark/Desktop/jupyter/Data_river/data04\n"
     ]
    }
   ],
   "source": [
    "cd /Users/jinsungpark/Desktop/jupyter/Data_river/data04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dln4LnKpc1CX"
   },
   "outputs": [],
   "source": [
    "UpStream_data = pd.read_excel('DS_Data_edit_log.xlsx')\n",
    "DownStream_data = pd.read_excel('NG_Data_edit_log.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GRAwoM_c068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'DS_DO', 'DS_BOD', 'DS_COD', 'DS_SS', 'DS_TN', 'DS_TP',\n",
      "       'DS_Chl_a', 'DS_Cells', 'GJ_Deep', 'GJ_Level', 'GJ_Outflow',\n",
      "       'DaeGu_Rain', 'DaeGu_Solar', 'SeoBu_COD', 'SeoBu_SS', 'SeoBu_TN',\n",
      "       'SeoBu_TP', 'SeoBu_Flow_mean', 'SeoBu_Flow_day', 'SeoBu_COD_load',\n",
      "       'SeoBu_SS_load', 'SeoBu_TN_load', 'SeoBu_TP_load', 'SungSeo_COD',\n",
      "       'SungSeo_SS', 'SungSeo_TN', 'SungSeo_TP', 'SungSeo_Flow_mean',\n",
      "       'SungSeo_Flow_day', 'SungSeo_COD_load', 'SungSeo_SS_load',\n",
      "       'SungSeo_TN_load', 'SungSeo_TP_load', 'GumHo_DO', 'GumHo_BOD',\n",
      "       'GumHo_COD', 'GumHo_SS', 'GumHo_TN', 'GumHo_TP', 'GumHo_Chl_a',\n",
      "       'GumHo_Flow', 'DS_Temp', 'GumHo_Temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(UpStream_data.columns)\n",
    "# print(DownStream_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [],
   "source": [
    "#날짜 인덱스화\n",
    "UpData = UpStream_data.set_index('Date')\n",
    "DownData = DownStream_data.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 43 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DS_DO              349 non-null    float64\n",
      " 1   DS_BOD             349 non-null    float64\n",
      " 2   DS_COD             349 non-null    float64\n",
      " 3   DS_SS              349 non-null    float64\n",
      " 4   DS_TN              349 non-null    float64\n",
      " 5   DS_TP              349 non-null    float64\n",
      " 6   DS_Chl_a           349 non-null    float64\n",
      " 7   DS_Cells           349 non-null    float64\n",
      " 8   GJ_Deep            349 non-null    float64\n",
      " 9   GJ_Level           349 non-null    float64\n",
      " 10  GJ_Outflow         349 non-null    float64\n",
      " 11  DaeGu_Rain         349 non-null    float64\n",
      " 12  DaeGu_Solar        349 non-null    float64\n",
      " 13  SeoBu_COD          349 non-null    float64\n",
      " 14  SeoBu_SS           349 non-null    float64\n",
      " 15  SeoBu_TN           349 non-null    float64\n",
      " 16  SeoBu_TP           349 non-null    float64\n",
      " 17  SeoBu_Flow_mean    349 non-null    float64\n",
      " 18  SeoBu_Flow_day     349 non-null    float64\n",
      " 19  SeoBu_COD_load     349 non-null    float64\n",
      " 20  SeoBu_SS_load      349 non-null    float64\n",
      " 21  SeoBu_TN_load      349 non-null    float64\n",
      " 22  SeoBu_TP_load      349 non-null    float64\n",
      " 23  SungSeo_COD        349 non-null    float64\n",
      " 24  SungSeo_SS         349 non-null    float64\n",
      " 25  SungSeo_TN         349 non-null    float64\n",
      " 26  SungSeo_TP         349 non-null    float64\n",
      " 27  SungSeo_Flow_mean  349 non-null    float64\n",
      " 28  SungSeo_Flow_day   349 non-null    float64\n",
      " 29  SungSeo_COD_load   349 non-null    float64\n",
      " 30  SungSeo_SS_load    349 non-null    float64\n",
      " 31  SungSeo_TN_load    349 non-null    float64\n",
      " 32  SungSeo_TP_load    349 non-null    float64\n",
      " 33  GumHo_DO           349 non-null    float64\n",
      " 34  GumHo_BOD          349 non-null    float64\n",
      " 35  GumHo_COD          349 non-null    float64\n",
      " 36  GumHo_SS           349 non-null    float64\n",
      " 37  GumHo_TN           349 non-null    float64\n",
      " 38  GumHo_TP           349 non-null    float64\n",
      " 39  GumHo_Chl_a        349 non-null    float64\n",
      " 40  GumHo_Flow         349 non-null    float64\n",
      " 41  DS_Temp            349 non-null    float64\n",
      " 42  GumHo_Temp         349 non-null    float64\n",
      "dtypes: float64(43)\n",
      "memory usage: 120.0+ KB\n"
     ]
    }
   ],
   "source": [
    "UpData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DS_DO', 'DS_COD', 'DS_SS', 'DS_TP', 'GJ_Outflow', 'DaeGu_Rain',\n",
      "       'SungSeo_Flow_day', 'SungSeo_COD_load', 'GumHo_DO', 'GumHo_SS',\n",
      "       'GumHo_TP', 'GumHo_Flow', 'DS_Temp', 'GumHo_Temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#넣고싶은 상류 항목 컬럼 선택\n",
    "UpData = UpData.iloc[:,[0,2,3,5,10,11,28,29,33,36,38,40,41,42]]\n",
    "print(UpData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UpData.to_csv('UpData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 349 entries, 2013-01-07 to 2019-09-30\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   NG_DO     349 non-null    float64\n",
      " 1   NG_BOD    349 non-null    float64\n",
      " 2   NG_COD    349 non-null    float64\n",
      " 3   NG_SS     349 non-null    float64\n",
      " 4   NG_TN     349 non-null    float64\n",
      " 5   NG_TP     349 non-null    float64\n",
      " 6   NG_Chl_a  349 non-null    float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 21.8+ KB\n"
     ]
    }
   ],
   "source": [
    "DownData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rSPrna-c0pt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG_TP\n"
     ]
    }
   ],
   "source": [
    "#알고싶은 하류 항목 컬럼 넘버 넣기('Date'항목이 인덱스화 돼서 컬럼 넘버가 -1씩 됨)\n",
    "Colum = 5\n",
    "print(DownData.columns[Colum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Tp4o0Hc0ZL"
   },
   "source": [
    "### 1.2 Data Preprocessing(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChKYCAtTdGpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "UpScaler = MinMaxScaler() #상류데이터용\n",
    "DownScaler = MinMaxScaler() #하류데이터용\n",
    "\n",
    "#나중에 결과를 DeNormalizing 하기 위해 나누어 사용 하였다.\n",
    "\n",
    "def DeNormalize(Y, Data_name, column_num, Scaler_Type):\n",
    "    \n",
    "    data = Data_name\n",
    "    Scaler = Scaler_Type\n",
    "    \n",
    "    _max = Scaler.data_max_[column_num] # 역정규화 하려는 데이터의 컬럼 번호\n",
    "    _min = Scaler.data_min_[column_num] \n",
    "    \n",
    "    X = Y*(_max-_min) + _min\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uneezLbLdGiC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_DO               0\n",
      "DS_COD              0\n",
      "DS_SS               0\n",
      "DS_TP               0\n",
      "GJ_Outflow          0\n",
      "DaeGu_Rain          0\n",
      "SungSeo_Flow_day    0\n",
      "SungSeo_COD_load    0\n",
      "GumHo_DO            0\n",
      "GumHo_SS            0\n",
      "GumHo_TP            0\n",
      "GumHo_Flow          0\n",
      "DS_Temp             0\n",
      "GumHo_Temp          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#데이터 정규화\n",
    "UpData = pd.DataFrame(UpScaler.fit_transform(UpData), columns=UpData.columns, index=UpData.index)\n",
    "DownData = pd.DataFrame(DownScaler.fit_transform(DownData), columns=DownData.columns, index=DownData.index)\n",
    "\n",
    "print(UpData.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDSVZGiUdGYz"
   },
   "source": [
    "#2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hxm2moSudGQD"
   },
   "outputs": [],
   "source": [
    "class RiverDataset(Dataset):\n",
    "    def __init__(self, UpData, DownData, x_frames, y_frames, start, end):\n",
    "        \n",
    "        self.x_frames = x_frames\n",
    "        self.y_frames = y_frames\n",
    "        \n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        self.UpData = UpData[start:end]\n",
    "        self.DownData = DownData[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.UpData) - (self.x_frames + self.y_frames) + 1\n",
    "    #데이터를 전처리 할때 UpData와 DownData의 길이가 동일해짐(날짜를 동일한것만 추출해야 하므로), 따라서 전체길이는 둘중 하나를 사용\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx += self.x_frames\n",
    "\n",
    "        X = self.UpData.iloc[idx-self.x_frames:idx].values\n",
    "        Y = self.DownData.iloc[idx:idx+self.y_frames].values\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj80CahhdGG9"
   },
   "source": [
    "# 3. Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuBGEc51dF-V"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, batch_size, dropout, use_bn):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers) #\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.regressor = self.make_regressor()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def make_regressor(self):\n",
    "        layers = []\n",
    "        if self.use_bn:\n",
    "            layers.append(nn.BatchNorm1d(self.hidden_dim))\n",
    "        layers.append(nn.Dropout(self.dropout))\n",
    "        \n",
    "        layers.append(nn.Linear(self.hidden_dim, self.hidden_dim // 2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(self.hidden_dim // 2, self.output_dim))\n",
    "        regressor = nn.Sequential(*layers)\n",
    "        return regressor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y_pred = self.regressor(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTpV_o6Wdglq"
   },
   "outputs": [],
   "source": [
    "# 정확도 : 예측확률을 100%로 봤을때 MAPE에 따른 오차비율을 빼줌 (100-MAPE) ##RMSE, MAPE 두개로 볼 수 있게\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2(y_true, y_pred):\n",
    "    R2_score = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    return R2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GPb8H8-djwB"
   },
   "source": [
    "# 4. Train, Validate, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrT2W-pqdjh6"
   },
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "\n",
    "        X = X.transpose(0, 1).float().to(args.device)#파이토치는 순서가 달라서 바꿔줌\n",
    "        y_true = y[:, :, Colum].float().to(args.device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred.append(y_pred)\n",
    "        true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 (배치사이즈의 텐서들을 한개씩 추가해주기) ==== #\n",
    "    for i in range(len(trainloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    train_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     train_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    return model, train_loss, train_acc1[0], train_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvAO-LVCdjgG"
   },
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred.view(-1), y_true.view(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 (배치사이즈의 텐서들을 한개씩 추가해주기) ==== #\n",
    "    for i in range(len(valloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================================================== #   \n",
    "    \n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc1 = RMSE(np.array(true_results), np.array(pred_results))\n",
    "    val_acc2 = R2(np.array(true_results), np.array(pred_results))\n",
    "#     val_acc3 = (100 - MAPE(np.array(true_results), np.array(pred_results)))\n",
    "\n",
    "    \n",
    "    return val_loss, val_acc1[0], val_acc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHWmu5EtdjXu"
   },
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    model.eval()\n",
    "\n",
    "    bat_siz = args.batch_size\n",
    "    pred = []\n",
    "    true = []\n",
    "    pred_results = []\n",
    "    true_results = []\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "            X = X.transpose(0, 1).float().to(args.device)\n",
    "            y_true = y[:, :, Colum].float().to(args.device)\n",
    "            model.hidden = [hidden.to(args.device) for hidden in model.init_hidden()]\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            pred.append(y_pred)\n",
    "            true.append(y_true)\n",
    "\n",
    "    # ==== test 데이터 시각화를 위해 x,y데이터 저장 ==== #\n",
    "    for i in range(len(testloader)):\n",
    "        tems1 = pred[i].view(bat_siz).cpu().detach().numpy()\n",
    "        tems2 = true[i].view(bat_siz).cpu().detach().numpy()\n",
    "        \n",
    "        for j in range(bat_siz):\n",
    "            value1 = np.exp(DeNormalize(tems1[j], DownData, Colum, DownScaler))\n",
    "            value2 = np.exp(DeNormalize(tems2[j], DownData, Colum, DownScaler))\n",
    "            \n",
    "            pred_results.append(value1)\n",
    "            true_results.append(value2)\n",
    "    # ========================================== #   \n",
    "\n",
    "    test_acc1 =  RMSE(np.array( true_results), np.array(pred_results))\n",
    "    test_acc2 =  R2(np.array( true_results), np.array(pred_results))\n",
    "#     test_acc3 =  (100 - MAPE(np.array( true_results), np.array(pred_results)))\n",
    "    \n",
    "    return test_acc1[0], test_acc2[0], pred_results, true_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkkF0-qmeOMq"
   },
   "source": [
    "# 5. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EOe2j_udjNv"
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "\n",
    "    model = LSTM(args.input_dim, args.hid_dim, args.y_frames, args.n_layers, args.batch_size, args.dropout, args.use_bn)\n",
    "    model.to(args.device)\n",
    "#     loss_fn = torch.nn.MSELoss() ##loss는 mse를 사용\n",
    "#     loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if args.loss == 'MSELoss':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss_fn = nn.MSELoss()\n",
    "    elif args.loss == 'L1Loss':\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "        loss_fn = nn.L1Loss()\n",
    "    elif args.loss == 'PoissonNLLLoss':\n",
    "        loss_fn = torch.nn.PoissonNLLLoss()\n",
    "        loss_fn = nn.PoissonNLLLoss()\n",
    "    elif args.loss == 'KLDivLoss':\n",
    "        loss_fn = torch.nn.KLDivLoss()\n",
    "        loss_fn = nn.KLDivLoss()\n",
    "    elif args.loss == 'BCELoss':\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "        loss_fn = nn.BCELoss()\n",
    "    elif args.loss == 'BCEWithLogitsLoss':\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise ValueError('In-valid LossFuction choice')\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs_RMSE = []\n",
    "    train_accs_R2 = []\n",
    "    val_accs_RMSE = []\n",
    "    val_accs_R2 = []\n",
    "    axis = []\n",
    "    # ===================================== #\n",
    "    \n",
    "    ## model starting point ##    \n",
    "    ts = time.time()\n",
    "    model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "    val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "    te = time.time()\n",
    "\n",
    "    # ====== Add Epoch Data ====== #\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs_RMSE.append(train_acc_RMSE)\n",
    "    val_accs_RMSE.append(val_acc_RMSE)\n",
    "    train_accs_R2.append(train_acc_R2)\n",
    "    val_accs_R2.append(val_acc_R2)\n",
    "    # ============================ #\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # # ===== Visdom visualizing ================================================================================== #\n",
    "    axis.append(0)\n",
    "    \n",
    "    plot1 = vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_loss'.format(num), legend=['train_loss','val_loss'], showlegend=True))\n",
    "    \n",
    "    plot2 = vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                     X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                     opts=dict(title='exp_{}_acc_RMSE'.format(num), legend=['train_acc','val_acc'], showlegend=True))\n",
    "    # # =========================================================================================================== #\n",
    "    \n",
    "    print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "          .format(0, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "    \n",
    "    for epoch in range(args.epoch-1):  # loop over the dataset multiple times\n",
    "        \n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc_RMSE, train_acc_R2 = train(model, partition, optimizer, loss_fn, args)\n",
    "        val_loss, val_acc_RMSE, val_acc_R2 = validate(model, partition, loss_fn, args)\n",
    "        te = time.time()\n",
    "\n",
    "        # ====== Add Epoch Data ====== #\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs_RMSE.append(train_acc_RMSE)\n",
    "        val_accs_RMSE.append(val_acc_RMSE)\n",
    "        train_accs_R2.append(train_acc_R2)\n",
    "        val_accs_R2.append(val_acc_R2)\n",
    "        # ============================ #\n",
    "\n",
    "        # # ===== Visdom visualizing ============================================================================== #\n",
    "        axis.append(epoch+1)\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_losses).view(-1,1), torch.Tensor(val_losses).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot1, update='replace')\n",
    "        \n",
    "        vis.line(Y=torch.cat((torch.Tensor(train_accs_RMSE).view(-1,1), torch.Tensor(val_accs_RMSE).view(-1,1)), -1),\n",
    "                 X=torch.cat((torch.Tensor(axis).view(-1,1), torch.Tensor(axis).view(-1,1)), -1),\n",
    "                 win=plot2, update='replace')\n",
    "#         # # ====================================================================================================== #\n",
    "        \n",
    "        print('Epoch {}, Acc_RMSE(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch+1, train_acc_RMSE, val_acc_RMSE, train_loss, val_loss, te-ts))\n",
    "        if epoch == 50:\n",
    "            test_RMSE_50, test_R2_50, Pred_data_50, True_data_50 = test(model, partition, args)\n",
    "            result['test_RMSE_50'] = test_RMSE_50\n",
    "            result['test_R2_50'] = test_R2_50\n",
    "            result['test_pred_50'] = Pred_data_50\n",
    "            result['test_true_50'] = True_data_50\n",
    "        if epoch == 100:\n",
    "            test_RMSE_100, test_R2_100, Pred_data_100, True_data_100 = test(model, partition, args)\n",
    "            result['test_RMSE_100'] = test_RMSE_100\n",
    "            result['test_R2_100'] = test_R2_100\n",
    "            result['test_pred_100'] = Pred_data_100\n",
    "            result['test_true_100'] = True_data_100\n",
    "        if epoch == 150:\n",
    "            test_RMSE_150, test_R2_150, Pred_data_150, True_data_150 = test(model, partition, args)\n",
    "            result['test_RMSE_150'] = test_RMSE_150\n",
    "            result['test_R2_150'] = test_R2_150\n",
    "            result['test_pred_150'] = Pred_data_150\n",
    "            result['test_true_150'] = True_data_150\n",
    "        if epoch == 200:\n",
    "            test_RMSE_200, test_R2_200, Pred_data_200, True_data_200 = test(model, partition, args)\n",
    "            result['test_RMSE_200'] = test_RMSE_200\n",
    "            result['test_R2_200'] = test_R2_200\n",
    "            result['test_pred_200'] = Pred_data_200\n",
    "            result['test_true_200'] = True_data_200\n",
    "        if epoch == 300:\n",
    "            test_RMSE_300, test_R2_300, Pred_data_300, True_data_300 = test(model, partition, args)\n",
    "            result['test_RMSE_300'] = test_RMSE_300\n",
    "            result['test_R2_300'] = test_R2_300\n",
    "            result['test_pred_300'] = Pred_data_300\n",
    "            result['test_true_300'] = True_data_300\n",
    "        if epoch == 500:\n",
    "            test_RMSE_500, test_R2_500, Pred_data_500, True_data_500 = test(model, partition, args)\n",
    "            result['test_RMSE_500'] = test_RMSE_500\n",
    "            result['test_R2_500'] = test_R2_500\n",
    "            result['test_pred_500'] = Pred_data_500\n",
    "            result['test_true_500'] = True_data_500\n",
    "        if epoch == 750:\n",
    "            test_RMSE_750, test_R2_750, Pred_data_750, True_data_750 = test(model, partition, args)\n",
    "            result['test_RMSE_750'] = test_RMSE_750\n",
    "            result['test_R2_750'] = test_R2_750\n",
    "            result['test_pred_750'] = Pred_data_750\n",
    "            result['test_true_750'] = True_data_750\n",
    "            \n",
    "    test_acc_RMSE, test_acc_R2, Pred_data, True_data = test(model, partition, args)\n",
    "    \n",
    "    # ======= Add Result to Dictionary ======= #\n",
    "#     result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    \n",
    "    result['train_accs_RMSE'] = train_accs_RMSE\n",
    "    result['train_accs_R2'] = train_accs_R2\n",
    "    result['val_accs_RMSE'] = val_accs_RMSE\n",
    "    result['val_accs_R2'] = val_accs_R2\n",
    "#     result['train_acc'] = train_acc\n",
    "#     result['val_acc'] = val_acc\n",
    "    result['test_RMSE'] = test_acc_RMSE\n",
    "    result['test_R2'] = test_acc_R2\n",
    "    result['test_pred'] = Pred_data\n",
    "    result['test_true'] = True_data\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNi-f5HyeJYi"
   },
   "source": [
    "# 6. LSTM Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4Bu6kFUdizj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of trainset :181 size of valset :38 size of testset :38\n",
      "\n",
      " exp_1\n",
      "loseFunc = MSELoss, optim=Adam, x_frames=4, n_layers=2, batch_size=8, hid_dim=32, epoch=1000, lr=0.01, l2=0.0001, dropout=0.1, use_bn=False\n",
      "Epoch 0, Acc_RMSE(train/val): 0.04/0.03, Loss(train/val) 0.06323/0.06664. Took 0.22 sec\n",
      "Epoch 1, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03704/0.05777. Took 0.23 sec\n",
      "Epoch 2, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.05026/0.07486. Took 0.23 sec\n",
      "Epoch 3, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03912/0.07363. Took 0.23 sec\n",
      "Epoch 4, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03723/0.06532. Took 0.23 sec\n",
      "Epoch 5, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03777/0.05850. Took 0.22 sec\n",
      "Epoch 6, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03918/0.06031. Took 0.22 sec\n",
      "Epoch 7, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03506/0.04929. Took 0.21 sec\n",
      "Epoch 8, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03475/0.05373. Took 0.24 sec\n",
      "Epoch 9, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03418/0.04393. Took 0.23 sec\n",
      "Epoch 10, Acc_RMSE(train/val): 0.03/0.03, Loss(train/val) 0.03313/0.04844. Took 0.23 sec\n",
      "Epoch 11, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.03218/0.04496. Took 0.23 sec\n",
      "Epoch 12, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02927/0.04538. Took 0.24 sec\n",
      "Epoch 13, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02902/0.04392. Took 0.22 sec\n",
      "Epoch 14, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02798/0.04423. Took 0.25 sec\n",
      "Epoch 15, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02704/0.04240. Took 0.22 sec\n",
      "Epoch 16, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02704/0.04334. Took 0.23 sec\n",
      "Epoch 17, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02572/0.04356. Took 0.24 sec\n",
      "Epoch 18, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02620/0.04075. Took 0.23 sec\n",
      "Epoch 19, Acc_RMSE(train/val): 0.03/0.02, Loss(train/val) 0.02552/0.04212. Took 0.22 sec\n",
      "Epoch 20, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02298/0.04091. Took 0.23 sec\n",
      "Epoch 21, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02266/0.04060. Took 0.21 sec\n",
      "Epoch 22, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02247/0.03896. Took 0.23 sec\n",
      "Epoch 23, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02173/0.03894. Took 0.23 sec\n",
      "Epoch 24, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02221/0.03678. Took 0.23 sec\n",
      "Epoch 25, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02181/0.03801. Took 0.22 sec\n",
      "Epoch 26, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02085/0.03336. Took 0.23 sec\n",
      "Epoch 27, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02107/0.03582. Took 0.22 sec\n",
      "Epoch 28, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02071/0.03249. Took 0.23 sec\n",
      "Epoch 29, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02208/0.03556. Took 0.23 sec\n",
      "Epoch 30, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01992/0.03333. Took 0.23 sec\n",
      "Epoch 31, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02033/0.03482. Took 0.24 sec\n",
      "Epoch 32, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01963/0.03201. Took 0.23 sec\n",
      "Epoch 33, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01950/0.03110. Took 0.23 sec\n",
      "Epoch 34, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01905/0.02913. Took 0.22 sec\n",
      "Epoch 35, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01875/0.03020. Took 0.23 sec\n",
      "Epoch 36, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01830/0.02936. Took 0.23 sec\n",
      "Epoch 37, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01849/0.02939. Took 0.24 sec\n",
      "Epoch 38, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01746/0.02687. Took 0.24 sec\n",
      "Epoch 39, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01794/0.02873. Took 0.25 sec\n",
      "Epoch 40, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01737/0.02703. Took 0.22 sec\n",
      "Epoch 41, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01719/0.02753. Took 0.23 sec\n",
      "Epoch 42, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01778/0.02695. Took 0.23 sec\n",
      "Epoch 43, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01607/0.02628. Took 0.23 sec\n",
      "Epoch 44, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01598/0.02553. Took 0.22 sec\n",
      "Epoch 45, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01551/0.02588. Took 0.23 sec\n",
      "Epoch 46, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01521/0.02622. Took 0.23 sec\n",
      "Epoch 47, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01559/0.02543. Took 0.23 sec\n",
      "Epoch 48, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01428/0.02457. Took 0.22 sec\n",
      "Epoch 49, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01516/0.02420. Took 0.22 sec\n",
      "Epoch 50, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01527/0.02862. Took 0.22 sec\n",
      "Epoch 51, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01679/0.02590. Took 0.22 sec\n",
      "Epoch 52, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01602/0.02971. Took 0.22 sec\n",
      "Epoch 53, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01762/0.02706. Took 0.22 sec\n",
      "Epoch 54, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01665/0.02680. Took 0.23 sec\n",
      "Epoch 55, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01541/0.02581. Took 0.24 sec\n",
      "Epoch 56, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01363/0.02403. Took 0.23 sec\n",
      "Epoch 57, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01407/0.02380. Took 0.22 sec\n",
      "Epoch 58, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01381/0.02340. Took 0.23 sec\n",
      "Epoch 59, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01397/0.02429. Took 0.22 sec\n",
      "Epoch 60, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01432/0.02320. Took 0.23 sec\n",
      "Epoch 61, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01475/0.02365. Took 0.22 sec\n",
      "Epoch 62, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01430/0.02314. Took 0.23 sec\n",
      "Epoch 63, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01391/0.02282. Took 0.23 sec\n",
      "Epoch 64, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01439/0.02389. Took 0.22 sec\n",
      "Epoch 65, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01531/0.03007. Took 0.21 sec\n",
      "Epoch 66, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01437/0.02243. Took 0.22 sec\n",
      "Epoch 67, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01458/0.02316. Took 0.21 sec\n",
      "Epoch 68, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01367/0.02202. Took 0.23 sec\n",
      "Epoch 69, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01408/0.02377. Took 0.22 sec\n",
      "Epoch 70, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01401/0.02205. Took 0.24 sec\n",
      "Epoch 71, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01345/0.02341. Took 0.22 sec\n",
      "Epoch 72, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01230/0.02199. Took 0.24 sec\n",
      "Epoch 73, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01296/0.02139. Took 0.22 sec\n",
      "Epoch 74, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01259/0.02122. Took 0.24 sec\n",
      "Epoch 75, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01294/0.02122. Took 0.23 sec\n",
      "Epoch 76, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01298/0.02221. Took 0.24 sec\n",
      "Epoch 77, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01414/0.02165. Took 0.22 sec\n",
      "Epoch 78, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01310/0.02438. Took 0.25 sec\n",
      "Epoch 79, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01374/0.01985. Took 0.23 sec\n",
      "Epoch 80, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01450/0.02591. Took 0.23 sec\n",
      "Epoch 81, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01499/0.02094. Took 0.22 sec\n",
      "Epoch 82, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01491/0.02497. Took 0.22 sec\n",
      "Epoch 83, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01495/0.02172. Took 0.21 sec\n",
      "Epoch 84, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01521/0.02427. Took 0.22 sec\n",
      "Epoch 85, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01539/0.02777. Took 0.22 sec\n",
      "Epoch 86, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01381/0.02176. Took 0.23 sec\n",
      "Epoch 87, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01415/0.02029. Took 0.21 sec\n",
      "Epoch 88, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01527/0.02768. Took 0.22 sec\n",
      "Epoch 89, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.02022/0.02300. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01350/0.02151. Took 0.24 sec\n",
      "Epoch 91, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01287/0.02073. Took 0.22 sec\n",
      "Epoch 92, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01315/0.02077. Took 0.24 sec\n",
      "Epoch 93, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01171/0.02061. Took 0.22 sec\n",
      "Epoch 94, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01258/0.01978. Took 0.23 sec\n",
      "Epoch 95, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01222/0.02006. Took 0.22 sec\n",
      "Epoch 96, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01228/0.01985. Took 0.24 sec\n",
      "Epoch 97, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01215/0.01909. Took 0.22 sec\n",
      "Epoch 98, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01252/0.01986. Took 0.23 sec\n",
      "Epoch 99, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01227/0.01897. Took 0.22 sec\n",
      "Epoch 100, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01271/0.01946. Took 0.24 sec\n",
      "Epoch 101, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01224/0.01982. Took 0.23 sec\n",
      "Epoch 102, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01266/0.02033. Took 0.23 sec\n",
      "Epoch 103, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01277/0.01970. Took 0.22 sec\n",
      "Epoch 104, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01301/0.02022. Took 0.23 sec\n",
      "Epoch 105, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01320/0.02447. Took 0.22 sec\n",
      "Epoch 106, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01260/0.01888. Took 0.23 sec\n",
      "Epoch 107, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01365/0.02155. Took 0.22 sec\n",
      "Epoch 108, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01344/0.01871. Took 0.22 sec\n",
      "Epoch 109, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01297/0.01872. Took 0.23 sec\n",
      "Epoch 110, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01139/0.01854. Took 0.24 sec\n",
      "Epoch 111, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01168/0.01820. Took 0.22 sec\n",
      "Epoch 112, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01206/0.01952. Took 0.24 sec\n",
      "Epoch 113, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01228/0.01949. Took 0.22 sec\n",
      "Epoch 114, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01387/0.02198. Took 0.23 sec\n",
      "Epoch 115, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01356/0.02191. Took 0.22 sec\n",
      "Epoch 116, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01399/0.02457. Took 0.24 sec\n",
      "Epoch 117, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01314/0.02111. Took 0.23 sec\n",
      "Epoch 118, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01196/0.02128. Took 0.25 sec\n",
      "Epoch 119, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01228/0.02086. Took 0.23 sec\n",
      "Epoch 120, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01230/0.01942. Took 0.22 sec\n",
      "Epoch 121, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01200/0.01969. Took 0.22 sec\n",
      "Epoch 122, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01242/0.01826. Took 0.25 sec\n",
      "Epoch 123, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01269/0.02245. Took 0.22 sec\n",
      "Epoch 124, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01387/0.01780. Took 0.24 sec\n",
      "Epoch 125, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01412/0.02175. Took 0.22 sec\n",
      "Epoch 126, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01349/0.01856. Took 0.23 sec\n",
      "Epoch 127, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01282/0.02241. Took 0.22 sec\n",
      "Epoch 128, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01356/0.01861. Took 0.23 sec\n",
      "Epoch 129, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.02005. Took 0.22 sec\n",
      "Epoch 130, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01198/0.02349. Took 0.23 sec\n",
      "Epoch 131, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01496/0.02455. Took 0.23 sec\n",
      "Epoch 132, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01455/0.02350. Took 0.24 sec\n",
      "Epoch 133, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01279/0.02198. Took 0.22 sec\n",
      "Epoch 134, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01337/0.01991. Took 0.23 sec\n",
      "Epoch 135, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01369/0.02391. Took 0.21 sec\n",
      "Epoch 136, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01532/0.02072. Took 0.22 sec\n",
      "Epoch 137, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01301/0.01750. Took 0.22 sec\n",
      "Epoch 138, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01226/0.02005. Took 0.23 sec\n",
      "Epoch 139, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01125/0.01884. Took 0.23 sec\n",
      "Epoch 140, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01218/0.02656. Took 0.22 sec\n",
      "Epoch 141, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01523/0.02674. Took 0.22 sec\n",
      "Epoch 142, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01617/0.02986. Took 0.23 sec\n",
      "Epoch 143, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01430/0.02241. Took 0.22 sec\n",
      "Epoch 144, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01218/0.01954. Took 0.24 sec\n",
      "Epoch 145, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01164/0.01963. Took 0.22 sec\n",
      "Epoch 146, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01163/0.01827. Took 0.22 sec\n",
      "Epoch 147, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01119/0.01822. Took 0.22 sec\n",
      "Epoch 148, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01111/0.01867. Took 0.23 sec\n",
      "Epoch 149, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01182/0.01725. Took 0.21 sec\n",
      "Epoch 150, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01121/0.01750. Took 0.23 sec\n",
      "Epoch 151, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01148/0.01789. Took 0.22 sec\n",
      "Epoch 152, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01090/0.01700. Took 0.22 sec\n",
      "Epoch 153, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01103/0.01830. Took 0.23 sec\n",
      "Epoch 154, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01100/0.02164. Took 0.24 sec\n",
      "Epoch 155, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01550/0.02824. Took 0.24 sec\n",
      "Epoch 156, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01841/0.02131. Took 0.24 sec\n",
      "Epoch 157, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01438/0.02548. Took 0.22 sec\n",
      "Epoch 158, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01377/0.02315. Took 0.23 sec\n",
      "Epoch 159, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01364/0.01931. Took 0.22 sec\n",
      "Epoch 160, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01264/0.02038. Took 0.23 sec\n",
      "Epoch 161, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01413/0.02160. Took 0.22 sec\n",
      "Epoch 162, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01300/0.02018. Took 0.23 sec\n",
      "Epoch 163, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01182/0.01859. Took 0.24 sec\n",
      "Epoch 164, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01177/0.01916. Took 0.24 sec\n",
      "Epoch 165, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01105/0.01822. Took 0.22 sec\n",
      "Epoch 166, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01201/0.01921. Took 0.23 sec\n",
      "Epoch 167, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01238/0.02116. Took 0.22 sec\n",
      "Epoch 168, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01249/0.01893. Took 0.23 sec\n",
      "Epoch 169, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01124/0.01737. Took 0.22 sec\n",
      "Epoch 170, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01187/0.02037. Took 0.22 sec\n",
      "Epoch 171, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01266/0.01770. Took 0.22 sec\n",
      "Epoch 172, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01107/0.01714. Took 0.23 sec\n",
      "Epoch 173, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01101/0.01689. Took 0.21 sec\n",
      "Epoch 174, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01140/0.01666. Took 0.23 sec\n",
      "Epoch 175, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01204/0.02011. Took 0.22 sec\n",
      "Epoch 176, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01091/0.01643. Took 0.22 sec\n",
      "Epoch 177, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01195/0.01871. Took 0.21 sec\n",
      "Epoch 178, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01346/0.01878. Took 0.23 sec\n",
      "Epoch 179, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01490/0.02023. Took 0.21 sec\n",
      "Epoch 180, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01261/0.01597. Took 0.23 sec\n",
      "Epoch 181, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01320/0.02256. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01385/0.02098. Took 0.23 sec\n",
      "Epoch 183, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01165/0.01891. Took 0.21 sec\n",
      "Epoch 184, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01135/0.01980. Took 0.22 sec\n",
      "Epoch 185, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01146/0.01948. Took 0.22 sec\n",
      "Epoch 186, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01108/0.01847. Took 0.22 sec\n",
      "Epoch 187, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01094/0.02091. Took 0.22 sec\n",
      "Epoch 188, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01235/0.02053. Took 0.24 sec\n",
      "Epoch 189, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01354/0.02126. Took 0.21 sec\n",
      "Epoch 190, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01289/0.02095. Took 0.22 sec\n",
      "Epoch 191, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01223/0.01931. Took 0.22 sec\n",
      "Epoch 192, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01227/0.02026. Took 0.23 sec\n",
      "Epoch 193, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01159/0.01841. Took 0.22 sec\n",
      "Epoch 194, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01180/0.02569. Took 0.23 sec\n",
      "Epoch 195, Acc_RMSE(train/val): 0.02/0.03, Loss(train/val) 0.01758/0.03166. Took 0.21 sec\n",
      "Epoch 196, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01637/0.02460. Took 0.22 sec\n",
      "Epoch 197, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01302/0.02575. Took 0.21 sec\n",
      "Epoch 198, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01278/0.02424. Took 0.22 sec\n",
      "Epoch 199, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01236/0.02213. Took 0.22 sec\n",
      "Epoch 200, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01186/0.02302. Took 0.23 sec\n",
      "Epoch 201, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01504/0.02638. Took 0.22 sec\n",
      "Epoch 202, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01471/0.02275. Took 0.23 sec\n",
      "Epoch 203, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01064/0.02211. Took 0.21 sec\n",
      "Epoch 204, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01175/0.02414. Took 0.23 sec\n",
      "Epoch 205, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01338/0.02173. Took 0.21 sec\n",
      "Epoch 206, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01158/0.02027. Took 0.23 sec\n",
      "Epoch 207, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01192/0.02189. Took 0.22 sec\n",
      "Epoch 208, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01218/0.02206. Took 0.22 sec\n",
      "Epoch 209, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01248/0.02189. Took 0.22 sec\n",
      "Epoch 210, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01170/0.02022. Took 0.22 sec\n",
      "Epoch 211, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01171/0.02119. Took 0.21 sec\n",
      "Epoch 212, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01122/0.02138. Took 0.23 sec\n",
      "Epoch 213, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01252/0.01948. Took 0.22 sec\n",
      "Epoch 214, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01132/0.01813. Took 0.23 sec\n",
      "Epoch 215, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01161/0.01977. Took 0.22 sec\n",
      "Epoch 216, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01112/0.01756. Took 0.23 sec\n",
      "Epoch 217, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01129/0.02235. Took 0.23 sec\n",
      "Epoch 218, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01383/0.02275. Took 0.22 sec\n",
      "Epoch 219, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01445/0.02354. Took 0.23 sec\n",
      "Epoch 220, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01264/0.01905. Took 0.23 sec\n",
      "Epoch 221, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01167/0.01928. Took 0.23 sec\n",
      "Epoch 222, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01257/0.02095. Took 0.22 sec\n",
      "Epoch 223, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01234/0.01956. Took 0.21 sec\n",
      "Epoch 224, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01239/0.01992. Took 0.23 sec\n",
      "Epoch 225, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01184/0.01925. Took 0.24 sec\n",
      "Epoch 226, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01213/0.01941. Took 0.24 sec\n",
      "Epoch 227, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01234/0.01853. Took 0.23 sec\n",
      "Epoch 228, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01243/0.02085. Took 0.24 sec\n",
      "Epoch 229, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01277/0.01857. Took 0.24 sec\n",
      "Epoch 230, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01375/0.02177. Took 0.24 sec\n",
      "Epoch 231, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01209/0.01986. Took 0.24 sec\n",
      "Epoch 232, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01139/0.02017. Took 0.23 sec\n",
      "Epoch 233, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01136/0.02026. Took 0.23 sec\n",
      "Epoch 234, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01173/0.02170. Took 0.23 sec\n",
      "Epoch 235, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01316/0.01779. Took 0.23 sec\n",
      "Epoch 236, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01190/0.01875. Took 0.23 sec\n",
      "Epoch 237, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01109/0.01879. Took 0.23 sec\n",
      "Epoch 238, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01204/0.02051. Took 0.22 sec\n",
      "Epoch 239, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01143/0.01887. Took 0.23 sec\n",
      "Epoch 240, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01111/0.01998. Took 0.23 sec\n",
      "Epoch 241, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01159/0.01896. Took 0.23 sec\n",
      "Epoch 242, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01213/0.02052. Took 0.22 sec\n",
      "Epoch 243, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01080/0.02252. Took 0.22 sec\n",
      "Epoch 244, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01105/0.01845. Took 0.23 sec\n",
      "Epoch 245, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01098/0.02021. Took 0.23 sec\n",
      "Epoch 246, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01201/0.01724. Took 0.22 sec\n",
      "Epoch 247, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01153/0.01721. Took 0.22 sec\n",
      "Epoch 248, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01071/0.01760. Took 0.23 sec\n",
      "Epoch 249, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01002/0.01728. Took 0.23 sec\n",
      "Epoch 250, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01125/0.01802. Took 0.23 sec\n",
      "Epoch 251, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01079/0.01921. Took 0.22 sec\n",
      "Epoch 252, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01131/0.01794. Took 0.22 sec\n",
      "Epoch 253, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01220/0.01842. Took 0.22 sec\n",
      "Epoch 254, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01194/0.01724. Took 0.23 sec\n",
      "Epoch 255, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01132/0.01677. Took 0.23 sec\n",
      "Epoch 256, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01118/0.01897. Took 0.24 sec\n",
      "Epoch 257, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.01964. Took 0.23 sec\n",
      "Epoch 258, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01065/0.02205. Took 0.23 sec\n",
      "Epoch 259, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01197/0.02151. Took 0.23 sec\n",
      "Epoch 260, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01389/0.02050. Took 0.22 sec\n",
      "Epoch 261, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01157/0.01973. Took 0.23 sec\n",
      "Epoch 262, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01239/0.01819. Took 0.23 sec\n",
      "Epoch 263, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01096/0.01789. Took 0.23 sec\n",
      "Epoch 264, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01056/0.01829. Took 0.22 sec\n",
      "Epoch 265, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01033/0.01942. Took 0.23 sec\n",
      "Epoch 266, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01080/0.01761. Took 0.22 sec\n",
      "Epoch 267, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01032/0.01674. Took 0.22 sec\n",
      "Epoch 268, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01058/0.01701. Took 0.23 sec\n",
      "Epoch 269, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00971/0.01986. Took 0.23 sec\n",
      "Epoch 270, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01122/0.01620. Took 0.22 sec\n",
      "Epoch 271, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01116/0.01764. Took 0.22 sec\n",
      "Epoch 272, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01067/0.01762. Took 0.22 sec\n",
      "Epoch 273, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.02215. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01671/0.02047. Took 0.22 sec\n",
      "Epoch 275, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01256/0.01939. Took 0.22 sec\n",
      "Epoch 276, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01139/0.01812. Took 0.23 sec\n",
      "Epoch 277, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01089/0.01873. Took 0.22 sec\n",
      "Epoch 278, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01144/0.01736. Took 0.22 sec\n",
      "Epoch 279, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01018/0.01741. Took 0.22 sec\n",
      "Epoch 280, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01038/0.02294. Took 0.23 sec\n",
      "Epoch 281, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01458/0.02044. Took 0.22 sec\n",
      "Epoch 282, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01070/0.01917. Took 0.23 sec\n",
      "Epoch 283, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01363/0.01929. Took 0.22 sec\n",
      "Epoch 284, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01129/0.02035. Took 0.22 sec\n",
      "Epoch 285, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01060/0.01571. Took 0.22 sec\n",
      "Epoch 286, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01121/0.02153. Took 0.22 sec\n",
      "Epoch 287, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01261/0.02078. Took 0.22 sec\n",
      "Epoch 288, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01142/0.01803. Took 0.22 sec\n",
      "Epoch 289, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00981/0.01635. Took 0.22 sec\n",
      "Epoch 290, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01189/0.02047. Took 0.22 sec\n",
      "Epoch 291, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01273/0.01993. Took 0.22 sec\n",
      "Epoch 292, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01152/0.01967. Took 0.22 sec\n",
      "Epoch 293, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01022/0.02010. Took 0.22 sec\n",
      "Epoch 294, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01097/0.01902. Took 0.22 sec\n",
      "Epoch 295, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01100/0.01827. Took 0.22 sec\n",
      "Epoch 296, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01124/0.01756. Took 0.22 sec\n",
      "Epoch 297, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01225/0.02197. Took 0.22 sec\n",
      "Epoch 298, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01206/0.02195. Took 0.23 sec\n",
      "Epoch 299, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01011/0.01778. Took 0.22 sec\n",
      "Epoch 300, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01180/0.01910. Took 0.22 sec\n",
      "Epoch 301, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01017/0.01693. Took 0.22 sec\n",
      "Epoch 302, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01026/0.01844. Took 0.22 sec\n",
      "Epoch 303, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01048/0.01732. Took 0.22 sec\n",
      "Epoch 304, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01148/0.02170. Took 0.22 sec\n",
      "Epoch 305, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01166/0.02201. Took 0.22 sec\n",
      "Epoch 306, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01113/0.01755. Took 0.22 sec\n",
      "Epoch 307, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01196/0.01947. Took 0.22 sec\n",
      "Epoch 308, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01136/0.02336. Took 0.22 sec\n",
      "Epoch 309, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01202/0.02245. Took 0.22 sec\n",
      "Epoch 310, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01090/0.01918. Took 0.22 sec\n",
      "Epoch 311, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01068/0.01921. Took 0.22 sec\n",
      "Epoch 312, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01195/0.01905. Took 0.22 sec\n",
      "Epoch 313, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01368/0.02583. Took 0.22 sec\n",
      "Epoch 314, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01382/0.01892. Took 0.22 sec\n",
      "Epoch 315, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00999/0.01767. Took 0.22 sec\n",
      "Epoch 316, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01127/0.01829. Took 0.22 sec\n",
      "Epoch 317, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00962/0.01907. Took 0.22 sec\n",
      "Epoch 318, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01334/0.02581. Took 0.23 sec\n",
      "Epoch 319, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01028/0.02173. Took 0.22 sec\n",
      "Epoch 320, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00917/0.01877. Took 0.22 sec\n",
      "Epoch 321, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00895/0.01766. Took 0.22 sec\n",
      "Epoch 322, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00888/0.01910. Took 0.22 sec\n",
      "Epoch 323, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01071/0.02348. Took 0.22 sec\n",
      "Epoch 324, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01448/0.02316. Took 0.23 sec\n",
      "Epoch 325, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01011/0.01792. Took 0.22 sec\n",
      "Epoch 326, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01169/0.02082. Took 0.22 sec\n",
      "Epoch 327, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01078/0.02050. Took 0.22 sec\n",
      "Epoch 328, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01203/0.02492. Took 0.22 sec\n",
      "Epoch 329, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01221/0.02370. Took 0.22 sec\n",
      "Epoch 330, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01152/0.01978. Took 0.22 sec\n",
      "Epoch 331, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01127/0.02207. Took 0.22 sec\n",
      "Epoch 332, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00963/0.02025. Took 0.22 sec\n",
      "Epoch 333, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01072/0.02312. Took 0.22 sec\n",
      "Epoch 334, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01136/0.01895. Took 0.22 sec\n",
      "Epoch 335, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01014/0.01649. Took 0.22 sec\n",
      "Epoch 336, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01270/0.01957. Took 0.22 sec\n",
      "Epoch 337, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01114/0.01970. Took 0.22 sec\n",
      "Epoch 338, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01007/0.01929. Took 0.22 sec\n",
      "Epoch 339, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00863/0.01706. Took 0.22 sec\n",
      "Epoch 340, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00877/0.01703. Took 0.22 sec\n",
      "Epoch 341, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00918/0.01703. Took 0.22 sec\n",
      "Epoch 342, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00978/0.01803. Took 0.23 sec\n",
      "Epoch 343, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01109/0.02161. Took 0.22 sec\n",
      "Epoch 344, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01196/0.01968. Took 0.22 sec\n",
      "Epoch 345, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01085/0.02290. Took 0.22 sec\n",
      "Epoch 346, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.02005. Took 0.22 sec\n",
      "Epoch 347, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01038/0.01764. Took 0.22 sec\n",
      "Epoch 348, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00945/0.01823. Took 0.22 sec\n",
      "Epoch 349, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00961/0.01934. Took 0.22 sec\n",
      "Epoch 350, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00881/0.01933. Took 0.22 sec\n",
      "Epoch 351, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00856/0.01892. Took 0.22 sec\n",
      "Epoch 352, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01185/0.02169. Took 0.22 sec\n",
      "Epoch 353, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01649/0.02219. Took 0.22 sec\n",
      "Epoch 354, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01199/0.02062. Took 0.22 sec\n",
      "Epoch 355, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01068/0.02302. Took 0.22 sec\n",
      "Epoch 356, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01199/0.02452. Took 0.22 sec\n",
      "Epoch 357, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01137/0.02018. Took 0.22 sec\n",
      "Epoch 358, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00984/0.01791. Took 0.22 sec\n",
      "Epoch 359, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00959/0.01804. Took 0.23 sec\n",
      "Epoch 360, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01195/0.02007. Took 0.22 sec\n",
      "Epoch 361, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01027/0.02142. Took 0.22 sec\n",
      "Epoch 362, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00927/0.02038. Took 0.22 sec\n",
      "Epoch 363, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00941/0.02026. Took 0.22 sec\n",
      "Epoch 364, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00886/0.01922. Took 0.22 sec\n",
      "Epoch 365, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00931/0.02482. Took 0.24 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01173/0.02179. Took 0.23 sec\n",
      "Epoch 367, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00956/0.02075. Took 0.22 sec\n",
      "Epoch 368, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00967/0.01857. Took 0.22 sec\n",
      "Epoch 369, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.01018/0.02066. Took 0.22 sec\n",
      "Epoch 370, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00983/0.02264. Took 0.22 sec\n",
      "Epoch 371, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01040/0.02270. Took 0.23 sec\n",
      "Epoch 372, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00881/0.02290. Took 0.22 sec\n",
      "Epoch 373, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00892/0.02213. Took 0.22 sec\n",
      "Epoch 374, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01194/0.02630. Took 0.22 sec\n",
      "Epoch 375, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01220/0.02424. Took 0.22 sec\n",
      "Epoch 376, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01332/0.02010. Took 0.22 sec\n",
      "Epoch 377, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.02232. Took 0.22 sec\n",
      "Epoch 378, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01156/0.02331. Took 0.22 sec\n",
      "Epoch 379, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00977/0.02167. Took 0.22 sec\n",
      "Epoch 380, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00871/0.01812. Took 0.22 sec\n",
      "Epoch 381, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00945/0.02124. Took 0.22 sec\n",
      "Epoch 382, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01075/0.02332. Took 0.22 sec\n",
      "Epoch 383, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00944/0.02225. Took 0.22 sec\n",
      "Epoch 384, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00853/0.02185. Took 0.22 sec\n",
      "Epoch 385, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01098/0.02557. Took 0.22 sec\n",
      "Epoch 386, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01070/0.02211. Took 0.22 sec\n",
      "Epoch 387, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00933/0.02480. Took 0.22 sec\n",
      "Epoch 388, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00872/0.02230. Took 0.22 sec\n",
      "Epoch 389, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00879/0.02732. Took 0.22 sec\n",
      "Epoch 390, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01273/0.02417. Took 0.22 sec\n",
      "Epoch 391, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01379/0.02451. Took 0.22 sec\n",
      "Epoch 392, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01037/0.02030. Took 0.22 sec\n",
      "Epoch 393, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00782/0.01897. Took 0.23 sec\n",
      "Epoch 394, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00848/0.01982. Took 0.22 sec\n",
      "Epoch 395, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01063/0.02289. Took 0.23 sec\n",
      "Epoch 396, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01131/0.02131. Took 0.22 sec\n",
      "Epoch 397, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00994/0.02152. Took 0.22 sec\n",
      "Epoch 398, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01034/0.01893. Took 0.22 sec\n",
      "Epoch 399, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01300/0.02458. Took 0.22 sec\n",
      "Epoch 400, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01207/0.02457. Took 0.22 sec\n",
      "Epoch 401, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00850/0.02071. Took 0.23 sec\n",
      "Epoch 402, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00810/0.02082. Took 0.22 sec\n",
      "Epoch 403, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00929/0.02216. Took 0.22 sec\n",
      "Epoch 404, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01194/0.02489. Took 0.22 sec\n",
      "Epoch 405, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01088/0.02364. Took 0.22 sec\n",
      "Epoch 406, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00861/0.02246. Took 0.22 sec\n",
      "Epoch 407, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00804/0.02196. Took 0.23 sec\n",
      "Epoch 408, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00736/0.02211. Took 0.22 sec\n",
      "Epoch 409, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00765/0.02111. Took 0.22 sec\n",
      "Epoch 410, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00791/0.02351. Took 0.22 sec\n",
      "Epoch 411, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00867/0.02300. Took 0.22 sec\n",
      "Epoch 412, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00845/0.02461. Took 0.22 sec\n",
      "Epoch 413, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01141/0.02175. Took 0.22 sec\n",
      "Epoch 414, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01181/0.02143. Took 0.22 sec\n",
      "Epoch 415, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00975/0.02448. Took 0.22 sec\n",
      "Epoch 416, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00828/0.02459. Took 0.22 sec\n",
      "Epoch 417, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00866/0.02297. Took 0.22 sec\n",
      "Epoch 418, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01110/0.02388. Took 0.22 sec\n",
      "Epoch 419, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01352/0.02046. Took 0.23 sec\n",
      "Epoch 420, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00911/0.02222. Took 0.22 sec\n",
      "Epoch 421, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00992/0.02320. Took 0.22 sec\n",
      "Epoch 422, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00966/0.02195. Took 0.22 sec\n",
      "Epoch 423, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00891/0.02244. Took 0.22 sec\n",
      "Epoch 424, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00932/0.02073. Took 0.22 sec\n",
      "Epoch 425, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00843/0.02111. Took 0.22 sec\n",
      "Epoch 426, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00973/0.02301. Took 0.22 sec\n",
      "Epoch 427, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00903/0.02347. Took 0.22 sec\n",
      "Epoch 428, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00751/0.02325. Took 0.22 sec\n",
      "Epoch 429, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00770/0.02027. Took 0.22 sec\n",
      "Epoch 430, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01037/0.02138. Took 0.22 sec\n",
      "Epoch 431, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01079/0.02618. Took 0.23 sec\n",
      "Epoch 432, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00966/0.02502. Took 0.22 sec\n",
      "Epoch 433, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00892/0.02294. Took 0.22 sec\n",
      "Epoch 434, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00845/0.02676. Took 0.22 sec\n",
      "Epoch 435, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00822/0.02764. Took 0.22 sec\n",
      "Epoch 436, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00800/0.02234. Took 0.22 sec\n",
      "Epoch 437, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00914/0.02646. Took 0.22 sec\n",
      "Epoch 438, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00935/0.02267. Took 0.22 sec\n",
      "Epoch 439, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00926/0.02108. Took 0.22 sec\n",
      "Epoch 440, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00858/0.02076. Took 0.22 sec\n",
      "Epoch 441, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00766/0.02180. Took 0.22 sec\n",
      "Epoch 442, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00719/0.02247. Took 0.22 sec\n",
      "Epoch 443, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00780/0.02394. Took 0.22 sec\n",
      "Epoch 444, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00703/0.02391. Took 0.23 sec\n",
      "Epoch 445, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00776/0.02172. Took 0.22 sec\n",
      "Epoch 446, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01024/0.02050. Took 0.22 sec\n",
      "Epoch 447, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01157/0.02162. Took 0.22 sec\n",
      "Epoch 448, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00818/0.02117. Took 0.23 sec\n",
      "Epoch 449, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00895/0.02344. Took 0.22 sec\n",
      "Epoch 450, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00870/0.02349. Took 0.22 sec\n",
      "Epoch 451, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01720/0.02538. Took 0.22 sec\n",
      "Epoch 452, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01585/0.03016. Took 0.23 sec\n",
      "Epoch 453, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01424/0.02300. Took 0.22 sec\n",
      "Epoch 454, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00998/0.02096. Took 0.22 sec\n",
      "Epoch 455, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00831/0.02050. Took 0.22 sec\n",
      "Epoch 456, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00822/0.02262. Took 0.23 sec\n",
      "Epoch 457, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01019/0.02162. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00973/0.02236. Took 0.22 sec\n",
      "Epoch 459, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00827/0.02183. Took 0.22 sec\n",
      "Epoch 460, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00798/0.02218. Took 0.22 sec\n",
      "Epoch 461, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00911/0.02889. Took 0.22 sec\n",
      "Epoch 462, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00852/0.02746. Took 0.22 sec\n",
      "Epoch 463, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00807/0.02313. Took 0.22 sec\n",
      "Epoch 464, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00722/0.02255. Took 0.22 sec\n",
      "Epoch 465, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00772/0.02351. Took 0.22 sec\n",
      "Epoch 466, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00801/0.02112. Took 0.22 sec\n",
      "Epoch 467, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00888/0.02436. Took 0.22 sec\n",
      "Epoch 468, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00763/0.02325. Took 0.22 sec\n",
      "Epoch 469, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00956/0.02372. Took 0.22 sec\n",
      "Epoch 470, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00852/0.02219. Took 0.22 sec\n",
      "Epoch 471, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00802/0.02577. Took 0.22 sec\n",
      "Epoch 472, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00923/0.02433. Took 0.22 sec\n",
      "Epoch 473, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01017/0.02616. Took 0.22 sec\n",
      "Epoch 474, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01025/0.02715. Took 0.22 sec\n",
      "Epoch 475, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00855/0.02518. Took 0.22 sec\n",
      "Epoch 476, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00836/0.02576. Took 0.22 sec\n",
      "Epoch 477, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00892/0.02179. Took 0.22 sec\n",
      "Epoch 478, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00702/0.02250. Took 0.23 sec\n",
      "Epoch 479, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00758/0.02724. Took 0.22 sec\n",
      "Epoch 480, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00840/0.02130. Took 0.22 sec\n",
      "Epoch 481, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00996/0.02462. Took 0.22 sec\n",
      "Epoch 482, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00819/0.02371. Took 0.23 sec\n",
      "Epoch 483, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00866/0.02831. Took 0.22 sec\n",
      "Epoch 484, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01000/0.02697. Took 0.22 sec\n",
      "Epoch 485, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00850/0.02323. Took 0.22 sec\n",
      "Epoch 486, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00782/0.02442. Took 0.22 sec\n",
      "Epoch 487, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00739/0.02378. Took 0.22 sec\n",
      "Epoch 488, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00819/0.02388. Took 0.22 sec\n",
      "Epoch 489, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00874/0.02427. Took 0.23 sec\n",
      "Epoch 490, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01044/0.02528. Took 0.22 sec\n",
      "Epoch 491, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00846/0.02383. Took 0.22 sec\n",
      "Epoch 492, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00848/0.02245. Took 0.22 sec\n",
      "Epoch 493, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00982/0.02439. Took 0.22 sec\n",
      "Epoch 494, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01090/0.02650. Took 0.22 sec\n",
      "Epoch 495, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01450/0.03233. Took 0.22 sec\n",
      "Epoch 496, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00996/0.02650. Took 0.22 sec\n",
      "Epoch 497, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00866/0.02515. Took 0.22 sec\n",
      "Epoch 498, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00810/0.02419. Took 0.22 sec\n",
      "Epoch 499, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00754/0.02657. Took 0.22 sec\n",
      "Epoch 500, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00727/0.02707. Took 0.23 sec\n",
      "Epoch 501, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00707/0.02352. Took 0.22 sec\n",
      "Epoch 502, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00802/0.02599. Took 0.21 sec\n",
      "Epoch 503, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00831/0.02211. Took 0.22 sec\n",
      "Epoch 504, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00682/0.02428. Took 0.22 sec\n",
      "Epoch 505, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00763/0.02683. Took 0.22 sec\n",
      "Epoch 506, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00717/0.02535. Took 0.22 sec\n",
      "Epoch 507, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00743/0.02800. Took 0.22 sec\n",
      "Epoch 508, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00739/0.02331. Took 0.22 sec\n",
      "Epoch 509, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00710/0.02490. Took 0.22 sec\n",
      "Epoch 510, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00724/0.02451. Took 0.23 sec\n",
      "Epoch 511, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00890/0.02887. Took 0.22 sec\n",
      "Epoch 512, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01138/0.02320. Took 0.22 sec\n",
      "Epoch 513, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01086/0.02541. Took 0.22 sec\n",
      "Epoch 514, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01040/0.02180. Took 0.22 sec\n",
      "Epoch 515, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00919/0.02228. Took 0.22 sec\n",
      "Epoch 516, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00813/0.02765. Took 0.22 sec\n",
      "Epoch 517, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01045/0.02823. Took 0.22 sec\n",
      "Epoch 518, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00914/0.02472. Took 0.22 sec\n",
      "Epoch 519, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01162/0.02476. Took 0.22 sec\n",
      "Epoch 520, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00904/0.02345. Took 0.22 sec\n",
      "Epoch 521, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00904/0.02178. Took 0.22 sec\n",
      "Epoch 522, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00819/0.02698. Took 0.22 sec\n",
      "Epoch 523, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00825/0.02383. Took 0.22 sec\n",
      "Epoch 524, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00750/0.02480. Took 0.22 sec\n",
      "Epoch 525, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00922/0.02766. Took 0.22 sec\n",
      "Epoch 526, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00969/0.02612. Took 0.22 sec\n",
      "Epoch 527, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00842/0.02500. Took 0.22 sec\n",
      "Epoch 528, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00780/0.02529. Took 0.22 sec\n",
      "Epoch 529, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.02643. Took 0.22 sec\n",
      "Epoch 530, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00927/0.02964. Took 0.22 sec\n",
      "Epoch 531, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01208/0.02432. Took 0.22 sec\n",
      "Epoch 532, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01066/0.02593. Took 0.22 sec\n",
      "Epoch 533, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00816/0.02330. Took 0.22 sec\n",
      "Epoch 534, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00778/0.02506. Took 0.22 sec\n",
      "Epoch 535, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01070/0.02870. Took 0.23 sec\n",
      "Epoch 536, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01040/0.02481. Took 0.22 sec\n",
      "Epoch 537, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00977/0.02643. Took 0.22 sec\n",
      "Epoch 538, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00861/0.02420. Took 0.22 sec\n",
      "Epoch 539, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00955/0.02238. Took 0.22 sec\n",
      "Epoch 540, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00835/0.02645. Took 0.22 sec\n",
      "Epoch 541, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00816/0.02484. Took 0.22 sec\n",
      "Epoch 542, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00634/0.02421. Took 0.22 sec\n",
      "Epoch 543, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00708/0.02253. Took 0.22 sec\n",
      "Epoch 544, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00829/0.02517. Took 0.22 sec\n",
      "Epoch 545, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00902/0.02382. Took 0.22 sec\n",
      "Epoch 546, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00832/0.02458. Took 0.22 sec\n",
      "Epoch 547, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00857/0.02377. Took 0.22 sec\n",
      "Epoch 548, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00766/0.02355. Took 0.22 sec\n",
      "Epoch 549, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00810/0.02625. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 550, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00732/0.02330. Took 0.22 sec\n",
      "Epoch 551, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00876/0.02583. Took 0.22 sec\n",
      "Epoch 552, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00892/0.03105. Took 0.22 sec\n",
      "Epoch 553, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00770/0.02422. Took 0.22 sec\n",
      "Epoch 554, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00717/0.02770. Took 0.22 sec\n",
      "Epoch 555, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00734/0.02431. Took 0.23 sec\n",
      "Epoch 556, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00730/0.02760. Took 0.22 sec\n",
      "Epoch 557, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00759/0.02847. Took 0.21 sec\n",
      "Epoch 558, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00842/0.02197. Took 0.22 sec\n",
      "Epoch 559, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.02647. Took 0.22 sec\n",
      "Epoch 560, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00703/0.02214. Took 0.22 sec\n",
      "Epoch 561, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00674/0.02396. Took 0.22 sec\n",
      "Epoch 562, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00839/0.02347. Took 0.22 sec\n",
      "Epoch 563, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00782/0.02308. Took 0.22 sec\n",
      "Epoch 564, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00765/0.02764. Took 0.22 sec\n",
      "Epoch 565, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00719/0.02628. Took 0.22 sec\n",
      "Epoch 566, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00678/0.02387. Took 0.23 sec\n",
      "Epoch 567, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00871/0.02859. Took 0.22 sec\n",
      "Epoch 568, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00863/0.02604. Took 0.22 sec\n",
      "Epoch 569, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00902/0.02563. Took 0.22 sec\n",
      "Epoch 570, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01026/0.02698. Took 0.22 sec\n",
      "Epoch 571, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01136/0.02606. Took 0.22 sec\n",
      "Epoch 572, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01036/0.02459. Took 0.22 sec\n",
      "Epoch 573, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00835/0.02269. Took 0.22 sec\n",
      "Epoch 574, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00768/0.02529. Took 0.22 sec\n",
      "Epoch 575, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00921/0.02639. Took 0.22 sec\n",
      "Epoch 576, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01004/0.03259. Took 0.22 sec\n",
      "Epoch 577, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00894/0.02873. Took 0.22 sec\n",
      "Epoch 578, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00735/0.02694. Took 0.22 sec\n",
      "Epoch 579, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00871/0.02902. Took 0.22 sec\n",
      "Epoch 580, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01167/0.02576. Took 0.22 sec\n",
      "Epoch 581, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01070/0.02724. Took 0.22 sec\n",
      "Epoch 582, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01313/0.02683. Took 0.22 sec\n",
      "Epoch 583, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01118/0.02783. Took 0.22 sec\n",
      "Epoch 584, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01009/0.02768. Took 0.22 sec\n",
      "Epoch 585, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00786/0.02361. Took 0.22 sec\n",
      "Epoch 586, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00717/0.02504. Took 0.22 sec\n",
      "Epoch 587, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00873/0.02290. Took 0.22 sec\n",
      "Epoch 588, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00977/0.02908. Took 0.22 sec\n",
      "Epoch 589, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00784/0.02556. Took 0.22 sec\n",
      "Epoch 590, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00798/0.02655. Took 0.22 sec\n",
      "Epoch 591, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00889/0.02464. Took 0.23 sec\n",
      "Epoch 592, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00802/0.02350. Took 0.22 sec\n",
      "Epoch 593, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00801/0.03029. Took 0.22 sec\n",
      "Epoch 594, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00742/0.02639. Took 0.22 sec\n",
      "Epoch 595, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00643/0.02712. Took 0.22 sec\n",
      "Epoch 596, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00710/0.02529. Took 0.22 sec\n",
      "Epoch 597, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00713/0.02434. Took 0.22 sec\n",
      "Epoch 598, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00784/0.02399. Took 0.23 sec\n",
      "Epoch 599, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00623/0.02525. Took 0.22 sec\n",
      "Epoch 600, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00846/0.02830. Took 0.22 sec\n",
      "Epoch 601, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00852/0.02766. Took 0.22 sec\n",
      "Epoch 602, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00847/0.02536. Took 0.22 sec\n",
      "Epoch 603, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00708/0.02659. Took 0.22 sec\n",
      "Epoch 604, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00731/0.02821. Took 0.22 sec\n",
      "Epoch 605, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00711/0.02883. Took 0.22 sec\n",
      "Epoch 606, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00798/0.03023. Took 0.22 sec\n",
      "Epoch 607, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01083/0.02540. Took 0.22 sec\n",
      "Epoch 608, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01022/0.02355. Took 0.23 sec\n",
      "Epoch 609, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00790/0.02404. Took 0.22 sec\n",
      "Epoch 610, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00740/0.02588. Took 0.22 sec\n",
      "Epoch 611, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00725/0.02314. Took 0.22 sec\n",
      "Epoch 612, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00747/0.02586. Took 0.22 sec\n",
      "Epoch 613, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00662/0.02303. Took 0.22 sec\n",
      "Epoch 614, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00638/0.02542. Took 0.22 sec\n",
      "Epoch 615, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01011/0.02486. Took 0.22 sec\n",
      "Epoch 616, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01328/0.02624. Took 0.22 sec\n",
      "Epoch 617, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01109/0.02698. Took 0.22 sec\n",
      "Epoch 618, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00853/0.03110. Took 0.22 sec\n",
      "Epoch 619, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00706/0.02792. Took 0.22 sec\n",
      "Epoch 620, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00672/0.02577. Took 0.22 sec\n",
      "Epoch 621, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00772/0.02821. Took 0.22 sec\n",
      "Epoch 622, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01011/0.02488. Took 0.22 sec\n",
      "Epoch 623, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00707/0.02478. Took 0.22 sec\n",
      "Epoch 624, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00692/0.02457. Took 0.22 sec\n",
      "Epoch 625, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00655/0.02498. Took 0.22 sec\n",
      "Epoch 626, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00761/0.02725. Took 0.22 sec\n",
      "Epoch 627, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00810/0.02514. Took 0.22 sec\n",
      "Epoch 628, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00637/0.02587. Took 0.22 sec\n",
      "Epoch 629, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00741/0.02643. Took 0.22 sec\n",
      "Epoch 630, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00648/0.02736. Took 0.22 sec\n",
      "Epoch 631, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00585/0.02609. Took 0.22 sec\n",
      "Epoch 632, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00849/0.02971. Took 0.22 sec\n",
      "Epoch 633, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00770/0.02711. Took 0.22 sec\n",
      "Epoch 634, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00900/0.02558. Took 0.22 sec\n",
      "Epoch 635, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00951/0.02262. Took 0.22 sec\n",
      "Epoch 636, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00712/0.02405. Took 0.22 sec\n",
      "Epoch 637, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00731/0.02484. Took 0.23 sec\n",
      "Epoch 638, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00779/0.02588. Took 0.22 sec\n",
      "Epoch 639, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01051/0.02544. Took 0.22 sec\n",
      "Epoch 640, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00942/0.02797. Took 0.22 sec\n",
      "Epoch 641, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00842/0.02740. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 642, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00780/0.03028. Took 0.22 sec\n",
      "Epoch 643, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00891/0.02867. Took 0.22 sec\n",
      "Epoch 644, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00743/0.02638. Took 0.22 sec\n",
      "Epoch 645, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00680/0.03054. Took 0.22 sec\n",
      "Epoch 646, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00739/0.02716. Took 0.22 sec\n",
      "Epoch 647, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00697/0.02696. Took 0.22 sec\n",
      "Epoch 648, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00839/0.02637. Took 0.22 sec\n",
      "Epoch 649, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00775/0.02519. Took 0.22 sec\n",
      "Epoch 650, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01090/0.02864. Took 0.22 sec\n",
      "Epoch 651, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01015/0.02747. Took 0.23 sec\n",
      "Epoch 652, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00841/0.02838. Took 0.22 sec\n",
      "Epoch 653, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00786/0.02682. Took 0.22 sec\n",
      "Epoch 654, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00984/0.02675. Took 0.22 sec\n",
      "Epoch 655, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00945/0.02470. Took 0.23 sec\n",
      "Epoch 656, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00943/0.02343. Took 0.23 sec\n",
      "Epoch 657, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00748/0.02611. Took 0.23 sec\n",
      "Epoch 658, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00705/0.02357. Took 0.22 sec\n",
      "Epoch 659, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00770/0.02522. Took 0.22 sec\n",
      "Epoch 660, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00885/0.02400. Took 0.22 sec\n",
      "Epoch 661, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00748/0.02541. Took 0.22 sec\n",
      "Epoch 662, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00642/0.02888. Took 0.22 sec\n",
      "Epoch 663, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00781/0.02848. Took 0.22 sec\n",
      "Epoch 664, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00810/0.02824. Took 0.22 sec\n",
      "Epoch 665, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00815/0.02422. Took 0.23 sec\n",
      "Epoch 666, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00873/0.02652. Took 0.22 sec\n",
      "Epoch 667, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00800/0.02463. Took 0.22 sec\n",
      "Epoch 668, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00859/0.02885. Took 0.22 sec\n",
      "Epoch 669, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00869/0.02711. Took 0.22 sec\n",
      "Epoch 670, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00711/0.02746. Took 0.22 sec\n",
      "Epoch 671, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00989/0.03368. Took 0.22 sec\n",
      "Epoch 672, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01078/0.02431. Took 0.22 sec\n",
      "Epoch 673, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00857/0.02752. Took 0.23 sec\n",
      "Epoch 674, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00848/0.03199. Took 0.22 sec\n",
      "Epoch 675, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00813/0.03508. Took 0.22 sec\n",
      "Epoch 676, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01045/0.02548. Took 0.22 sec\n",
      "Epoch 677, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00733/0.02596. Took 0.22 sec\n",
      "Epoch 678, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00882/0.02386. Took 0.23 sec\n",
      "Epoch 679, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00684/0.02741. Took 0.23 sec\n",
      "Epoch 680, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00792/0.02733. Took 0.22 sec\n",
      "Epoch 681, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00946/0.02975. Took 0.22 sec\n",
      "Epoch 682, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01246/0.02495. Took 0.21 sec\n",
      "Epoch 683, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00955/0.02425. Took 0.22 sec\n",
      "Epoch 684, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01032/0.02857. Took 0.22 sec\n",
      "Epoch 685, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00816/0.03172. Took 0.22 sec\n",
      "Epoch 686, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00718/0.02669. Took 0.22 sec\n",
      "Epoch 687, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00783/0.03105. Took 0.22 sec\n",
      "Epoch 688, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01141/0.02738. Took 0.22 sec\n",
      "Epoch 689, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00956/0.02307. Took 0.22 sec\n",
      "Epoch 690, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00782/0.02650. Took 0.22 sec\n",
      "Epoch 691, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00692/0.02305. Took 0.21 sec\n",
      "Epoch 692, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00648/0.02573. Took 0.22 sec\n",
      "Epoch 693, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00610/0.02576. Took 0.22 sec\n",
      "Epoch 694, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00891/0.02451. Took 0.22 sec\n",
      "Epoch 695, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00791/0.02437. Took 0.22 sec\n",
      "Epoch 696, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00801/0.02631. Took 0.22 sec\n",
      "Epoch 697, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00833/0.02512. Took 0.22 sec\n",
      "Epoch 698, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00689/0.02492. Took 0.22 sec\n",
      "Epoch 699, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00723/0.02472. Took 0.22 sec\n",
      "Epoch 700, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00642/0.02695. Took 0.22 sec\n",
      "Epoch 701, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00759/0.02962. Took 0.22 sec\n",
      "Epoch 702, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00691/0.03133. Took 0.22 sec\n",
      "Epoch 703, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00895/0.02713. Took 0.22 sec\n",
      "Epoch 704, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00912/0.02530. Took 0.22 sec\n",
      "Epoch 705, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00658/0.02646. Took 0.22 sec\n",
      "Epoch 706, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00906/0.02426. Took 0.22 sec\n",
      "Epoch 707, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00796/0.02551. Took 0.22 sec\n",
      "Epoch 708, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00842/0.02770. Took 0.22 sec\n",
      "Epoch 709, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00959/0.03104. Took 0.22 sec\n",
      "Epoch 710, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00829/0.02687. Took 0.22 sec\n",
      "Epoch 711, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00674/0.03074. Took 0.22 sec\n",
      "Epoch 712, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.03071. Took 0.22 sec\n",
      "Epoch 713, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00644/0.02490. Took 0.22 sec\n",
      "Epoch 714, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00651/0.02507. Took 0.22 sec\n",
      "Epoch 715, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00778/0.02790. Took 0.22 sec\n",
      "Epoch 716, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00989/0.02817. Took 0.22 sec\n",
      "Epoch 717, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00685/0.02739. Took 0.22 sec\n",
      "Epoch 718, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00735/0.02630. Took 0.22 sec\n",
      "Epoch 719, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00750/0.02860. Took 0.22 sec\n",
      "Epoch 720, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00747/0.02554. Took 0.23 sec\n",
      "Epoch 721, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00845/0.02814. Took 0.22 sec\n",
      "Epoch 722, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00838/0.02723. Took 0.22 sec\n",
      "Epoch 723, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00746/0.02918. Took 0.22 sec\n",
      "Epoch 724, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00947/0.02446. Took 0.22 sec\n",
      "Epoch 725, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00838/0.02553. Took 0.23 sec\n",
      "Epoch 726, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00776/0.02691. Took 0.22 sec\n",
      "Epoch 727, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00792/0.02689. Took 0.22 sec\n",
      "Epoch 728, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00737/0.02666. Took 0.22 sec\n",
      "Epoch 729, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01067/0.03016. Took 0.22 sec\n",
      "Epoch 730, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01000/0.02982. Took 0.22 sec\n",
      "Epoch 731, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00927/0.03083. Took 0.22 sec\n",
      "Epoch 732, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00940/0.02592. Took 0.22 sec\n",
      "Epoch 733, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00834/0.02967. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 734, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00893/0.02728. Took 0.22 sec\n",
      "Epoch 735, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01018/0.02530. Took 0.22 sec\n",
      "Epoch 736, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00776/0.02646. Took 0.23 sec\n",
      "Epoch 737, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00764/0.02746. Took 0.22 sec\n",
      "Epoch 738, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00755/0.02471. Took 0.22 sec\n",
      "Epoch 739, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00792/0.02553. Took 0.22 sec\n",
      "Epoch 740, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00795/0.02553. Took 0.22 sec\n",
      "Epoch 741, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00720/0.02746. Took 0.23 sec\n",
      "Epoch 742, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00589/0.02709. Took 0.22 sec\n",
      "Epoch 743, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00687/0.02896. Took 0.22 sec\n",
      "Epoch 744, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00759/0.02760. Took 0.22 sec\n",
      "Epoch 745, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00666/0.02801. Took 0.22 sec\n",
      "Epoch 746, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00624/0.02937. Took 0.22 sec\n",
      "Epoch 747, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00733/0.02738. Took 0.22 sec\n",
      "Epoch 748, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00699/0.02799. Took 0.22 sec\n",
      "Epoch 749, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00662/0.02791. Took 0.22 sec\n",
      "Epoch 750, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00776/0.02460. Took 0.22 sec\n",
      "Epoch 751, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01033/0.02283. Took 0.22 sec\n",
      "Epoch 752, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00728/0.02538. Took 0.23 sec\n",
      "Epoch 753, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00862/0.02732. Took 0.22 sec\n",
      "Epoch 754, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01249/0.02900. Took 0.22 sec\n",
      "Epoch 755, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00943/0.02876. Took 0.23 sec\n",
      "Epoch 756, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01537/0.03161. Took 0.23 sec\n",
      "Epoch 757, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01358/0.03021. Took 0.23 sec\n",
      "Epoch 758, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00952/0.02722. Took 0.22 sec\n",
      "Epoch 759, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01219/0.02258. Took 0.23 sec\n",
      "Epoch 760, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00918/0.02740. Took 0.22 sec\n",
      "Epoch 761, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00941/0.02805. Took 0.22 sec\n",
      "Epoch 762, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00809/0.02676. Took 0.22 sec\n",
      "Epoch 763, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00766/0.02439. Took 0.23 sec\n",
      "Epoch 764, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00892/0.03148. Took 0.22 sec\n",
      "Epoch 765, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00966/0.02769. Took 0.22 sec\n",
      "Epoch 766, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00730/0.02856. Took 0.22 sec\n",
      "Epoch 767, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00657/0.02616. Took 0.22 sec\n",
      "Epoch 768, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00842/0.02440. Took 0.22 sec\n",
      "Epoch 769, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00685/0.02670. Took 0.22 sec\n",
      "Epoch 770, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00675/0.02752. Took 0.23 sec\n",
      "Epoch 771, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00655/0.02966. Took 0.22 sec\n",
      "Epoch 772, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00727/0.02941. Took 0.22 sec\n",
      "Epoch 773, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00774/0.03022. Took 0.22 sec\n",
      "Epoch 774, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00704/0.03077. Took 0.23 sec\n",
      "Epoch 775, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00832/0.02444. Took 0.22 sec\n",
      "Epoch 776, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00619/0.02761. Took 0.23 sec\n",
      "Epoch 777, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00657/0.02816. Took 0.23 sec\n",
      "Epoch 778, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00950/0.02684. Took 0.23 sec\n",
      "Epoch 779, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.02412. Took 0.22 sec\n",
      "Epoch 780, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00888/0.02897. Took 0.22 sec\n",
      "Epoch 781, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00833/0.03071. Took 0.22 sec\n",
      "Epoch 782, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00739/0.02829. Took 0.22 sec\n",
      "Epoch 783, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00875/0.03113. Took 0.23 sec\n",
      "Epoch 784, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01098/0.02617. Took 0.22 sec\n",
      "Epoch 785, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00888/0.02743. Took 0.22 sec\n",
      "Epoch 786, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00791/0.02893. Took 0.22 sec\n",
      "Epoch 787, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00792/0.02871. Took 0.22 sec\n",
      "Epoch 788, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00743/0.03037. Took 0.22 sec\n",
      "Epoch 789, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.02700. Took 0.22 sec\n",
      "Epoch 790, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00692/0.02762. Took 0.23 sec\n",
      "Epoch 791, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00731/0.02565. Took 0.22 sec\n",
      "Epoch 792, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00674/0.02901. Took 0.22 sec\n",
      "Epoch 793, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00690/0.02733. Took 0.22 sec\n",
      "Epoch 794, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00677/0.02453. Took 0.23 sec\n",
      "Epoch 795, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00903/0.02593. Took 0.23 sec\n",
      "Epoch 796, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00782/0.02907. Took 0.22 sec\n",
      "Epoch 797, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00822/0.02772. Took 0.22 sec\n",
      "Epoch 798, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00878/0.02650. Took 0.22 sec\n",
      "Epoch 799, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00765/0.03057. Took 0.22 sec\n",
      "Epoch 800, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00994/0.02649. Took 0.22 sec\n",
      "Epoch 801, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00745/0.02685. Took 0.22 sec\n",
      "Epoch 802, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01293/0.02949. Took 0.23 sec\n",
      "Epoch 803, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01332/0.02849. Took 0.23 sec\n",
      "Epoch 804, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01110/0.02866. Took 0.23 sec\n",
      "Epoch 805, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01091/0.02685. Took 0.22 sec\n",
      "Epoch 806, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01120/0.02749. Took 0.22 sec\n",
      "Epoch 807, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00957/0.02753. Took 0.22 sec\n",
      "Epoch 808, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01105/0.03089. Took 0.22 sec\n",
      "Epoch 809, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00791/0.02673. Took 0.22 sec\n",
      "Epoch 810, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00719/0.02890. Took 0.22 sec\n",
      "Epoch 811, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00639/0.03008. Took 0.23 sec\n",
      "Epoch 812, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00659/0.02605. Took 0.23 sec\n",
      "Epoch 813, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00685/0.02996. Took 0.22 sec\n",
      "Epoch 814, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00638/0.02849. Took 0.22 sec\n",
      "Epoch 815, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00933/0.03149. Took 0.22 sec\n",
      "Epoch 816, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00918/0.02837. Took 0.22 sec\n",
      "Epoch 817, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00654/0.02990. Took 0.23 sec\n",
      "Epoch 818, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00740/0.02816. Took 0.22 sec\n",
      "Epoch 819, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00931/0.02652. Took 0.22 sec\n",
      "Epoch 820, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00883/0.02612. Took 0.23 sec\n",
      "Epoch 821, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00826/0.02767. Took 0.23 sec\n",
      "Epoch 822, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00767/0.02634. Took 0.23 sec\n",
      "Epoch 823, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00770/0.02893. Took 0.22 sec\n",
      "Epoch 824, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00747/0.02726. Took 0.22 sec\n",
      "Epoch 825, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00761/0.02586. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00919/0.02519. Took 0.22 sec\n",
      "Epoch 827, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00764/0.02514. Took 0.23 sec\n",
      "Epoch 828, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00699/0.02766. Took 0.22 sec\n",
      "Epoch 829, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00823/0.03024. Took 0.22 sec\n",
      "Epoch 830, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00848/0.02881. Took 0.22 sec\n",
      "Epoch 831, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00863/0.02410. Took 0.23 sec\n",
      "Epoch 832, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00692/0.02618. Took 0.22 sec\n",
      "Epoch 833, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00623/0.02607. Took 0.22 sec\n",
      "Epoch 834, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00717/0.02831. Took 0.22 sec\n",
      "Epoch 835, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00680/0.02619. Took 0.23 sec\n",
      "Epoch 836, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00747/0.02498. Took 0.22 sec\n",
      "Epoch 837, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00681/0.02692. Took 0.23 sec\n",
      "Epoch 838, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00673/0.02597. Took 0.22 sec\n",
      "Epoch 839, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00614/0.02803. Took 0.22 sec\n",
      "Epoch 840, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00766/0.02716. Took 0.22 sec\n",
      "Epoch 841, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00633/0.02917. Took 0.22 sec\n",
      "Epoch 842, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00666/0.02912. Took 0.23 sec\n",
      "Epoch 843, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00837/0.03101. Took 0.22 sec\n",
      "Epoch 844, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00786/0.02834. Took 0.22 sec\n",
      "Epoch 845, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00823/0.02995. Took 0.22 sec\n",
      "Epoch 846, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01211/0.02834. Took 0.22 sec\n",
      "Epoch 847, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00842/0.02430. Took 0.23 sec\n",
      "Epoch 848, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00704/0.02749. Took 0.22 sec\n",
      "Epoch 849, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00665/0.02869. Took 0.22 sec\n",
      "Epoch 850, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00760/0.02561. Took 0.22 sec\n",
      "Epoch 851, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01074/0.02783. Took 0.22 sec\n",
      "Epoch 852, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01270/0.03399. Took 0.22 sec\n",
      "Epoch 853, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01071/0.03302. Took 0.23 sec\n",
      "Epoch 854, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00878/0.02976. Took 0.22 sec\n",
      "Epoch 855, Acc_RMSE(train/val): 0.01/0.03, Loss(train/val) 0.00723/0.03464. Took 0.23 sec\n",
      "Epoch 856, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01350/0.02620. Took 0.22 sec\n",
      "Epoch 857, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00935/0.02981. Took 0.22 sec\n",
      "Epoch 858, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00781/0.02483. Took 0.23 sec\n",
      "Epoch 859, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00635/0.02795. Took 0.22 sec\n",
      "Epoch 860, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00612/0.02803. Took 0.22 sec\n",
      "Epoch 861, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00675/0.02717. Took 0.22 sec\n",
      "Epoch 862, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00704/0.02721. Took 0.23 sec\n",
      "Epoch 863, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00696/0.02734. Took 0.23 sec\n",
      "Epoch 864, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00679/0.02733. Took 0.22 sec\n",
      "Epoch 865, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00700/0.03132. Took 0.22 sec\n",
      "Epoch 866, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01137/0.02950. Took 0.22 sec\n",
      "Epoch 867, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00877/0.02753. Took 0.22 sec\n",
      "Epoch 868, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01097/0.02453. Took 0.22 sec\n",
      "Epoch 869, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00793/0.02732. Took 0.22 sec\n",
      "Epoch 870, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01273/0.03503. Took 0.22 sec\n",
      "Epoch 871, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01169/0.02837. Took 0.23 sec\n",
      "Epoch 872, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00924/0.02722. Took 0.23 sec\n",
      "Epoch 873, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00844/0.02672. Took 0.22 sec\n",
      "Epoch 874, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00761/0.02739. Took 0.23 sec\n",
      "Epoch 875, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00662/0.02668. Took 0.22 sec\n",
      "Epoch 876, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00713/0.02573. Took 0.22 sec\n",
      "Epoch 877, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00802/0.02629. Took 0.22 sec\n",
      "Epoch 878, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00890/0.02672. Took 0.22 sec\n",
      "Epoch 879, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01186/0.03365. Took 0.23 sec\n",
      "Epoch 880, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01144/0.03135. Took 0.22 sec\n",
      "Epoch 881, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00714/0.02647. Took 0.22 sec\n",
      "Epoch 882, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00817/0.02495. Took 0.23 sec\n",
      "Epoch 883, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00752/0.02996. Took 0.22 sec\n",
      "Epoch 884, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00952/0.02535. Took 0.22 sec\n",
      "Epoch 885, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00908/0.03117. Took 0.22 sec\n",
      "Epoch 886, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00696/0.02644. Took 0.22 sec\n",
      "Epoch 887, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00806/0.02881. Took 0.22 sec\n",
      "Epoch 888, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01087/0.02852. Took 0.22 sec\n",
      "Epoch 889, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00880/0.03065. Took 0.22 sec\n",
      "Epoch 890, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00693/0.02833. Took 0.22 sec\n",
      "Epoch 891, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00737/0.02883. Took 0.22 sec\n",
      "Epoch 892, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01014/0.02558. Took 0.22 sec\n",
      "Epoch 893, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00929/0.02760. Took 0.22 sec\n",
      "Epoch 894, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01069/0.02496. Took 0.22 sec\n",
      "Epoch 895, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00937/0.02999. Took 0.23 sec\n",
      "Epoch 896, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00896/0.03152. Took 0.22 sec\n",
      "Epoch 897, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00885/0.02566. Took 0.22 sec\n",
      "Epoch 898, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00908/0.03137. Took 0.22 sec\n",
      "Epoch 899, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00821/0.02590. Took 0.22 sec\n",
      "Epoch 900, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00661/0.02540. Took 0.23 sec\n",
      "Epoch 901, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00694/0.02676. Took 0.22 sec\n",
      "Epoch 902, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00923/0.03034. Took 0.22 sec\n",
      "Epoch 903, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00750/0.02696. Took 0.23 sec\n",
      "Epoch 904, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00742/0.02490. Took 0.23 sec\n",
      "Epoch 905, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00608/0.02554. Took 0.23 sec\n",
      "Epoch 906, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00651/0.02750. Took 0.23 sec\n",
      "Epoch 907, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00734/0.02764. Took 0.23 sec\n",
      "Epoch 908, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00760/0.02723. Took 0.22 sec\n",
      "Epoch 909, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00718/0.02790. Took 0.22 sec\n",
      "Epoch 910, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00878/0.02771. Took 0.23 sec\n",
      "Epoch 911, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00933/0.02510. Took 0.22 sec\n",
      "Epoch 912, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00942/0.03119. Took 0.22 sec\n",
      "Epoch 913, Acc_RMSE(train/val): 0.02/0.03, Loss(train/val) 0.01096/0.03628. Took 0.22 sec\n",
      "Epoch 914, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00838/0.02811. Took 0.22 sec\n",
      "Epoch 915, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00913/0.02370. Took 0.23 sec\n",
      "Epoch 916, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00858/0.02543. Took 0.22 sec\n",
      "Epoch 917, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00767/0.02835. Took 0.22 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 918, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00685/0.02651. Took 0.22 sec\n",
      "Epoch 919, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00725/0.02767. Took 0.22 sec\n",
      "Epoch 920, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00992/0.02451. Took 0.22 sec\n",
      "Epoch 921, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00896/0.02304. Took 0.22 sec\n",
      "Epoch 922, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00668/0.02630. Took 0.22 sec\n",
      "Epoch 923, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00593/0.02554. Took 0.22 sec\n",
      "Epoch 924, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00637/0.02729. Took 0.22 sec\n",
      "Epoch 925, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00917/0.02883. Took 0.22 sec\n",
      "Epoch 926, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00780/0.02619. Took 0.23 sec\n",
      "Epoch 927, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00713/0.02784. Took 0.22 sec\n",
      "Epoch 928, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00681/0.02780. Took 0.23 sec\n",
      "Epoch 929, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00752/0.02778. Took 0.22 sec\n",
      "Epoch 930, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00859/0.02845. Took 0.22 sec\n",
      "Epoch 931, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00885/0.02798. Took 0.22 sec\n",
      "Epoch 932, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01064/0.02665. Took 0.22 sec\n",
      "Epoch 933, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00744/0.02585. Took 0.22 sec\n",
      "Epoch 934, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00724/0.02938. Took 0.22 sec\n",
      "Epoch 935, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00777/0.02652. Took 0.23 sec\n",
      "Epoch 936, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00816/0.03174. Took 0.23 sec\n",
      "Epoch 937, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00798/0.02829. Took 0.22 sec\n",
      "Epoch 938, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00720/0.02774. Took 0.23 sec\n",
      "Epoch 939, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00771/0.02869. Took 0.22 sec\n",
      "Epoch 940, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00693/0.02777. Took 0.22 sec\n",
      "Epoch 941, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00655/0.02683. Took 0.22 sec\n",
      "Epoch 942, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00810/0.02601. Took 0.23 sec\n",
      "Epoch 943, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00770/0.02448. Took 0.22 sec\n",
      "Epoch 944, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00989/0.02519. Took 0.23 sec\n",
      "Epoch 945, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00722/0.02822. Took 0.23 sec\n",
      "Epoch 946, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00747/0.02530. Took 0.23 sec\n",
      "Epoch 947, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00902/0.03164. Took 0.22 sec\n",
      "Epoch 948, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01091/0.03135. Took 0.22 sec\n",
      "Epoch 949, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01087/0.02687. Took 0.22 sec\n",
      "Epoch 950, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01168/0.03157. Took 0.23 sec\n",
      "Epoch 951, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01002/0.03216. Took 0.22 sec\n",
      "Epoch 952, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00813/0.03291. Took 0.22 sec\n",
      "Epoch 953, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01649/0.02613. Took 0.22 sec\n",
      "Epoch 954, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01264/0.02689. Took 0.23 sec\n",
      "Epoch 955, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00878/0.02673. Took 0.23 sec\n",
      "Epoch 956, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00781/0.02695. Took 0.22 sec\n",
      "Epoch 957, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00653/0.02701. Took 0.23 sec\n",
      "Epoch 958, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00769/0.02447. Took 0.22 sec\n",
      "Epoch 959, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00768/0.02752. Took 0.22 sec\n",
      "Epoch 960, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00744/0.02635. Took 0.22 sec\n",
      "Epoch 961, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00681/0.02764. Took 0.22 sec\n",
      "Epoch 962, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00912/0.02837. Took 0.22 sec\n",
      "Epoch 963, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01104/0.02934. Took 0.22 sec\n",
      "Epoch 964, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01026/0.02867. Took 0.22 sec\n",
      "Epoch 965, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00759/0.02664. Took 0.22 sec\n",
      "Epoch 966, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00889/0.02518. Took 0.22 sec\n",
      "Epoch 967, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00683/0.02761. Took 0.22 sec\n",
      "Epoch 968, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00931/0.03200. Took 0.23 sec\n",
      "Epoch 969, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00741/0.02619. Took 0.22 sec\n",
      "Epoch 970, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00916/0.02782. Took 0.22 sec\n",
      "Epoch 971, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00814/0.02950. Took 0.22 sec\n",
      "Epoch 972, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00736/0.02854. Took 0.22 sec\n",
      "Epoch 973, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00671/0.02908. Took 0.22 sec\n",
      "Epoch 974, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00633/0.02896. Took 0.22 sec\n",
      "Epoch 975, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00709/0.02597. Took 0.23 sec\n",
      "Epoch 976, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00728/0.02703. Took 0.23 sec\n",
      "Epoch 977, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00670/0.02694. Took 0.22 sec\n",
      "Epoch 978, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00619/0.02662. Took 0.22 sec\n",
      "Epoch 979, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00616/0.02716. Took 0.22 sec\n",
      "Epoch 980, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00712/0.02811. Took 0.22 sec\n",
      "Epoch 981, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00610/0.02747. Took 0.22 sec\n",
      "Epoch 982, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00754/0.02670. Took 0.22 sec\n",
      "Epoch 983, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00690/0.02847. Took 0.22 sec\n",
      "Epoch 984, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00594/0.02785. Took 0.22 sec\n",
      "Epoch 985, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00623/0.02734. Took 0.22 sec\n",
      "Epoch 986, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00661/0.03196. Took 0.22 sec\n",
      "Epoch 987, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00770/0.02922. Took 0.23 sec\n",
      "Epoch 988, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00918/0.02766. Took 0.22 sec\n",
      "Epoch 989, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.01128/0.02877. Took 0.22 sec\n",
      "Epoch 990, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.02915. Took 0.22 sec\n",
      "Epoch 991, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00830/0.02835. Took 0.23 sec\n",
      "Epoch 992, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00853/0.02647. Took 0.23 sec\n",
      "Epoch 993, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00627/0.02454. Took 0.22 sec\n",
      "Epoch 994, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00696/0.03002. Took 0.22 sec\n",
      "Epoch 995, Acc_RMSE(train/val): 0.02/0.02, Loss(train/val) 0.00866/0.03054. Took 0.22 sec\n",
      "Epoch 996, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00772/0.02635. Took 0.22 sec\n",
      "Epoch 997, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00749/0.03050. Took 0.22 sec\n",
      "Epoch 998, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00857/0.02763. Took 0.22 sec\n",
      "Epoch 999, Acc_RMSE(train/val): 0.01/0.02, Loss(train/val) 0.00622/0.03005. Took 0.22 sec\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ====== Data Loading ====== #\n",
    "args.batch_size = 4 # 2~8\n",
    "args.UpData = UpData\n",
    "args.DownData = DownData\n",
    "args.x_frames = 4\n",
    "args.y_frames = 1\n",
    "\n",
    "# ====== Model Capacity ===== #\n",
    "args.input_dim = len(UpData.columns)\n",
    "args.hid_dim = 32\n",
    "args.n_layers = 2\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.0001 \n",
    "args.dropout = 0.1 \n",
    "args.use_bn = False\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam'  #SGD, RMSprop, Adam...\n",
    "args.loss = 'MSELoss'#'MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss'\n",
    "args.lr = 0.01\n",
    "args.epoch = 3\n",
    "\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'x_frames'\n",
    "name_var2 = 'batch_size'\n",
    "name_var3 = 'optim'\n",
    "name_var4 = 'lr'\n",
    "name_var5 = 'epoch'\n",
    "list_var1 = [4]\n",
    "list_var2 = [8]\n",
    "list_var3 = ['Adam']\n",
    "list_var4 = [0.01]\n",
    "list_var5 = [1000]\n",
    "\n",
    "\n",
    "num = 0 #초기화\n",
    "\n",
    "\n",
    "trainset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2013-01-01', '2016-07-31')\n",
    "valset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-08-01', '2017-05-19')\n",
    "testset = RiverDataset(args.UpData, args.DownData, args.x_frames, args.y_frames, '2016-08-01', '2017-05-19')\n",
    "partition = {'train': trainset, 'val':valset, 'test':testset}\n",
    "\n",
    "\n",
    "print('size of trainset :{}'.format(len(trainset)),\n",
    "      'size of valset :{}'.format(len(valset)),\n",
    "      'size of testset :{}'.format(len(testset)))\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        for var3 in list_var3:\n",
    "            for var4 in list_var4:\n",
    "                for var5 in list_var5:\n",
    "                    num += 1\n",
    "                    setattr(args, name_var1, var1)\n",
    "                    setattr(args, name_var2, var2)\n",
    "                    setattr(args, name_var3, var3)\n",
    "                    setattr(args, name_var4, var4)\n",
    "                    setattr(args, name_var5, var5)\n",
    "                    print('\\n exp_{}'.format(num))\n",
    "                    print('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                          .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn))     \n",
    "\n",
    "                    result = experiment(partition, deepcopy(args))\n",
    "\n",
    "            #         print('train_acc_RMSE = {:2.2f}%, val_acc_RMSE = {:2.2f}%, test_RMSE = {:2.2f}%, test_R2 = {:2.2f}%'\n",
    "            #               .format(result['train_acc_RMSE'],result['val_acc_RMSE'],result['test_RMSE'],result['test_R2']))\n",
    "\n",
    "                    vis.text('loseFunc = {}, optim={}, x_frames={}, n_layers={}, batch_size={}, hid_dim={}, epoch={}, lr={}, l2={}, dropout={}, use_bn={}'\n",
    "                             .format(args.loss,args.optim,args.x_frames,args.n_layers,args.batch_size,args.hid_dim,args.epoch,args.lr,args.l2,args.dropout,args.use_bn),\n",
    "                             opts=dict(title='exp_{}_text'.format(num)))\n",
    "                    # 만든 모델의 test 데이터 예측 시각화\n",
    "\n",
    "                    predict = torch.Tensor(result['test_pred']).view(-1,1)\n",
    "                    truth = torch.Tensor(result['test_true']).view(-1,1)\n",
    "                    axis = torch.Tensor(range(len(result['test_pred']))).view(-1,1)\n",
    "                    Y_axis = torch.cat((predict, truth), -1)\n",
    "                    X_axis = torch.cat((axis, axis), -1)\n",
    "                    vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                               .format(num,result['test_RMSE'],result['test_R2']),\n",
    "                                                               legend=['predict','true'],\n",
    "                                                               showlegend=True,\n",
    "                                                               layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "                    \n",
    "                    if args.epoch >= 50:\n",
    "                        predict = torch.Tensor(result['test_pred_50']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_50']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_50']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[50]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_50'],result['test_R2_50']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "\n",
    "\n",
    "                    if args.epoch >= 100:\n",
    "                        predict = torch.Tensor(result['test_pred_100']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_100']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_100']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[100]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_100'],result['test_R2_100']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "\n",
    "                    if args.epoch >= 150:\n",
    "                        predict = torch.Tensor(result['test_pred_150']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_150']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_150']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[150]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_150'],result['test_R2_150']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "                    \n",
    "                    if args.epoch >= 200:\n",
    "                        predict = torch.Tensor(result['test_pred_200']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_200']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_200']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[200]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_200'],result['test_R2_200']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "                        \n",
    "                    if args.epoch >= 300:\n",
    "                        predict = torch.Tensor(result['test_pred_300']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_300']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_300']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[300]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_300'],result['test_R2_300']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "                        \n",
    "                    if args.epoch >= 500:\n",
    "                        predict = torch.Tensor(result['test_pred_500']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_500']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_500']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[500]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_500'],result['test_R2_500']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "                        \n",
    "                    if args.epoch >= 750:\n",
    "                        predict = torch.Tensor(result['test_pred_750']).view(-1,1)\n",
    "                        truth = torch.Tensor(result['test_true_750']).view(-1,1)\n",
    "                        axis = torch.Tensor(range(len(result['test_pred_750']))).view(-1,1)\n",
    "                        Y_axis = torch.cat((predict, truth), -1)\n",
    "                        X_axis = torch.cat((axis, axis), -1)\n",
    "                        vis.line(Y = Y_axis, X = X_axis, opts=dict(title='Result_exp_{}_epoch[750]_RMSE[{:2.3f}]_R2[{:2.3f}]'\n",
    "                                                                   .format(num,result['test_RMSE_750'],result['test_R2_750']),\n",
    "                                                                   legend=['predict','true'],\n",
    "                                                                   showlegend=True,\n",
    "                                                                   layoutopts = {'plotly': {'legend': {'x':0, 'y':0}}}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lose = ['MSELoss','L1Loss','PoissonNLLLoss','KLDivLoss','BCELoss','BCEWithLogitsLoss']\n",
    "optim = ['SGD', 'RMSprop', 'Adam']\n",
    "dropout = ['']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
